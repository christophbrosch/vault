[{"id":"14:00-17:00ISOIEC91261","abstract":"Software engineering — Product quality — Part 1: Quality model","accessed":{"date-parts":[["2022",11,1]]},"author":[{"literal":"14:00-17:00"}],"citation-key":"14:00-17:00ISOIEC91261","container-title":"ISO","language":"en","title":"ISO/IEC 9126-1:2001","title-short":"ISO/IEC 9126-1","type":"webpage","URL":"https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/02/27/22749.html"},{"id":"abuodaTransformingRDFstarProperty","abstract":"RDF and property graph models have many similarities, such as using basic graph concepts like nodes and edges. However, such models differ in their modeling approach, expressivity, serialization, and the nature of applications. RDF is the de-facto standard model for knowledge graphs on the Semantic Web and supported by a rich ecosystem for inference and processing. The property graph model, in contrast, provides advantages in scalable graph analytical tasks, such as graph matching, path analysis, and graph traversal. RDF-star extends RDF and allows capturing metadata as a first-class citizen. To tap on the advantages of alternative models, the literature proposes different ways of transforming knowledge graphs between property graphs and RDF. However, most of these approaches cannot provide complete transformations for RDF-star graphs. Hence, this paper provides a step towards transforming RDF-star graphs into property graphs. In particular, we identify different cases to evaluate transformation approaches from RDF-star to property graphs. Specifically, we categorize two classes of transformation approaches and analyze them based on the test cases. The obtained insights will form the foundation for building complete transformation approaches in the future.","author":[{"family":"Abuoda","given":"Ghadeer"},{"family":"Dell’Aglio","given":"Daniele"},{"family":"Keen","given":"Arthur"},{"family":"Hose","given":"Katja"}],"citation-key":"abuodaTransformingRDFstarProperty","language":"en","source":"Zotero","title":"Transforming RDF-star to Property Graphs: A Preliminary Analysis of Transformation Approaches","type":"article-journal"},{"id":"ActionSchemaOrg","accessed":{"date-parts":[["2024",2,12]]},"citation-key":"ActionSchemaOrg","title":"Action - Schema.org Type","type":"webpage","URL":"https://schema.org/Action"},{"id":"adatoAutomaticallyMonitoringRetail2020","abstract":"A system for acquiring images of products in a retail store is disclosed. The system may include at least one first housing configured for location on a retail shelving unit, and at least one image capture device included in the at least one first housing and configured relative to the at least one first housing such that an optical axis of the at least one image capture device is directed toward an opposing retail shelving unit when the at least one first housing is fixedly mounted on the retail shelving unit. The system may further include a second housing configured for location on the retail shelving unit separate from the at least one first housing, the second housing may contain at least one processor configured to control the at least one image capture device and also to control a network interface for communicating with a remote server. The system may also include at least one data conduit extending between the at least one first housing and the second housing, the at least one data conduit being configured to enable transfer of control signals from the at least one processor to the at least one image capture device and to enable collection of image data acquired by the at least one image capture device for transmission by the network interface.","accessed":{"date-parts":[["2023",1,14]]},"author":[{"family":"ADATO","given":"Yair"},{"family":"LISHNER","given":"Itai"},{"family":"COHEN","given":"Daniel Shimon"},{"family":"EISENSCHTAT","given":"Aviv"},{"family":"POMERANZ","given":"Dolev"},{"family":"MHABARY","given":"Ziv"},{"family":"Yanushevsky","given":"Osnat"},{"family":"MICHAEL","given":"Yotam"},{"family":"Adar","given":"Yonatan"},{"family":"Kushnir","given":"Maria"},{"family":"Yashpe","given":"Dror"},{"family":"Devir","given":"Yohai"},{"family":"Yudkin","given":"Paul"},{"family":"Bronicki","given":"Youval"},{"family":"Dayan","given":"Shlomi"},{"family":"Peled","given":"Galit"},{"family":"Gottlieb","given":"David M."},{"family":"Grubshtein","given":"Alon"},{"family":"Hemed","given":"Nir"}],"citation-key":"adatoAutomaticallyMonitoringRetail2020","issued":{"date-parts":[["2020",12]]},"number":"EP3754546A1","title":"Automatically monitoring retail products based on captured images","type":"patent","URL":"https://patents.google.com/patent/EP3754546A1/en"},{"id":"adatoAutomaticallyMonitoringRetail2020a","abstract":"A system for acquiring images of products in a retail store is disclosed. The system may include at least one first housing configured for location on a retail shelving unit, and at least one image capture device included in the at least one first housing and configured relative to the at least one first housing such that an optical axis of the at least one image capture device is directed toward an opposing retail shelving unit when the at least one first housing is fixedly mounted on the retail shelving unit. The system may further include a second housing configured for location on the retail shelving unit separate from the at least one first housing, the second housing may contain at least one processor configured to control the at least one image capture device and also to control a network interface for communicating with a remote server. The system may also include at least one data conduit extending between the at least one first housing and the second housing, the at least one data conduit being configured to enable transfer of control signals from the at least one processor to the at least one image capture device and to enable collection of image data acquired by the at least one image capture device for transmission by the network interface.","accessed":{"date-parts":[["2023",1,14]]},"author":[{"family":"ADATO","given":"Yair"},{"family":"LISHNER","given":"Itai"},{"family":"COHEN","given":"Daniel Shimon"},{"family":"EISENSCHTAT","given":"Aviv"},{"family":"POMERANZ","given":"Dolev"},{"family":"MHABARY","given":"Ziv"},{"family":"Yanushevsky","given":"Osnat"},{"family":"MICHAEL","given":"Yotam"},{"family":"Adar","given":"Yonatan"},{"family":"Kushnir","given":"Maria"},{"family":"Yashpe","given":"Dror"},{"family":"Devir","given":"Yohai"},{"family":"Yudkin","given":"Paul"},{"family":"Bronicki","given":"Youval"},{"family":"Dayan","given":"Shlomi"},{"family":"Peled","given":"Galit"},{"family":"Gottlieb","given":"David M."},{"family":"Grubshtein","given":"Alon"},{"family":"Hemed","given":"Nir"}],"authority":"European Union","call-number":"EP20177107.8A","citation-key":"adatoAutomaticallyMonitoringRetail2020a","issued":{"date-parts":[["2020",12,23]]},"number":"EP3754546A1","submitted":{"date-parts":[["2019",1,10]]},"title":"Automatically monitoring retail products based on captured images","type":"patent","URL":"https://patents.google.com/patent/EP3754546A1/en"},{"id":"aiLearningHeterogeneousKnowledge2018","abstract":"Providing model-generated explanations in recommender systems is important to user experience. State-of-the-art recommendation algorithms—especially the collaborative filtering (CF)- based approaches with shallow or deep models—usually work with various unstructured information sources for recommendation, such as textual reviews, visual images, and various implicit or explicit feedbacks. Though structured knowledge bases were considered in content-based approaches, they have been largely ignored recently due to the availability of vast amounts of data and the learning power of many complex models. However, structured knowledge bases exhibit unique advantages in personalized recommendation systems. When the explicit knowledge about users and items is considered for recommendation, the system could provide highly customized recommendations based on users’ historical behaviors and the knowledge is helpful for providing informed explanations regarding the recommended items. A great challenge for using knowledge bases for recommendation is how to integrate large-scale structured and unstructured data, while taking advantage of collaborative filtering for highly accurate performance. Recent achievements in knowledge-base embedding (KBE) sheds light on this problem, which makes it possible to learn user and item representations while preserving the structure of their relationship with external knowledge for explanation. In this work, we propose to explain knowledge-base embeddings for explainable recommendation. Specifically, we propose a knowledge-base representation learning framework to embed heterogeneous entities for recommendation, and based on the embedded knowledge base, a soft matching algorithm is proposed to generate personalized explanations for the recommended items. Experimental results on real-world e-commerce datasets verified the superior recommendation performance and the explainability power of our approach compared with state-of-the-art baselines.","accessed":{"date-parts":[["2024",2,12]]},"author":[{"family":"Ai","given":"Qingyao"},{"family":"Azizi","given":"Vahid"},{"family":"Chen","given":"Xu"},{"family":"Zhang","given":"Yongfeng"}],"citation-key":"aiLearningHeterogeneousKnowledge2018","container-title":"Algorithms","DOI":"10.3390/a11090137","ISSN":"1999-4893","issue":"9","issued":{"date-parts":[["2018",9]]},"language":"en","license":"http://creativecommons.org/licenses/by/3.0/","number":"9","page":"137","publisher":"Multidisciplinary Digital Publishing Institute","source":"www.mdpi.com","title":"Learning Heterogeneous Knowledge Base Embeddings for Explainable Recommendation","type":"article-journal","URL":"https://www.mdpi.com/1999-4893/11/9/137","volume":"11"},{"id":"alanturinginstituteKnowledgeGraphs2024","accessed":{"date-parts":[["2024",6,23]]},"author":[{"family":"AlanTuringInstitute","given":""}],"citation-key":"alanturinginstituteKnowledgeGraphs2024","container-title":"The Alan Turing Institute","issued":{"date-parts":[["2024"]]},"language":"en","title":"Knowledge graphs","type":"webpage","URL":"https://www.turing.ac.uk/research/interest-groups/knowledge-graphs"},{"id":"allemangChapter10SKOS2011","abstract":"SKOS (Simple Knowledge Organization System) is a W3C Recommendation that provides a means for representing knowledge organization systems in a distributed and linkable way. SKOS was designed from the start to allow modelers to create modular knowledge organizations that can be reused and referenced across the Web. It was designed to augment thesaurus standard by bringing the distributed nature of the Semantic Web to thesauri and controlled vocabularies. Toward this end, it was also a design goal of SKOS that it be possible to map any thesaurus standards to SKOS in a fairly straightforward way. SKOS takes advantage of the distributed nature of RDF to allow extension to a network of information to be distributed across the Web. It relies on the inferencing structure of RDFS-Plus to add completeness to their information structure. SKOS vocabularies provide a cornerstone for linking information on the Web. Publishing vocabularies in SKOS allows the concepts they define to be referenced on a global scale.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapter10SKOS2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10010-X","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"207-219","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 10 - SKOS—managing vocabularies with RDFS-Plus","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B978012385965510010X"},{"id":"allemangChapter11Basic2011","abstract":"This chapter presents the modeling capabilities of Web Ontology Language (OWL) that go beyond RDFS-Plus. A key functionality of OWL is the ability to define restriction classes. The unnamed classes are defined based on restrictions on the values for particular properties of the class. Using this mechanism, OWL can be used to model situations in which the members of a particular class must have certain properties. In RDFS, the domain and range restrictions can allow one to make inferences about all the members of a class (such as plays relating a baseball player to a team). In OWL, one can use restriction statements to differentiate the case between something that applies to all members of a class versus some members, and even to insist on a particular value for a specific property of all members of a class. When restrictions are used in combination with the constructs of RDFS, and then they are cascaded with one another, they can be used to model complex relationships between properties, classes, and individuals. The advantage of modeling relationships in this way (over informal specification) is that interactions of multiple specifications can be understood and even processed automatically.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapter11Basic2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10011-1","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"221-248","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 11 - Basic OWL","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100111"},{"id":"allemangChapter12Counting2011","abstract":"The restrictions are powerful methods for defining classes of individuals. This chapter helps see that Web Ontology Language (OWL) augments this capability with a full set theory language, including intersections, unions, and complements. These can be used to combine restrictions together (e.g., the set of planets that go around the sun and have at least one moon) or to combine the classes one uses to define restrictions (a vegetarian is someone who eats food that is not meat). This combination provides a potent system for making very detailed descriptions of information. OWL also includes restrictions that refer to cardinalities—that is, referring to the number of distinct values for a particular property some individual has. Reasoning with cardinalities in OWL is surprisingly subtle. Perhaps one shouldn’t be surprised that it is difficult to count how many distinct things there are when one thing might have more than one name (i.e., more than one URI), and one never knows when someone might tell about a new thing one didn’t know about before. These are the main reasons why cardinality inferencing in OWL is quite conservative in the conclusions it can draw.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapter12Counting2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10012-3","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"249-278","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 12 - Counting and sets in OWL","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100123"},{"id":"allemangChapter13Ontologies2011","abstract":"This chapter discusses three ontologies: Good Relations, Quantities/Units/Dimensions/Types (QUDT), and OBO Foundry. These ontologies cover the spectrum from ontologies that include almost no data at all (Good Relations) to ontologies that include very large amounts of richly interconnected data (OBO). They all supply the basic capabilities of the Semantic Web by sharing information in a coherent way across multiple systems. Good Relations is the smallest of the ontologies described in the chapter. Its main goal in the Semantic Web is to provide a framework in which information can be shared—a vocabulary that different suppliers can use to describe their offerings. The data in Good Relations aren’t in the ontology at all; it is distributed across the Web. OBO Foundry ontologies, in contrast, are larger, and include large amounts of data about biology, medicine, life sciences, etc. There are complex questions that can be answered using OBO as a data resource. QUDT sits in the middle; it contains a good deal of data, but its main purpose is to provide connection between other data sets; two data sources that both use Good Relations might still fail to be interoperable because of mismatch of units; QUDT provides enhanced interoperability in these cases. All three of these ontologies play the basic role in the Semantic Web of providing globally unambiguous names for standard entities—they differ only in the details of how these relationships can be used.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapter13Ontologies2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10013-5","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"279-305","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 13 - Ontologies on the Web—putting it all together","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100135"},{"id":"allemangChapter14Good2011","abstract":"The basic assumptions behind the Semantic Web—the AAA, Open World, and Nonunique Naming assumptions—place very specific restrictions on the modeling language. The structure of RDF is in the form of statements with familiar grammatical constructs like subject, predicate, and object. The structure of Web Ontology Language (OWL) includes familiar concepts like class, subClassOf, and property. But the meaning of a model is given by the inference rules of OWL, which incorporate the assumptions of the Semantic Web. One can tell if one has built a useful model, one that conforms to these assumptions, by making sure that the inferences it supports are useful and meaningful. According to the AAA slogan, one cannot say that any of the practices in the chapter are “errors” because Anyone can say Anything about Any topic. All of these models are valid expressions in RDF/OWL. However, they are erroneous in the sense that they do not accomplish what the modeler intended by creating them. In each case, the mismatch can be revealed through careful examination of the inferences that the model entails. There are occasions where these modeling constructs are insufficient, and more advanced capabilities are required. Many of these have been included in version 2 of the OWL standard. This chapter outlines some fundamental new capabilities. It provides some of the background one will need to search through the OWL standard documents to explore its rich landscape.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapter14Good2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10014-7","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"307-324","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 14 - Good and bad modeling practices","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100147"},{"id":"allemangChapter15Expert2011","abstract":"OWL 2 was designed to be fully backward compatible with version 1.0. That means that any valid OWL 1.0 model is also a valid model in OWL 2. But more importantly, it means that all the styles of modeling that we learned for OWL 1.0 are still valid for OWL 2. The new constructs in OWL 2 can be used in conjunction with all the models. OWL 2 includes a precise description of four subsets of the OWL language identified for various practical technological reasons, often having to do with how OWL relates to other technology. The four subsets are called OWL 2 EL, OWL 2 QL, OWL 2 RL, and OWL 2 DL. The chapter describes each subset and the rationale for why it has been identified and named. OWL should be considered a living language, growing in the context of the ways it is being used on the Web and in commerce. As shortcomings in the language are identified, the system grows to accommodate them. Sometimes that growth takes the form of additional constructs in the language (e.g., multipart properties), sometimes as connections to other systems (rules), and sometimes progress in a language comes in the form of specifying subsets of the language.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapter15Expert2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10015-9","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"325-333","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 15 - Expert modeling in OWL","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100159"},{"id":"allemangChapter16Conclusions2011","abstract":"The Semantic Web looks familiar to those accustomed to various sorts of knowledge modeling. The modeling structure that is examined does have a strong connection to a heritage of knowledge modeling languages. However, there is something new that has come along since the early days of expert systems and object-oriented programming; something that has had a far more revolutionizing effect on culture, business, commerce, education, and society than any expert system designer ever dreamed of. It is something so revolutionary that it is often compared in cultural significance to the invention of the printing press. That something new is the World Wide Web. The Semantic Web is the application of advanced technologies that have been used in the context of artificial intelligence, expert systems, and business rules execution in the context of a World Wide Web of information. The Semantic Web is not simply an application running on the Web somewhere; it is a part of the very infrastructure of the Web. It isn’t on the Web; it is the Web.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapter16Conclusions2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10016-0","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"335-338","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 16 - Conclusions","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100160"},{"id":"allemangChapterQueryingSemantic2011","abstract":"RDF provides a simple way to represent distributed data. The triple is the simplest way to represent a named connection between two things. But a representation of data is useless without some means of accessing that data. The standard way to access RDF data uses a query language called SPARQL. SPARQL stands for SPARQL Protocol And RDF Query Language. The SPARQL query language works closely with the structure of RDF itself. SPARQL query patterns are represented in a variant of Turtle. The queried RDF graph can be created from one kind of data or merged from many; in either case, SPARQL is the way to query it. This chapter gives examples of the SPARQL query language. Most of the examples are based on version 1.0 of the standard. SPARQL is a query language and shares many features with other query languages like XQUERY and SQL. But it differs from each of these query languages in important ways. SPARQL queries can be used to fetch information (like SQL queries) or to transform a graph into a new form (like rules). Both forms use the same notion of graph pattern to specify the desired information.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapterQueryingSemantic2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10005-6","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"61-112","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 5 - Querying the Semantic Web—SPARQL","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100056"},{"id":"allemangChapterRDFBasis2011","abstract":"RDF is a system for modeling data. It gives up in compactness what it gains in flexibility. Every relationship between any two data elements is explicitly represented, allowing for a very simple model of merging data. A relationship is either present or it is not. Merging data is thus reduced to a simple matter of considering all such statements from all sources, together in a single place. The only challenge that remains in such a system is the challenge of identity. This problem is not unique to the RDF data model. The infrastructure of the Web itself has the same issue and has a standard solution: the URI. RDF borrows this solution. Since RDF is a Web language, a fundamental consideration is the distribution of information from multiple sources across the Web. On the Web, the AAA slogan holds: Anyone can say Anything about Any topic. RDF supports this slogan by allowing any data source to refer to resources in any namespace. Even a single triple can refer to resources in multiple namespaces. As a data model, RDF provides a clear specification of what has to happen to merge information from multiple sources.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapterRDFBasis2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10003-2","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"27-50","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 3 - RDF—The basis of the Semantic Web","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100032"},{"id":"allemangChapterRDFInferencing2011","abstract":"RDF provides a way to represent data so that information from multiple sources can be brought together and treated as if they came from a single source. But when we want to use that data, the differences in those sources comes out. The Semantic Web provides an approach to this problem in the form of modeling languages in which the relationship between data sources can be described. A modeling construct’s meaning is given by the pattern of inferences that can be drawn from it. Inferencing is the process by which new triples are systematically added to a graph based on patterns in existing triples. Information integration can be achieved by invoking inferencing before or during the query process; a query returns not only the asserted data but also inferred information. This inferred information can draw on more than one data source. It is seen that simple inferencing can provide value for data integration.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapterRDFInferencing2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10006-8","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"113-123","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 6 - RDF and inferencing","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100068"},{"id":"allemangChapterRDFSchema2011","abstract":"RDFS is the schema language for RDF. It describes constructs for types of objects, relating types to one another, properties that describe objects, and relationships between them. The Class system in RDFS includes a simple and elegant notion of inheritance, based on set inclusion; one class is a subclass of another means that instances of the one are also instances of the other. The RDFS language benefits from the distributed nature of RDF by being expressed in RDF itself. All schema information (classes, subclasses, subproperties, domain, range, etc.) is expressed in RDF triples. In particular, this makes schema information, as well as data, subject to the AAA slogan: Anyone can say Anything about Any topic—even about the schema. The semantics of RDFS is expressed through the mechanism of inferencing; that is, the meaning of any construct in RDFS is given by the inferences that can be inferred from it. RDFS also includes the constructs rdfs:domain and rdfs:range to describe the relationship between properties and classes. The meanings of these constructs are given by very simple rules, but these rules have subtle and far-reaching impact. The rules may be simple, but the statements are powerful. When used in careful combination, the constructs of RDFS are particularly effective at defining how differently structured information can be used together in a uniform way.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapterRDFSchema2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10007-X","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"125-152","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 7 - RDF schema","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B978012385965510007X"},{"id":"allemangChapterRDFSPlus2011","abstract":"RDF Schema (RDFS) provides a very limited set of inference capabilities that have considerable utility in a Semantic Web setting for merging information from multiple sources. This chapter takes the first step toward the Web Ontology Language (OWL) in which more elaborate constraints on how information is to be merged can be specified. A particular set of OWL constructs are selected to present at this stage. This set was selected to satisfy a number of goals that are presented in the chapter. This subset is of OWL RDFS-Plus, because a trend is seen among vendors of Semantic Web tools and Web applications designers for determining a subset of OWL that is at the same time useful and can be implemented quickly. This particular subset is identified via an informal poll of cutting-edge vendors. RDFS-Plus is expressed entirely in RDF.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapterRDFSPlus2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10008-1","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"153-185","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 8 - RDFS-Plus","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100081"},{"id":"allemangChapterSemanticModeling2011","abstract":"The Semantic Web is based on some radical notions of information sharing. These ideas provide for an environment in which information sharing can thrive and a network effect of knowledge synergy is possible. But this style of information gathering creates a chaotic landscape rife with confusion, disagreement, and conflict. However, modeling provides an answer to this problem. Modeling is the process of organizing information for community use. Modeling supports this in three ways: it provides a framework for human communication, a means for explaining conclusions, and a structure for managing varying viewpoints. In the context of the Semantic Web, modeling is an ongoing process. At any point in time, some knowledge is well structured and understood, and these structures can be represented in the Semantic Web modeling language. At the same time, other knowledge is still in the chaotic, discordant stage, where everyone is expressing himself differently. And typically, as different people provide their own opinions about any topic under the sun, the Web simultaneously contains organized and unorganized knowledge about the very same topic. The modeling activity is the activity of distilling communal knowledge out of a chaotic mess of information. This is nicely illustrated in the Pluto example.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapterSemanticModeling2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10002-0","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"13-25","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 2 - Semantic modeling","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100020"},{"id":"allemangChapterSemanticWeb2011","abstract":"The components described in this chapter—RDF parsers, serializers, stores, and query engines—are the components of a system that includes semantic models. The information represented in RDF is the building block that goes into making and using a semantic model. The model is represented in RDF. The semantic modeling languages of the W3C, RDFS, and OWL are built entirely in RDF, and they can be federated just like any other RDF data. As data expressed in RDF, semantic models are housed in the RDF store, along with all other data. Semantic models also include meta-data--data that help to organize other data. When the information is federated from multiple sources, the RDF data model allows one to represent all the data in a single, uniform way. A semantic model acts as a sort of glue between disparate, federated data sources, so they can be described as how they fit together. Anyone can contribute to the definition and mapping between information sources. In this way, not only can a federated, RDF-based, semantic application get its information from multiple sources, but it can even get the instructions on how to combine information from multiple sources. In this way, the Semantic Web really is a web of meaning, with multiple sources describing what the information on the Web means.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapterSemanticWeb2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10004-4","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"51-60","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 4 - Semantic Web application architecture","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100044"},{"id":"allemangChapterUsingRDFSPlus2011","abstract":"This chapter describes two example uses of the RDFS-Plus constructs. Both of these applications of RDFS-Plus make essential use of the constructs in RDFS-Plus, though often in quite different ways. These are real modeling applications built by groups who originally had no technology commitment to RDFS or OWL (though both were conceived as RDF applications). In both cases, the projects are about setting up an infrastructure for a particular Web community. The use of RDFS-Plus appears in the models that describe data in these communities, rather than in the everyday use in these communities. This chapter describes how modeling works in RDFS and OWL; hence the focus is on the community infrastructure of these projects. The first application is part of a major US government effort called Data.gov. Data.gov is an effort made by the US government to publish public information. There are hundreds of thousands of datasets in Data.gov, of which hundreds are made available in RDF, with many more being converted all the time. Data.gov is a great example of the data wilderness; the published data sets come from a wide variety of source formats and collection methodologies, resulting in idiosyncratic data representations. The second application is called FOAF, for “Friend of a Friend.” FOAF is a project dedicated to creating and using machine-readable homepages that describe people, the links between them, and the things they create and do. FOAF was originally based on RDF because of the inherently distributed and Web-like nature of the project requirements. As the project evolved, there was a need to describe the relationships between various resources in a formal way; this led it to RDFS and then on to RDFS-Plus. The chapter describes each of these efforts and shows their use of the RDFS-Plus.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapterUsingRDFSPlus2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10009-3","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"187-206","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 9 - Using RDFS-Plus in the wild","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100093"},{"id":"allemangChapterWhatSemantic2011","abstract":"In the world of web today, there is an unprecedented level of information. Without the model, there is an undifferentiated mass of data; there is no way to tell which data can or should interact with other data. The model itself has no significance without data to describe it. Put the two together, however, and you have a dynamic web of information, where data flow from one point to another in a principled, systematic fashion. This is the vision of the Semantic Web—an organized worldwide system where information flows from one place to another in a smooth but orderly way. This book is about Semantic Web and is for the working ontologist. The aim is to provide the tools necessary for working with it. It also shows how these tools can be used together to create semantic models, sometimes called ontologies that are understandable, useful, durable, and perhaps even beautiful.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"citation-key":"allemangChapterWhatSemantic2011","container-title":"Semantic Web for the Working Ontologist (Second Edition)","DOI":"10.1016/B978-0-12-385965-5.10001-9","editor":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"Jim"}],"event-place":"Boston","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011",1,1]]},"page":"1-12","publisher":"Morgan Kaufmann","publisher-place":"Boston","source":"ScienceDirect","title":"Chapter 1 - What is the Semantic Web?","type":"chapter","URL":"https://www.sciencedirect.com/science/article/pii/B9780123859655100019"},{"id":"allemangSemanticWebWorking2011","author":[{"family":"Allemang","given":"Dean"},{"family":"Hendler","given":"James A."}],"call-number":"TK5105.888 .A45 2011","citation-key":"allemangSemanticWebWorking2011","edition":"2nd ed","event-place":"Waltham, MA","ISBN":"978-0-12-385965-5","issued":{"date-parts":[["2011"]]},"note":"OCLC: ocn712780761","number-of-pages":"354","publisher":"Morgan Kaufmann/Elsevier","publisher-place":"Waltham, MA","source":"Library of Congress ISBN","title":"Semantic Web for the working ontologist: effective modeling in RDFS and OWL","title-short":"Semantic Web for the working ontologist","type":"book"},{"id":"AmazonGoWikipedia","accessed":{"date-parts":[["2023",2,13]]},"citation-key":"AmazonGoWikipedia","title":"Amazon Go – Wikipedia","type":"document","URL":"https://de.wikipedia.org/wiki/Amazon_Go"},{"id":"amazonneuptuneVerwalteteGraphdatenbankAmazon2024","abstract":"Amazon Neptune ist ein schneller, vollständig verwalteter Datenbankservice, der Graphen-Anwendungsfälle wie Identitätsgraphen, Wissensgraphen und Betrugserkennung unterstützt.","accessed":{"date-parts":[["2024",6,26]]},"author":[{"family":"AmazonNeuptune","given":""}],"citation-key":"amazonneuptuneVerwalteteGraphdatenbankAmazon2024","container-title":"Amazon Web Services, Inc.","issued":{"date-parts":[["2024"]]},"language":"de-DE","title":"Verwaltete Graphdatenbank – Amazon Neptune – AWS","type":"webpage","URL":"https://aws.amazon.com/de/neptune/"},{"id":"AmazonReviews23","accessed":{"date-parts":[["2024",10,4]]},"citation-key":"AmazonReviews23","title":"Amazon Reviews'23","type":"webpage","URL":"https://amazon-reviews-2023.github.io/"},{"id":"anglesSurveyGraphDatabase2008","abstract":"Graph database models can be defined as those in which data structures for the schema and instances are modeled as graphs or generalizations of them, and data manipulation is expressed by graph-oriented operations and type constructors. These models took off in the eighties and early nineties alongside object-oriented models. Their influence gradually died out with the emergence of other database models, in particular geographical, spatial, semistructured, and XML. Recently, the need to manage information with graph-like nature has reestablished the relevance of this area. The main objective of this survey is to present the work that has been conducted in the area of graph database modeling, concentrating on data structures, query languages, and integrity constraints.","accessed":{"date-parts":[["2024",7,20]]},"author":[{"family":"Angles","given":"Renzo"},{"family":"Gutierrez","given":"Claudio"}],"citation-key":"anglesSurveyGraphDatabase2008","container-title":"ACM Comput. Surv.","DOI":"10.1145/1322432.1322433","ISSN":"0360-0300","issue":"1","issued":{"date-parts":[["2008",2,22]]},"page":"1:1–1:39","source":"ACM Digital Library","title":"Survey of graph database models","type":"article-journal","URL":"https://doi.org/10.1145/1322432.1322433","volume":"40"},{"id":"annieWhatWeRe2019","abstract":"Research shows shoppers aren't finding products in-store, and it's causing them to abandon purchases. Discover the solution to recovering that lost revenue.","accessed":{"date-parts":[["2022",12,6]]},"author":[{"literal":"annie"}],"citation-key":"annieWhatWeRe2019","issued":{"date-parts":[["2019",10]]},"title":"What We're All Missing: Unfound Products = Lost Revenue","title-short":"What We're All Missing","type":"document","URL":"https://www.oriient.me/unfound-products-lost-revenue/"},{"id":"annieWhatWeRe2019a","abstract":"Research shows shoppers aren't finding products in-store, and it's causing them to abandon purchases. Discover the solution to recovering that lost revenue.","accessed":{"date-parts":[["2022",12,6]]},"author":[{"family":"annie","given":""}],"citation-key":"annieWhatWeRe2019a","container-title":"Oriient","issued":{"date-parts":[["2019",10,16]]},"title":"What We're All Missing: Unfound Products = Lost Revenue","title-short":"What We're All Missing","type":"webpage","URL":"https://www.oriient.me/unfound-products-lost-revenue/"},{"id":"AnyDaggerSpammer","abstract":"This build utilizes a quick recharging attack chain composed of fast-activating attacks in order to trigger on hit effects, such as Strength of Honor, Mark of Pain, or Barbs, as frequently as possible. Optional skills: \"Save Yourselves!\"FactionsWarrior. Kurzick rank \"Save Yourselves...","accessed":{"date-parts":[["2023",2,1]]},"citation-key":"AnyDaggerSpammer","container-title":"PvXwiki","language":"en","title":"A/any Dagger Spammer","type":"webpage","URL":"https://gwpvx.fandom.com/wiki/Build:A/any_Dagger_Spammer"},{"id":"arangodbHome","abstract":"ArangoDB is the leading multi-model database for high-performance applications. Try it now for flexible data modeling and efficient querying.","accessed":{"date-parts":[["2024",6,26]]},"author":[{"family":"Arangodb","given":""}],"citation-key":"arangodbHome","container-title":"ArangoDB","language":"en-US","title":"Home","type":"webpage","URL":"https://arangodb.com/"},{"id":"arenas-guerreroMorphKGCScalableKnowledge2024","abstract":"Knowledge graphs are often constructed from heterogeneous data sources, using declarative rules that map them to a target ontology and materializing them into RDF. When these data sources are large, the materialization of the entire knowledge graph may be computationally expensive and not suitable for those cases where a rapid materialization is required. In this work, we propose an approach to overcome this limitation, based on the novel concept of mapping partitions. Mapping partitions are defined as groups of mapping rules that generate disjoint subsets of the knowledge graph. Each of these groups can be processed separately, reducing the total amount of memory and execution time required by the materialization process. We have included this optimization in our materialization engine Morph-KGC, and we have evaluated it over three different benchmarks. Our experimental results show that, compared with state-of-the-art techniques, the use of mapping partitions in Morph-KGC presents the following advantages: (i) it decreases significantly the time required for materialization, (ii) it reduces the maximum peak of memory used, and (iii) it scales to data sizes that other engines are not capable of processing currently.","accessed":{"date-parts":[["2024",3,19]]},"author":[{"family":"Arenas-Guerrero","given":"Julián"},{"family":"Chaves-Fraga","given":"David"},{"family":"Toledo","given":"Jhon"},{"family":"Pérez","given":"María S."},{"family":"Corcho","given":"Oscar"}],"citation-key":"arenas-guerreroMorphKGCScalableKnowledge2024","container-title":"Semantic Web","container-title-short":"SW","DOI":"10.3233/SW-223135","editor":[{"family":"Demidova","given":"Elena"}],"ISSN":"22104968, 15700844","issue":"1","issued":{"date-parts":[["2024",1,12]]},"page":"1-20","source":"DOI.org (Crossref)","title":"Morph-KGC: Scalable knowledge graph materialization with mapping partitions","title-short":"Morph-KGC","type":"article-journal","URL":"https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-223135","volume":"15"},{"id":"arenas-guerreroRMLFNMLModulePython2024","abstract":"The RML mapping language declares schema transformations to map heterogeneous data into knowledge graphs. Although the schema transformations provided by RML are sufficient for simple use cases, large realworld ones typically involve diverse data and require complex computations. User-defined functions provide the flexibility to enable the creation and application of knowledge graphs for these use cases and also allow reusing existing software packages for data processing in multiple domains. In this work, we present an implementation for data transformations in Morph-KGC. This implementation has the following benefits: (i) it conforms to RML-FNML, the standard module for data transformations in RML, (ii) it can handle user-defined functions written in Python, a widely used programming language for data processing, (iii) it includes support for YARRRML, a user-friendly syntax of RML to maximize usability. The implementation is currently being used by BASF, a large multinational chemical company, to semantically integrate its industrial data in a maintainable and reproducible manner.","accessed":{"date-parts":[["2024",7,29]]},"author":[{"family":"Arenas-Guerrero","given":"Julián"},{"family":"Espinoza-Arias","given":"Paola"},{"family":"Bernabé-Diaz","given":"José Antonio"},{"family":"Deshmukh","given":"Prashant"},{"family":"Sánchez-Fernández","given":"José Luis"},{"family":"Corcho","given":"Oscar"}],"citation-key":"arenas-guerreroRMLFNMLModulePython2024","container-title":"SoftwareX","container-title-short":"SoftwareX","DOI":"10.1016/j.softx.2024.101709","ISSN":"23527110","issued":{"date-parts":[["2024",5]]},"language":"en","page":"101709","source":"DOI.org (Crossref)","title":"An RML-FNML module for Python user-defined functions in Morph-KGC","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S2352711024000803","volume":"26"},{"id":"arenas2024morph","author":[{"family":"Arenas-Guerrero","given":"Julián"},{"family":"Chaves-Fraga","given":"David"},{"family":"Toledo","given":"Jhon"},{"family":"Pérez","given":"María S."},{"family":"Corcho","given":"Oscar"}],"citation-key":"arenas2024morph","container-title":"Semantic Web","DOI":"10.3233/SW-223135","ISSN":"2210-4968","issue":"1","issued":{"date-parts":[["2024"]]},"page":"1-20","publisher":"IOS Press","title":"Morph-KGC: Scalable knowledge graph materialization with mapping partitions","type":"article-journal","volume":"15"},{"id":"ArmorCalculationGuild","accessed":{"date-parts":[["2023",2,5]]},"citation-key":"ArmorCalculationGuild","title":"Armor calculation - Guild Wars Wiki (GWW)","type":"webpage","URL":"https://wiki.guildwars.com/wiki/Armor_calculation"},{"id":"ArmorGuildWars","accessed":{"date-parts":[["2023",2,5]]},"citation-key":"ArmorGuildWars","title":"Armor - Guild Wars Wiki (GWW)","type":"webpage","URL":"https://wiki.guildwars.com/wiki/Armor"},{"id":"ArmorRatingGuild","accessed":{"date-parts":[["2023",2,5]]},"citation-key":"ArmorRatingGuild","title":"Armor rating - Guild Wars Wiki (GWW)","type":"webpage","URL":"https://wiki.guildwars.com/wiki/Armor_rating"},{"id":"asturianoVasturianoReactforcegraph2024","abstract":"React component for 2D, 3D, VR and AR force directed graphs","accessed":{"date-parts":[["2024",8,3]]},"author":[{"family":"Asturiano","given":"Vasco"}],"citation-key":"asturianoVasturianoReactforcegraph2024","genre":"HTML","issued":{"date-parts":[["2024",7,31]]},"license":"MIT","original-date":{"date-parts":[["2018",3,14]]},"source":"GitHub","title":"vasturiano/react-force-graph","type":"software","URL":"https://github.com/vasturiano/react-force-graph"},{"id":"auerImprovingAccessScientific2020","abstract":"The transfer of knowledge has not changed fundamentally for many hundreds of years: It is usually document-based-formerly printed on paper as a classic essay and nowadays as PDF. With around 2.5 million new research contributions every year, researchers drown in a flood of pseudo-digitized PDF publications. As a result research is seriously weakened. In this article, we argue for representing scholarly contributions in a structured and semantic way as a knowledge graph. The advantage is that information represented in a knowledge graph is readable by machines and humans. As an example, we give an overview on the Open Research Knowledge Graph (ORKG), a service implementing this approach. For creating the knowledge graph representation, we rely on a mixture of manual (crowd/expert sourcing) and (semi-)automated techniques. Only with such a combination of human and machine intelligence, we can achieve the required quality of the representation to allow for novel exploration and assistance services for researchers. As a result, a scholarly knowledge graph such as the ORKG can be used to give a condensed overview on the state-of-the-art addressing a particular research quest, for example as a tabular comparison of contributions according to various characteristics of the approaches. Further possible intuitive access interfaces to such scholarly knowledge graphs include domain-specific (chart) visualizations or answering of natural language questions.","accessed":{"date-parts":[["2024",6,17]]},"author":[{"family":"Auer","given":"Sören"},{"family":"Oelen","given":"Allard"},{"family":"Haris","given":"Muhammad"},{"family":"Stocker","given":"Markus"},{"family":"D’Souza","given":"Jennifer"},{"family":"Farfar","given":"Kheir Eddine"},{"family":"Vogt","given":"Lars"},{"family":"Prinz","given":"Manuel"},{"family":"Wiens","given":"Vitalis"},{"family":"Jaradeh","given":"Mohamad Yaser"}],"citation-key":"auerImprovingAccessScientific2020","container-title":"Bibliothek Forschung und Praxis","DOI":"10.1515/bfp-2020-2042","ISSN":"1865-7648","issue":"3","issued":{"date-parts":[["2020",12,20]]},"language":"en","license":"De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.","page":"516-529","publisher":"De Gruyter","source":"www.degruyter.com","title":"Improving Access to Scientific Literature with Knowledge Graphs","type":"article-journal","URL":"https://www.degruyter.com/document/doi/10.1515/bfp-2020-2042/html","volume":"44"},{"id":"AutomatedSelectionMultiple","accessed":{"date-parts":[["2024",6,27]]},"citation-key":"AutomatedSelectionMultiple","title":"Automated Selection of Multiple Datasets for Extension by Integration | Proceedings of the 30th ACM International Conference on Information & Knowledge Management","type":"webpage","URL":"https://dl.acm.org/doi/10.1145/3459637.3482322"},{"id":"aydinHumanVisualSystem","abstract":"At the receiving end of visual data are humans; thus it is only natural to take into account various properties and limitations of the human visual system while designing new image and video processing methods. In this dissertation we build multiple models of human vision with diﬀerent focuses and complexities, and demonstrate their use in computer graphics context.","author":[{"family":"Aydın","given":"Tunc Ozan"}],"citation-key":"aydinHumanVisualSystem","language":"en","page":"148","source":"Zotero","title":"Human Visual System Models in Computer Graphics","type":"article-journal"},{"id":"aylottExploratoryStudyGrocery1998","abstract":"Many factors affect the store patronage decision, e.g. location, service levels, pricing policies, merchandise assortment, store environment and store image, but very little research has considered stress as a determinant. This is despite the increase in dual income families and longer working hours which are making general shopping a more stressful activity for many families because of time pressure and lack of response by retailers. This exploratory research confirms grocery shopping to be stressful, but time pressure was mentioned as only one factor causing shopping stress; other factors included: crowd density, staff attitude and training, store layout/relocation, impulse purchasing pressure, location, product assortment, music, and lighting. The article concludes by proposing a shopping stress curve for future examination.","accessed":{"date-parts":[["2024",2,15]]},"author":[{"family":"Aylott","given":"Russell"},{"family":"Mitchell","given":"Vincent‐Wayne"}],"citation-key":"aylottExploratoryStudyGrocery1998","container-title":"International Journal of Retail & Distribution Management","DOI":"10.1108/09590559810237908","ISSN":"0959-0552","issue":"9","issued":{"date-parts":[["1998",10,1]]},"language":"en","page":"362-373","source":"DOI.org (Crossref)","title":"An exploratory study of grocery shopping stressors","type":"article-journal","URL":"https://www.emerald.com/insight/content/doi/10.1108/09590559810237908/full/html","volume":"26"},{"id":"bambachEntwicklungKlassifikatorsFur2023","abstract":"Das Erstellen von Produktdatensätzen durch einen Webcrawler basierend auf Produkt-\nwebseiten in Webshops stellt gerade dann eine sehr große Herausforderung dar, wenn\nausschließlich die Lebensmittel eines Webshops zur Erstellung eines Produktdatensat-\nzes betrachtet werden sollen. Hierfür ist es notwendig, eine Klassifizierung vorzuneh-\nmen, die Produkte zuverlässig als Lebensmittel oder Nicht-Lebensmittel identifiziert und\nauch als solche kategorisiert.\nDiese Arbeit beschäftigt sich mit der Entwicklung eines Klassifikators für Lebensmittel\nund Nicht-Lebensmittel in Webshops. Der Klassifikator basiert auf einem BERT-Modell,\neinem Modell des maschinellen Lernens, das sich besonders dazu eignet, Texte zu klas-\nsifizieren. Das in dieser Arbeit verwendete BERT-Modell wird für die Klassifikation von\nLebensmitteln und Nicht-Lebensmitteln auf 50.243 Produktdatensätzen von sieben\nWebshops trainiert und optimiert. Neben den Auswirkungen des zu klassifizierenden\nProduktattributs und dem verwendeten BERT-Modell für den Klassifikator werden au-\nßerdem auch die Auswirkungen verschiedener Hyperparameter wie der Anzahl an Trai-\nningsepochen, der Lernrate, sowie der Batchgröße untersucht. Der Klassifikator erzielt\nbei der Klassifizierung von Lebensmitteln und Nicht-Lebensmitteln eine Genauigkeit von\n97,6%.","author":[{"family":"Bambach","given":"Jan-Lukas"}],"citation-key":"bambachEntwicklungKlassifikatorsFur2023","genre":"Bachelorthesis","issued":{"date-parts":[["2023"]]},"number-of-pages":"50","publisher":"Hochschule Trier, Umwelt-Campus Birkenfeld","title":"Entwicklung eines Klassifikators für Food- und Non-Food-Produktweb- seiten in Webshops","type":"thesis","URL":"https://bibliothek.umwelt-campus.de/onlinedokumente/UPT/BT2023/BT%202023%20Bambach%20%20Jan-Lukas%20Entwicklung.pdf"},{"id":"bangashEnergyConsumptionEstimation","abstract":"Smartphone application (app) developers measure the energy consumption of their apps to ensure that they do not consume excessive energy. However, existing techniques require developers to generate and execute test cases on expensive, sophisticated hardware. To address these challenges, we propose a static-analysis approach that estimates the energy consumption of API usage in an app, eliminating the need for test case execution. To instantiate our approach, we have profiled the energy consumption of the Swift SQLite API operations. Given a Swift app, we first scan it for uses of SQLite. We then combine that information with the measured energy profile to compute Efactor, an estimate of the energy consumption of the API usage in an app. To evaluate the usability of E-factor, we have calculated the E-factor of 56 real-world iOS apps. We have also compared the E-factor of 16 versions and 11 methods from 3 of those apps to their hardware-based energy measurements. Our findings show that E-factor positively correlates with the hardware-based energy measurements, indicating that E-factor is a practical estimate to compare the energy consumption difference in API usage across different versions of an app. Developers may also use E-factor to identify excessive energy-consuming methods in their apps and focus on optimizing them. Our approach is most useful in an Integrated Development Environment (IDE) or Continuous Integration (CI) pipeline, where developers receive energy consumption insights within milliseconds of making a code modification.","author":[{"family":"Bangash","given":"Abdul Ali"},{"family":"Eng","given":"Kalvin"},{"family":"Jamal","given":"Qasim"},{"family":"Ali","given":"Karim"},{"family":"Hindle","given":"Abram"}],"citation-key":"bangashEnergyConsumptionEstimation","language":"en","source":"Zotero","title":"Energy Consumption Estimation of API-usage in Mobile Apps via Static Analysis","type":"article-journal"},{"id":"bangashEnergyConsumptionEstimation2023","abstract":"Smartphone application (app) developers measure the energy consumption of their apps to ensure that they do not consume excessive energy. However, existing techniques require developers to generate and execute test cases on expensive, sophisticated hardware. To address these challenges, we propose a static-analysis approach that estimates the energy consumption of API usage in an app, eliminating the need for test case execution. To instantiate our approach, we have profiled the energy consumption of the Swift SQLite API operations. Given a Swift app, we first scan it for uses of SQLite. We then combine that information with the measured energy profile to compute Efactor, an estimate of the energy consumption of the API usage in an app. To evaluate the usability of E-factor, we have calculated the E-factor of 56 real-world iOS apps. We have also compared the E-factor of 16 versions and 11 methods from 3 of those apps to their hardware-based energy measurements. Our findings show that E-factor positively correlates with the hardware-based energy measurements, indicating that E-factor is a practical estimate to compare the energy consumption difference in API usage across different versions of an app. Developers may also use E-factor to identify excessive energy-consuming methods in their apps and focus on optimizing them. Our approach is most useful in an Integrated Development Environment (IDE) or Continuous Integration (CI) pipeline, where developers receive energy consumption insights within milliseconds of making a code modification.","accessed":{"date-parts":[["2023",7,25]]},"author":[{"family":"Bangash","given":"Abdul Ali"},{"family":"Eng","given":"Kalvin"},{"family":"Jamal","given":"Qasim"},{"family":"Ali","given":"Karim"},{"family":"Hindle","given":"Abram"}],"citation-key":"bangashEnergyConsumptionEstimation2023","container-title":"2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","DOI":"10.1109/MSR59073.2023.00047","event-place":"Melbourne, Australia","event-title":"2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","ISBN":"979-8-3503-1184-6","issued":{"date-parts":[["2023",5]]},"language":"en","page":"272-283","publisher":"IEEE","publisher-place":"Melbourne, Australia","source":"DOI.org (Crossref)","title":"Energy Consumption Estimation of API-usage in Smartphone Apps via Static Analysis","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10174069/"},{"id":"barrasaNeosemanticsN10sNeo4j","abstract":"neosemantics (n10s): Neo4j RDF & Semantics toolkit - Neo4j Labs","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Barrasa","given":"Jesús"},{"family":"Cowley","given":"Adam"}],"citation-key":"barrasaNeosemanticsN10sNeo4j","container-title":"Neo4j Graph Data Platform","language":"en","title":"neosemantics (n10s): Neo4j RDF & Semantics toolkit - Neo4j Labs","title-short":"neosemantics (n10s)","type":"webpage","URL":"https://neo4j.com/labs/neosemantics/"},{"id":"BasicArmorGuild","accessed":{"date-parts":[["2023",2,1]]},"citation-key":"BasicArmorGuild","title":"Basic armor - Guild Wars Wiki (GWW)","type":"webpage","URL":"https://wiki.guildwars.com/wiki/Basic_armor"},{"id":"bastHybridApproachProduct2022","author":[{"family":"Bast","given":"Sebastian"},{"family":"Brosch","given":"Christoph"},{"family":"Krieger","given":"Rolf"}],"citation-key":"bastHybridApproachProduct2022","DOI":"10.5220/0011260200003269","issued":{"date-parts":[["2022",1]]},"page":"293–300","title":"A Hybrid Approach for Product Classification based on Image and Text Matching","type":"paper-conference"},{"id":"bastHybridApproachProduct2022a","author":[{"family":"Bast","given":"Sebastian"},{"family":"Brosch","given":"Christoph"},{"family":"Krieger","given":"Rolf"}],"citation-key":"bastHybridApproachProduct2022a","DOI":"10.5220/0011260200003269","issued":{"date-parts":[["2022",1]]},"page":"293–300","title":"A Hybrid Approach for Product Classification based on Image and Text Matching","type":"paper-conference"},{"id":"bastHybridApproachProduct2022b","author":[{"family":"Bast","given":"Sebastian"},{"family":"Brosch","given":"Christoph"},{"family":"Krieger","given":"Rolf"}],"citation-key":"bastHybridApproachProduct2022b","DOI":"10.5220/0011260200003269","issued":{"date-parts":[["2022",1,1]]},"page":"293-300","source":"ResearchGate","title":"A Hybrid Approach for Product Classification based on Image and Text Matching","type":"paper-conference"},{"id":"bastHybridApproachProduct2022c","author":[{"family":"Bast","given":"Sebastian"},{"family":"Brosch","given":"Christoph"},{"family":"Krieger","given":"Rolf"}],"citation-key":"bastHybridApproachProduct2022c","DOI":"10.5220/0011260200003269","issued":{"date-parts":[["2022",1,1]]},"page":"293-300","source":"ResearchGate","title":"A Hybrid Approach for Product Classification based on Image and Text Matching","type":"paper-conference"},{"id":"BBCFoodOntology","accessed":{"date-parts":[["2024",3,28]]},"citation-key":"BBCFoodOntology","title":"BBC Food ontology","type":"webpage","URL":"https://iptc.org/thirdparty/bbc-ontologies/fo.html"},{"id":"beckerSoftwareBasedEstimation2017","author":[{"family":"Becker","given":"Yannick"},{"family":"Naumann","given":"Stefan"}],"citation-key":"beckerSoftwareBasedEstimation2017","container-title":"From Science to Society: The Bridge provided by Environmental Informatics","issued":{"date-parts":[["2017"]]},"page":"69–73","publisher":"Shaker Verlag","title":"Software Based Estimation of Software Induced Energy Dissipation with powerstat","type":"paper-conference"},{"id":"beckerSoftwareBasedEstimation2017a","author":[{"family":"Becker","given":"Yannick"},{"family":"Naumann","given":"Stefan"}],"citation-key":"beckerSoftwareBasedEstimation2017a","issued":{"date-parts":[["2017",9,1]]},"source":"ResearchGate","title":"Software based Estimation of Software induced Energy Dissipation with powerstat","type":"paper-conference"},{"id":"belliniKnowledgeawareAutoencodersExplainable2018","abstract":"Recommender Systems have been widely used to help users in finding what they are looking for thus tackling the information overload problem. After several years of research and industrial findings looking after better algorithms to improve accuracy and diversity metrics, explanation services for recommendation are gaining momentum as a tool to provide a human-understandable feedback to results computed, in most of the cases, by black-box machine learning techniques. As a matter of fact, explanations may guarantee users satisfaction, trust, and loyalty in a system. In this paper, we evaluate how different information encoded in a Knowledge Graph are perceived by users when they are adopted to show them an explanation. More precisely, we compare how the use of categorical information, factual one or a mixture of them both in building explanations, affect explanatory criteria for a recommender system. Experimental results are validated through an A/B testing platform which uses a recommendation engine based on a Semantics-Aware Autoencoder to build users profiles which are in turn exploited to compute recommendation lists and to provide an explanation.","accessed":{"date-parts":[["2024",2,12]]},"author":[{"family":"Bellini","given":"Vito"},{"family":"Schiavone","given":"Angelo"},{"family":"Di Noia","given":"Tommaso"},{"family":"Ragone","given":"Azzurra"},{"family":"Di Sciascio","given":"Eugenio"}],"citation-key":"belliniKnowledgeawareAutoencodersExplainable2018","collection-title":"DLRS 2018","container-title":"Proceedings of the 3rd Workshop on Deep Learning for Recommender Systems","DOI":"10.1145/3270323.3270327","event-place":"New York, NY, USA","ISBN":"978-1-4503-6617-5","issued":{"date-parts":[["2018",10,6]]},"page":"24–31","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Knowledge-aware Autoencoders for Explainable Recommender Systems","type":"paper-conference","URL":"https://doi.org/10.1145/3270323.3270327"},{"id":"benhurFriendlyIntroductionSiamese2022","abstract":"You don’t always need a lot of data to train your model, learn how to create a model with a tiny number of images per class","accessed":{"date-parts":[["2022",12,16]]},"author":[{"family":"Benhur","given":"Sean"}],"citation-key":"benhurFriendlyIntroductionSiamese2022","issued":{"date-parts":[["2022",6]]},"language":"en","title":"A friendly Introduction to Siamese Networks","type":"document","URL":"https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942"},{"id":"benhurFriendlyIntroductionSiamese2022a","abstract":"You don’t always need a lot of data to train your model, learn how to create a model with a tiny number of images per class","accessed":{"date-parts":[["2022",12,16]]},"author":[{"family":"Benhur","given":"Sean"}],"citation-key":"benhurFriendlyIntroductionSiamese2022a","container-title":"Medium","issued":{"date-parts":[["2022",6,5]]},"language":"en","title":"A friendly Introduction to Siamese Networks","type":"webpage","URL":"https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942"},{"id":"bergImageClassificationMachine","abstract":"Machine learning is a growing area of artificial intelligence that is widely used in our world today. Training machine learning models requires knowledge and computing power. Machine Learning as a Service (MLaaS) tries to solve these issues. By storing the datasets and using virtual computing instances in the cloud, one can create machine learning models without writing a single line of code. When selecting an MLaaS platform to use, the natural question of which one to use arises. This thesis conducts controlled experiments to compare the image classification capabilities of Microsoft Azure ML, Amazon Web Services SageMaker, and Google Cloud Platform Vertex AI. The prediction accuracy, training time, and cost will be measured with three different datasets. Some subjective comments about the user experience while conducting these experiments will also be provided. The results of these experiments will be used to make recommendations as to which MLaaS platform to use depending on which metric is most suitable. This thesis found that Microsoft Azure ML performed best in terms of prediction accuracy, and training cost, across all datasets. Amazon Web Services SageMaker had the shortest time to train but performed the worst in terms of accuracy and had trouble with two of the three datasets. Google Cloud Platform Vertex AI did achieve the second-best prediction accuracy but was the most expensive platform by far as it had the largest time to train. It did, however, provide the smoothest user experience. Overall, Azure ML would be the platform of choice for image classification tasks after weighing together the results of the experiments as well as the subjective user experience.","author":[{"family":"Berg","given":"Gustav"},{"family":"Lincke","given":"Alisa"}],"citation-key":"bergImageClassificationMachine","language":"en","source":"Zotero","title":"Image Classification with Machine Learning as a Service","type":"article-journal"},{"id":"berners-leeLinkedDataDesign2006","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Berners-Lee","given":"Tim"}],"citation-key":"berners-leeLinkedDataDesign2006","issued":{"date-parts":[["2006"]]},"title":"Linked Data - Design Issues","type":"webpage","URL":"https://www.w3.org/DesignIssues/LinkedData"},{"id":"berners-leeReificationRDFN32004","accessed":{"date-parts":[["2024",6,26]]},"author":[{"family":"Berners-Lee","given":"Tim"}],"citation-key":"berners-leeReificationRDFN32004","issued":{"date-parts":[["2004"]]},"title":"Reification of RDF and N3 - Design Issues","type":"webpage","URL":"https://www.w3.org/DesignIssues/Reify.html"},{"id":"berners-leeWeavingWebOriginal1999","abstract":"Includes index","accessed":{"date-parts":[["2024",6,19]]},"author":[{"family":"Berners-Lee","given":"Tim"},{"family":"Fischetti","given":"Mark"}],"citation-key":"berners-leeWeavingWebOriginal1999","contributor":[{"literal":"Internet Archive"}],"ISBN":"978-0-06-251586-5 978-0-06-251587-2","issued":{"date-parts":[["1999"]]},"language":"eng","number-of-pages":"262","publisher":"[San Francisco] : HarperSanFrancisco","source":"Internet Archive","title":"Weaving the Web : the original design and ultimate destiny of the World Wide Web by its inventor","title-short":"Weaving the Web","type":"book","URL":"http://archive.org/details/isbn_9780062515872"},{"id":"biermannSAPECCERP","abstract":"SAP ECC ist ein Enterprise-Resource-Planning-System und dient als Software-Lösung zur Bearbeitung sämtlicher Geschäftsprozesse eines Unternehmens.","accessed":{"date-parts":[["2024",3,28]]},"author":[{"family":"Biermann","given":"Ingo"}],"citation-key":"biermannSAPECCERP","container-title":"mindsquare","language":"de-DE","title":"SAP ECC (ERP Central Component)","type":"post-weblog","URL":"https://mindsquare.de/knowhow/sap-ecc/"},{"id":"bizerBenchmarkingPerformanceStorage2009","abstract":"The SPARQL Query Language for RDF and the SPARQL Protocol for RDF are implemented by a growing number of storage systems and are used within enterprise and open web settings. As SPARQL is taken up by the community there is a growing need for benchmarks to compare the performance of storage systems that expose SPARQL endpoints via the SPARQL protocol. Such systems include native RDF stores, systems that map relational databases into RDF, and SPARQL wrappers around other kinds of data sources. This paper introduces the Berlin SPARQL Benchmark (BSBM) for comparing the performance of these systems across architectures. The benchmark is built around an e-commerce use case in which a set of products is offered by different vendors and consumers have posted reviews about products. The benchmark query mix illustrates the search and navigation pattern of a consumer looking for a product. After giving an overview about the design of the benchmark, the paper presents the results of an experiment comparing the performance of D2R Server, a relational database to RDF wrapper, with the performance of Sesame, Virtuoso, and Jena SDB, three popular RDF stores.","author":[{"family":"Bizer","given":"Christian"},{"family":"Schultz","given":"Andreas"}],"citation-key":"bizerBenchmarkingPerformanceStorage2009","issued":{"date-parts":[["2009"]]},"language":"en","source":"Zotero","title":"Benchmarking the Performance of Storage Systems that expose SPARQL Endpoints","type":"article-journal"},{"id":"bognerAntworttendenzenStandardisiertenUmfragenAntworttendenzen2014","accessed":{"date-parts":[["2024",2,14]]},"author":[{"family":"Bogner","given":"Kathrin"},{"family":"Landrock","given":"Uta"}],"citation-key":"bognerAntworttendenzenStandardisiertenUmfragenAntworttendenzen2014","container-title":"SDM Survey Guidelines","contributor":[{"literal":"GESIS-Leibniz-Institut Für Sozialwissenschaften"}],"DOI":"10.15465/SDM-SG_016","issued":{"date-parts":[["2014"]]},"language":"de","license":"Creative Commons License - Attribution-NonCommercial-NoDerivs 3.0 Germany (CC BY-NC-ND 3.0 DE), Creative Commons Lizenz - Namensnennung - Nicht-kommerziell - Keine Bearbeitung 3.0 Deutschland (CC BY-NC-ND 3.0 DE)","publisher":"SDM-Survey Guidelines (GESIS Leibniz Institute for the Social Sciences)","source":"DOI.org (Datacite)","title":"Antworttendenzen in standardisierten UmfragenAntworttendenzen in standardisierten Umfragen","type":"article-journal","URL":"http://www.gesis.org/gesis-survey-guidelines/instruments/fragebogenkonstruktion/antworttendenzen/","version":"1.0"},{"id":"bohnEstimatingFoodIngredient2022","abstract":"Having a specific understanding of the actual ingredient composition of products helps to calculate additional nutritional information, such as containing fatty and amino acids, minerals and vitamins, as well as to determine its environmental impacts. Unfortunately, producers rarely provide information on how much of each ingredient is in a product. Food manufacturers are, however, required to declare their products in terms of a label comprising an ingredient list (in descending order) and Big7 nutrient values. In this paper, we propose an automated approach for estimating ingredient contents in food products. First, we parse product labels to extract declared ingredients. Next, we exert mathematical formulations on the assumption that the weighted sum of Big7 ingredients as available from food compositional tables should resemble the product’s declared overall Big7 composition. We apply mathematical optimization techniques to find the best fitting ingredient composition estimate. We apply the proposed method to a dataset of 1804 food products spanning 11 product categories. We find that 76% of these products could be analyzed by our approach, and a composition within the prescribed nutrient tolerances could be calculated, using 20% of the allowed tolerances per Big7 ingredient on average. The remaining 24% of the food products could still be estimated when relaxing one or multiple nutrient tolerances. A study with known ingredient compositions shows that estimates are within a 0.9% difference of products’ actual recipes. Hence, the automated approach presented here allows for further analysis of large product quantities and provides possibilities for more intensive nutritional and ecological evaluations of food.","accessed":{"date-parts":[["2024",6,1]]},"author":[{"family":"Bohn","given":"Kristin"},{"family":"Amberg","given":"Michael"},{"family":"Meier","given":"Toni"},{"family":"Forner","given":"Frank"},{"family":"Stangl","given":"Gabriele I."},{"family":"Mäder","given":"Patrick"}],"citation-key":"bohnEstimatingFoodIngredient2022","container-title":"Journal of Food Composition and Analysis","container-title-short":"Journal of Food Composition and Analysis","DOI":"10.1016/j.jfca.2022.104508","ISSN":"0889-1575","issued":{"date-parts":[["2022",7,1]]},"page":"104508","source":"ScienceDirect","title":"Estimating food ingredient compositions based on mandatory product labeling","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0889157522001260","volume":"110"},{"id":"boydConvexOptimization2004","author":[{"family":"Boyd","given":"Stephen P."},{"family":"Vandenberghe","given":"Lieven"}],"call-number":"QA402.5 .B69 2004","citation-key":"boydConvexOptimization2004","event-place":"Cambridge, UK ; New York","ISBN":"978-0-521-83378-3","issued":{"date-parts":[["2004"]]},"language":"en","number-of-pages":"716","publisher":"Cambridge University Press","publisher-place":"Cambridge, UK ; New York","source":"Library of Congress ISBN","title":"Convex optimization","type":"book"},{"id":"brinkmannProductInformationExtraction2023","abstract":"Structured product data in the form of attribute/value pairs is the foundation of many e-commerce applications such as faceted product search, product comparison, and product recommendation. Product offers often only contain textual descriptions of the product attributes in the form of titles or free text. Hence, extracting attribute/value pairs from textual product descriptions is an essential enabler for e-commerce applications. In order to excel, state-of-theart product information extraction methods require large quantities of task-specific training data. The methods also struggle with generalizing to out-of-distribution attributes and attribute values that were not a part of the training data. Due to being pre-trained on huge amounts of text as well as due to emergent effects resulting from the model size, Large Language Models like ChatGPT have the potential to address both of these shortcomings. This paper explores the potential of ChatGPT for extracting attribute/value pairs from product descriptions. We experiment with different zeroshot and few-shot prompt designs. Our results show that ChatGPT achieves a performance similar to a pre-trained language model but requires much smaller amounts of training data and computation for fine-tuning.","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Brinkmann","given":"Alexander"},{"family":"Shraga","given":"Roee"},{"family":"Der","given":"Reng Chiz"},{"family":"Bizer","given":"Christian"}],"citation-key":"brinkmannProductInformationExtraction2023","issued":{"date-parts":[["2023",6,23]]},"language":"en","number":"arXiv:2306.14921","publisher":"arXiv","source":"arXiv.org","title":"Product Information Extraction using ChatGPT","type":"article","URL":"http://arxiv.org/abs/2306.14921"},{"id":"broschCreationEvaluationFood2023","abstract":": The enormous progress in the field of artificial intelligence (AI) enables retail companies to automate their processes and thus to save costs. Thereby, many AI-based automation approaches are based on machine learning and computer vision. The realization of such approaches requires high-quality training data. In this paper, we describe the creation process of an annotated dataset that contains 1,034 images of single food products, taken under studio conditions, annotated with 5 class labels and 30 object detection labels, which can be used for product recognition and classification tasks. We based all images and labels on standards presented by GS1, a global non-profit organisation. The objective of our work is to support the development of machine learning models in the retail domain and to provide a reference process for creating the necessary training data.","accessed":{"date-parts":[["2024",7,25]]},"author":[{"family":"Brosch","given":"Christoph"},{"family":"Bouwens","given":"Alexander"},{"family":"Bast","given":"Sebastian"},{"family":"Haab","given":"Swen"},{"family":"Krieger","given":"Rolf"}],"citation-key":"broschCreationEvaluationFood2023","container-title":"Proceedings of the 12th International Conference on Data Science, Technology and Applications","DOI":"10.5220/0012132400003541","event-place":"Rome, Italy","event-title":"12th International Conference on Data Science, Technology and Applications","ISBN":"978-989-758-664-4","issued":{"date-parts":[["2023"]]},"page":"488-495","publisher":"SCITEPRESS - Science and Technology Publications","publisher-place":"Rome, Italy","source":"Semantic Scholar","title":"Creation and Evaluation of a Food Product Image Dataset for Product Property Extraction:","title-short":"Creation and Evaluation of a Food Product Image Dataset for Product Property Extraction","type":"paper-conference","URL":"https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0012132400003541"},{"id":"broschCreationEvaluationFood2023a","abstract":": The enormous progress in the field of artificial intelligence (AI) enables retail companies to automate their processes and thus to save costs. Thereby, many AI-based automation approaches are based on machine learning and computer vision. The realization of such approaches requires high-quality training data. In this paper, we describe the creation process of an annotated dataset that contains 1,034 images of single food products, taken under studio conditions, annotated with 5 class labels and 30 object detection labels, which can be used for product recognition and classification tasks. We based all images and labels on standards presented by GS1, a global non-profit organisation. The objective of our work is to support the development of machine learning models in the retail domain and to provide a reference process for creating the necessary training data.","accessed":{"date-parts":[["2024",8,1]]},"author":[{"family":"Brosch","given":"Christoph"},{"family":"Bouwens","given":"Alexander"},{"family":"Bast","given":"Sebastian"},{"family":"Haab","given":"Swen"},{"family":"Krieger","given":"Rolf"}],"citation-key":"broschCreationEvaluationFood2023a","container-title":"Proceedings of the 12th International Conference on Data Science, Technology and Applications","DOI":"10.5220/0012132400003541","event-place":"Rome, Italy","event-title":"12th International Conference on Data Science, Technology and Applications","ISBN":"978-989-758-664-4","issued":{"date-parts":[["2023"]]},"page":"488-495","publisher":"SCITEPRESS - Science and Technology Publications","publisher-place":"Rome, Italy","source":"Semantic Scholar","title":"Creation and Evaluation of a Food Product Image Dataset for Product Property Extraction:","title-short":"Creation and Evaluation of a Food Product Image Dataset for Product Property Extraction","type":"paper-conference","URL":"https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0012132400003541"},{"id":"brummEntwicklungVerfahrenZur2024","author":[{"family":"Brumm","given":"Sian"}],"citation-key":"brummEntwicklungVerfahrenZur2024","genre":"Masterthesis","issued":{"date-parts":[["2024"]]},"publisher":"Hochschule Trier, Umwelt-Campus Birkenfeld","title":"Entwicklung von Verfahren zur Bewertung der Ähnlichkeit von Produkten im Lebensmittelsegment","type":"thesis"},{"id":"brummEntwicklungWebCrawlers2022","abstract":"Das Extrahieren von Produktinformationen aus Online-Shops mithilfe eines Web\nScrapers liefert Produktdaten, mit denen sich der gesamte Produktkatalog des Shops\nabbilden lässt. Um in diese Menge Struktur zu bringen und den Prozess der Datenge-\nwinnung zu regulieren wird ein Web Crawler entwickelt.\nDieser Crawler wurde unter der Einhaltung von Grundsätzen aus den Crawler Policies\nund im Sinne der Usability realisiert, mit dem Ziel einen ressourcenschonenden und\nflexiblen Crawl-Prozess durchführen zu können.\nDurch die Anbindung der NoSQL-Datenbank MongoDB wird ein Datenbestand aufge-\nbaut, der durch den Crawler verwaltet wird. Es wird geklärt, welche Anpassungen vor-\ngenommen werden müssen, um eine möglichst aktuelle und vollständige Datenbank zu\nunterhalten. Zentrale Fragen sind hierbei, wie eine geeignete Filterung nach Lebens-\nmittelprodukten durchgeführt werden kann, wann bestimmte Webseiten erneut besucht\nwerden sollen oder wie mit gelöschten Produktseiten umgegangen werden soll. Der\nentwickelte Web Crawler beinhaltet Umsetzungen und Konzepte, mit denen diese Fra-\ngen beantwortet werden sollen.","author":[{"family":"Brumm","given":"Sian"}],"citation-key":"brummEntwicklungWebCrawlers2022","genre":"Bachelorthesis","issued":{"date-parts":[["2022"]]},"publisher":"Hochschule Trier, Umwelt-Campus Birkenfeld","title":"Entwicklung eines Web Crawlers für Produktdaten mit Einbindung einer NoSQL-Datenbank","type":"thesis","URL":"https://bibliothek.umwelt-campus.de/onlinedokumente/UPT/BT2022/BT%202022%20Brumm%20%20Sian%20Entwicklung.pdf"},{"id":"changLiteratureDerivedReference2005","abstract":"The aim of the authors of this paper was to identify areas that would aid in developing a better understanding of the dynamics of a customer’s decision to shop online. To accomplish this, a review of the empirical studies on the antecedents of online shopping was performed. From an extensive literature search, a total of 45 relevant articles were identified. The factors that have been investigated in these studies were classified according to their similarity and patterns of their findings were identified and analyzed.","accessed":{"date-parts":[["2024",2,15]]},"author":[{"family":"Chang","given":"Man Kit"},{"family":"Cheung","given":"Waiman"},{"family":"Lai","given":"Vincent S."}],"citation-key":"changLiteratureDerivedReference2005","container-title":"Information & Management","container-title-short":"Information & Management","DOI":"10.1016/j.im.2004.02.006","ISSN":"0378-7206","issue":"4","issued":{"date-parts":[["2005",5,1]]},"page":"543-559","source":"ScienceDirect","title":"Literature derived reference models for the adoption of online shopping","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0378720604000515","volume":"42"},{"id":"chavesFindingMisplacedItems2010","abstract":"In retail, products are organized according to layout plans, so-called planograms. Compliance to planograms is important, since good product placement can significantly increase sales. Currently, retailers are about to implement RFID installations consisting of smart shelves and RFID-tagged items to support in-store logistics and processes. In principle, they can also use these installations to implement planogram compliance verification: Each antenna is supposed to detect all tagged items in one location of the planogram. But due to physical constraints, RFID tags can be identified by more than one RFID antenna. Thus, one cannot decide if an item carrying such a tag complies with the planogram. We propose a new method called RPCV which checks planogram compliance on large databases of items. It is based on the observation that the number of times an antenna identifies each item of a certain product type roughly follows a normal distribution. RPCV represents each item as a two-dimensional vector containing the number of readings both by the right antenna and by wrong ones according to the planogram. It clusters this data, separately for each product type. A cluster then is a set of correctly placed items or of misplaced ones. RPCV produces one order of magnitude less wrong predictions than current state of the art, and it requires less data to yield good predictions. A study with RFID-equipped goods and smart shelves shows that our approach is effective in realistic scenarios.","author":[{"family":"Chaves","given":"Leonardo"},{"family":"Buchmann","given":"Erik"},{"family":"Böhm","given":"Klemens"}],"citation-key":"chavesFindingMisplacedItems2010","DOI":"10.1145/1739041.1739102","issued":{"date-parts":[["2010",1]]},"page":"501–512","title":"Finding misplaced items in retail by clustering RFID data","type":"paper-conference"},{"id":"chavesFindingMisplacedItems2010a","abstract":"In retail, products are organized according to layout plans, so-called planograms. Compliance to planograms is important, since good product placement can significantly increase sales. Currently, retailers are about to implement RFID installations consisting of smart shelves and RFID-tagged items to support in-store logistics and processes. In principle, they can also use these installations to implement planogram compliance verification: Each antenna is supposed to detect all tagged items in one location of the planogram. But due to physical constraints, RFID tags can be identified by more than one RFID antenna. Thus, one cannot decide if an item carrying such a tag complies with the planogram. We propose a new method called RPCV which checks planogram compliance on large databases of items. It is based on the observation that the number of times an antenna identifies each item of a certain product type roughly follows a normal distribution. RPCV represents each item as a two-dimensional vector containing the number of readings both by the right antenna and by wrong ones according to the planogram. It clusters this data, separately for each product type. A cluster then is a set of correctly placed items or of misplaced ones. RPCV produces one order of magnitude less wrong predictions than current state of the art, and it requires less data to yield good predictions. A study with RFID-equipped goods and smart shelves shows that our approach is effective in realistic scenarios.","author":[{"family":"Chaves","given":"Leonardo"},{"family":"Buchmann","given":"Erik"},{"family":"Böhm","given":"Klemens"}],"citation-key":"chavesFindingMisplacedItems2010a","DOI":"10.1145/1739041.1739102","issued":{"date-parts":[["2010",1,1]]},"page":"501-512","source":"ResearchGate","title":"Finding misplaced items in retail by clustering RFID data","type":"paper-conference"},{"id":"chenGraphRepresentationLearning2020","abstract":"Research on graph representation learning has received a lot of attention in recent years since many data in real-world applications come in form of graphs. High-dimensional graph data are often in irregular form, which makes them more difficult to analyze than image/video/audio data defined on regular lattices. Various graph embedding techniques have been developed to convert the raw graph data into a low-dimensional vector representation while preserving the intrinsic graph properties. In this review, we first explain the graph embedding task and its challenges. Next, we review a wide range of graph embedding techniques with insights. Then, we evaluate several state-of-the-art methods against small and large datasets and compare their performance. Finally, potential applications and future directions are presented.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Chen","given":"Fenxiao"},{"family":"Wang","given":"Yuncheng"},{"family":"Wang","given":"Bin"},{"family":"Kuo","given":"C.-C. Jay"}],"citation-key":"chenGraphRepresentationLearning2020","container-title":"APSIPA Transactions on Signal and Information Processing","container-title-short":"SIP","DOI":"10.1017/ATSIP.2020.13","ISSN":"2048-7703, 2048-7703","issue":"1","issued":{"date-parts":[["2020"]]},"source":"arXiv.org","title":"Graph Representation Learning: A Survey","title-short":"Graph Representation Learning","type":"article-journal","URL":"http://arxiv.org/abs/1909.00958","volume":"9"},{"id":"chenUnitailDetectingReading2022","abstract":"To make full use of computer vision technology in stores, it is required to consider the actual needs that fit the characteristics of the retail scene. Pursuing this goal, we introduce the United Retail Datasets (Unitail), a large-scale benchmark of basic visual tasks on products that challenges algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped instances annotated, the Unitail offers a detection dataset to align product appearance better. Furthermore, it provides a gallery-style OCR dataset containing 1454 product categories, 30k text regions, and 21k transcriptions to enable robust reading on products and motivate enhanced product matching. Besides benchmarking the datasets using various start-of-the-arts, we customize a new detector for product detection and provide a simple OCR-based matching solution that verifies its effectiveness. The Unitail and its evaluation server is publicly available at https://unitedretail.github.io.","accessed":{"date-parts":[["2023",3,16]]},"author":[{"family":"Chen","given":"Fangyi"},{"family":"Zhang","given":"Han"},{"family":"Li","given":"Zaiwang"},{"family":"Dou","given":"Jiachen"},{"family":"Mo","given":"Shentong"},{"family":"Chen","given":"Hao"},{"family":"Zhang","given":"Yongxin"},{"family":"Ahmed","given":"Uzair"},{"family":"Zhu","given":"Chenchen"},{"family":"Savvides","given":"Marios"}],"citation-key":"chenUnitailDetectingReading2022","issued":{"date-parts":[["2022",7]]},"language":"en","publisher":"arXiv","title":"Unitail: Detecting, Reading, and Matching in Retail Scene","title-short":"Unitail","type":"document","URL":"http://arxiv.org/abs/2204.00298"},{"id":"chenUnitailDetectingReading2022a","abstract":"To make full use of computer vision technology in stores, it is required to consider the actual needs that fit the characteristics of the retail scene. Pursuing this goal, we introduce the United Retail Datasets (Unitail), a large-scale benchmark of basic visual tasks on products that challenges algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped instances annotated, the Unitail offers a detection dataset to align product appearance better. Furthermore, it provides a gallery-style OCR dataset containing 1454 product categories, 30k text regions, and 21k transcriptions to enable robust reading on products and motivate enhanced product matching. Besides benchmarking the datasets using various start-of-the-arts, we customize a new detector for product detection and provide a simple OCR-based matching solution that verifies its effectiveness. The Unitail and its evaluation server is publicly available at https://unitedretail.github.io.","accessed":{"date-parts":[["2023",3,16]]},"author":[{"family":"Chen","given":"Fangyi"},{"family":"Zhang","given":"Han"},{"family":"Li","given":"Zaiwang"},{"family":"Dou","given":"Jiachen"},{"family":"Mo","given":"Shentong"},{"family":"Chen","given":"Hao"},{"family":"Zhang","given":"Yongxin"},{"family":"Ahmed","given":"Uzair"},{"family":"Zhu","given":"Chenchen"},{"family":"Savvides","given":"Marios"}],"citation-key":"chenUnitailDetectingReading2022a","issued":{"date-parts":[["2022",7,20]]},"language":"en","number":"arXiv:2204.00298","publisher":"arXiv","source":"arXiv.org","title":"Unitail: Detecting, Reading, and Matching in Retail Scene","title-short":"Unitail","type":"article","URL":"http://arxiv.org/abs/2204.00298"},{"id":"chipRewePickGo2023","abstract":"Die Pick and Go Geschäfte von Rewe finden Sie zurzeit nur in sehr wenigen Städten. Mit dem innovativen Konzept prescht die Handelskette voran und es bleibt natürlich abzuwarten, wie die Pick & Go Läden von den Kunden angenommen werden.","accessed":{"date-parts":[["2023",2,13]]},"author":[{"family":"chip","given":""}],"citation-key":"chipRewePickGo2023","issued":{"date-parts":[["2023",2,8]]},"language":"de","title":"Rewe Pick&Go: So funktioniert der Rewe ohne Kasse","title-short":"Rewe Pick&Go","type":"webpage","URL":"https://praxistipps.chip.de/rewe-pickgo-so-funktioniert-der-rewe-ohne-kasse_155600"},{"id":"chiuKnowledgeGraphConstruction2021","abstract":"A knowledge graph (KG) describes entities and their relations in the form of nodes and edges. The property of KG’s modeling topological data has brought possible solutions for many tasks, including web search and information retrieval. Recently, e-retailers start utilizing KGs to optimize their work processes, aiming to develop efficient approaches to handle massive, diverse, and interconnected data about suppliers, products, and customers. Various approaches have been proposed, but extant knowledge seems to be rather scarce. Thus, this study summarizes how KGs have recently been constructed and applied in the e-retailing domain. We perform a systematic literature review and categorize KG literature into multiple aspects based on their construction methodologies, application areas, and roles in supporting retailing workflow. We also describe few challenges (e.g., product variety and domain complexity), which can provide insights for future research.","author":[{"family":"Chiu","given":"Billy"},{"family":"Kuen","given":"Wing"},{"family":"See-To","given":"Eric"},{"family":"Ngai","given":"E W T"}],"citation-key":"chiuKnowledgeGraphConstruction2021","issued":{"date-parts":[["2021"]]},"language":"en","source":"Zotero","title":"Knowledge Graph Construction and Applications in E-Retailing: A Review of Literature","type":"article-journal"},{"id":"christakisWhatDevelopersWant2016","accessed":{"date-parts":[["2022",5,14]]},"author":[{"family":"Christakis","given":"Maria"},{"family":"Bird","given":"Christian"}],"citation-key":"christakisWhatDevelopersWant2016","container-title":"Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering","DOI":"10.1145/2970276.2970347","event-place":"Singapore Singapore","event-title":"ASE'16: ACM/IEEE International Conference on Automated Software Engineering","ISBN":"978-1-4503-3845-5","issued":{"date-parts":[["2016",8,25]]},"language":"en","page":"332-343","publisher":"ACM","publisher-place":"Singapore Singapore","source":"DOI.org (Crossref)","title":"What developers want and need from program analysis: an empirical study","title-short":"What developers want and need from program analysis","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/2970276.2970347"},{"id":"christenDataMatchingConcepts2012","author":[{"family":"Christen","given":"Peter"}],"citation-key":"christenDataMatchingConcepts2012","collection-title":"Data-centric systems and applications","event-place":"Berlin Heidelberg","ISBN":"978-3-642-43001-5 978-3-642-31163-5","issued":{"date-parts":[["2012"]]},"language":"eng","number-of-pages":"270","publisher":"Springer","publisher-place":"Berlin Heidelberg","source":"K10plus ISBN","title":"Data matching: concepts and techniques for record linkage, entity resolution, and duplicate detection","title-short":"Data matching","type":"book"},{"id":"cioccaHowAssessImage2014","abstract":"Image quality assessment (IQA) is a multi-dimensional research problem and an active and evolving research area. This paper aims to provide an overview of the state of the art of the IQA methods, putting in evidence their applicability and limitations in different application domains. We outline the relationship between the image workflow chain and the IQA approaches reviewing the literature on IQA methods, classifying and summarizing the available metrics. We present general guidelines for three workflow chains in which IQA policies are required. The three workflow chains refer to: high-quality image archives, biometric system and consumer collections of personal photos. Finally, we illustrate a real case study referring to a printing workflow chain, where we suggest and actually evaluate the performance of a set of specific IQA methods.","author":[{"family":"Ciocca","given":"Gianluigi"},{"family":"Corchs","given":"Silvia"},{"family":"Gasparini","given":"Francesca"},{"family":"Schettini","given":"Raimondo"}],"citation-key":"cioccaHowAssessImage2014","container-title":"International Journal on Digital Libraries","container-title-short":"International Journal on Digital Libraries","DOI":"10.1007/s00799-014-0124-0","issued":{"date-parts":[["2014",11,1]]},"page":"1-25","source":"ResearchGate","title":"How to assess image quality within a workflow chain: an overview","title-short":"How to assess image quality within a workflow chain","type":"article-journal","volume":"15"},{"id":"cioccaQualityImages2016","abstract":"An image is the result of the optical imaging process which maps physical scene properties onto a two-dimensional luminance distribution; it encodes important and useful information about the geometry of the scene and the properties of the objects located within this scene [339, 611, 687].","accessed":{"date-parts":[["2022",5,10]]},"author":[{"family":"Ciocca","given":"Gianluigi"},{"family":"Corchs","given":"Silvia"},{"family":"Gasparini","given":"Francesca"},{"family":"Batini","given":"Carlo"},{"family":"Schettini","given":"Raimondo"}],"citation-key":"cioccaQualityImages2016","collection-title":"Data-Centric Systems and Applications","container-title":"Data and Information Quality: Dimensions, Principles and Techniques","DOI":"10.1007/978-3-319-24106-7_5","editor":[{"family":"Batini","given":"Carlo"},{"family":"Scannapieco","given":"Monica"}],"event-place":"Cham","ISBN":"978-3-319-24106-7","issued":{"date-parts":[["2016"]]},"language":"en","page":"113-135","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"Quality of Images","type":"chapter","URL":"https://doi.org/10.1007/978-3-319-24106-7_5"},{"id":"clarsenRevisitingContinuumResistance2021","abstract":"Background\nThe continuum of resistance model’s premise is that delayed respondents to a survey are more similar to non-respondents than early respondents are. For decades, survey researchers have applied this model in attempts to evaluate and adjust for non-response bias. Despite a recent resurgence in the model’s popularity, its value has only been assessed in one large online population health survey.\n\nMethods\nRespondents to the Norwegian Counties Public Health Survey in Hordaland, Norway, were divided into three groups: those who responded within 7 days of the initial email/SMS invitation (wave 1, n = 6950); those who responded after 8 to 14 days and 1 reminder (wave 2, n = 4950); and those who responded after 15 or more days and 2 reminders (wave 3, n = 4045). Logistic regression analyses were used to compare respondents’ age, sex and educational level between waves, as well as the prevalence of poor general health, life dissatisfaction, mental distress, chronic health problems, weekly alcohol consumption, monthly binge drinking, daily smoking, physical activity, low social support and receipt of a disability pension.\n\nResults\nThe overall response to the survey was 41.5%. Respondents in wave 1 were more likely to be older, female and more highly educated than those in waves 2 and 3. However, there were no substantial differences between waves for any health outcomes, with a maximal prevalence difference of 2.6% for weekly alcohol consumption (wave 1: 21.3%, wave 3: 18.7%).\n\nConclusions\nThere appeared to be a mild continuum of resistance for demographic variables. However, this was not reflected in health and related outcomes, which were uniformly similar across waves. The continuum of resistance model is unlikely to be useful to adjust for nonresponse bias in large online surveys of population health.","author":[{"family":"Clarsen","given":"Benjamin"},{"family":"Skogen","given":"Jens"},{"family":"Nilsen","given":"Thomas"},{"family":"Aarø","given":"Leif"}],"citation-key":"clarsenRevisitingContinuumResistance2021","container-title":"BMC Public Health","container-title-short":"BMC Public Health","DOI":"10.1186/s12889-021-10764-2","issued":{"date-parts":[["2021",4,15]]},"source":"ResearchGate","title":"Revisiting the continuum of resistance model in the digital age: a comparison of early and delayed respondents to the Norwegian counties public health survey","title-short":"Revisiting the continuum of resistance model in the digital age","type":"article-journal","volume":"21"},{"id":"cms-svetovanjeMacroSpaceManagement","accessed":{"date-parts":[["2023",3,12]]},"author":[{"family":"cms-svetovanje","given":""}],"citation-key":"cms-svetovanjeMacroSpaceManagement","title":"Macro Space Management","type":"webpage","URL":"http://www.cms-svetovanje.si/en/services-and-knowledge/macro-space-management"},{"id":"ConvexOptimizationBoyd","accessed":{"date-parts":[["2022",11,3]]},"citation-key":"ConvexOptimizationBoyd","title":"Convex Optimization – Boyd and Vandenberghe","type":"webpage","URL":"https://web.stanford.edu/~boyd/cvxbook/"},{"id":"CorchsGaspariniMarini","accessed":{"date-parts":[["2022",5,10]]},"citation-key":"CorchsGaspariniMarini","title":"Corchs S, Gasparini F, Marini F, Schettini R (2012) A sharpness measure on automatically selected edge segments. Image Quality and System Performance - Google Suche","type":"webpage","URL":"https://www.google.com/search?client=firefox-b-d&q=Corchs+S%2C+Gasparini+F%2C+Marini+F%2C+Schettini+R+%282012%29+A+sharpness+measure+on+automatically+selected+edge+segments.+Image+Quality+and+System+Performance+"},{"id":"corchsSharpnessMeasureAutomatically2012","accessed":{"date-parts":[["2022",5,10]]},"author":[{"family":"Corchs","given":"Silvia"},{"family":"Gasparini","given":"Francesca"},{"family":"Marini","given":"Fabrizio"},{"family":"Schettini","given":"Raimondo"}],"citation-key":"corchsSharpnessMeasureAutomatically2012","DOI":"10.1117/12.908473","editor":[{"family":"Gaykema","given":"Frans"},{"family":"Burns","given":"Peter D."}],"event-place":"Burlingame, California, United States","event-title":"IS&T/SPIE Electronic Imaging","issued":{"date-parts":[["2012",1,22]]},"page":"82930A","publisher-place":"Burlingame, California, United States","source":"DOI.org (Crossref)","title":"A sharpness measure on automatically selected edge segments","type":"paper-conference","URL":"http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.908473"},{"id":"CriticalHitGuild","accessed":{"date-parts":[["2023",2,1]]},"citation-key":"CriticalHitGuild","language":"en","title":"Critical hit - Guild Wars Wiki (GWW)","type":"webpage","URL":"https://wiki.guildwars.com/wiki/Critical_hit"},{"id":"cucenMakingSenseData2022","accessed":{"date-parts":[["2024",7,27]]},"author":[{"family":"Cucen","given":"Ebru"}],"citation-key":"cucenMakingSenseData2022","container-title":"Making Sense of Data with RDF* vs. LPG","genre":"Blog","issued":{"date-parts":[["2022",1,31]]},"title":"Making Sense of Data with RDF* vs. LPG","type":"webpage","URL":"https://www.opencredo.com/blogs/making-sense-of-data-with-rdf-vs-lpg"},{"id":"cypherIntroductionCypherManual2024","abstract":"This section provides an introduction to the Cypher query language.","accessed":{"date-parts":[["2024",6,25]]},"author":[{"family":"Cypher","given":""}],"citation-key":"cypherIntroductionCypherManual2024","container-title":"Neo4j Graph Data Platform","issued":{"date-parts":[["2024"]]},"language":"en","title":"Introduction - Cypher Manual","type":"webpage","URL":"https://neo4j.com/docs/cypher-manual/5/introduction/"},{"id":"DataInformationQuality","accessed":{"date-parts":[["2022",5,10]]},"citation-key":"DataInformationQuality","language":"en","source":"link.springer.com","title":"Data and Information Quality","type":"book","URL":"https://link.springer.com/book/10.1007/978-3-319-24106-7"},{"id":"DataModelSchema","accessed":{"date-parts":[["2024",2,16]]},"citation-key":"DataModelSchema","title":"Data Model - schema.org","type":"webpage","URL":"https://schema.org/docs/datamodel.html"},{"id":"DbpediaExtractionframework2024","abstract":"The software used to extract structured data from Wikipedia","accessed":{"date-parts":[["2024",6,18]]},"citation-key":"DbpediaExtractionframework2024","genre":"Scala","issued":{"date-parts":[["2024",6,10]]},"original-date":{"date-parts":[["2013",2,7]]},"publisher":"DBpedia","source":"GitHub","title":"dbpedia/extraction-framework","type":"software","URL":"https://github.com/dbpedia/extraction-framework"},{"id":"dcmimetadatatermsDCMIMetadataTerms","abstract":"This document is an up-to-date specification of all metadata terms maintained by the Dublin Core Metadata Initiative, including properties, vocabulary encoding schemes, syntax encoding schemes, and classes.","accessed":{"date-parts":[["2024",6,5]]},"author":[{"family":"DCMI Metadata Terms","given":""}],"citation-key":"dcmimetadatatermsDCMIMetadataTerms","language":"en","title":"DCMI Metadata Terms","type":"webpage","URL":"https://www.dublincore.org/specifications/dublin-core/dcmi-terms/"},{"id":"deinhardtNachtsImSupermarkt","abstract":"Automatisierte Regalsysteme gibt es schon seit vielen Jahren. Doch im Einzelhandel war es bisher unmöglich, Regale automatisiert befüllen oder sortieren zu lassen. Das Regalbediensystem der Universität Stuttgart könnte das nun ändern.","accessed":{"date-parts":[["2023",1,9]]},"author":[{"family":"Deinhardt","given":"Christina"}],"citation-key":"deinhardtNachtsImSupermarkt","language":"de-DE","title":"Nachts im Supermarkt: Automatisiertes Befüllen von Regalen im Einzelhandel","title-short":"Nachts im Supermarkt","type":"document","URL":"https://www.elektroniknet.de/automation/automatisiertes-befuellen-von-regalen-im-einzelhandel.156869.html"},{"id":"deinhardtNachtsImSupermarkta","abstract":"Automatisierte Regalsysteme gibt es schon seit vielen Jahren. Doch im Einzelhandel war es bisher unmöglich, Regale automatisiert befüllen oder sortieren zu lassen. Das Regalbediensystem der Universität Stuttgart könnte das nun ändern.","accessed":{"date-parts":[["2023",1,9]]},"author":[{"family":"Deinhardt","given":"Christina"}],"citation-key":"deinhardtNachtsImSupermarkta","container-title":"Elektroniknet","language":"de-DE","title":"Nachts im Supermarkt: Automatisiertes Befüllen von Regalen im Einzelhandel","title-short":"Nachts im Supermarkt","type":"webpage","URL":"https://www.elektroniknet.de/automation/automatisiertes-befuellen-von-regalen-im-einzelhandel.156869.html"},{"id":"deivasigamaniRetailGraphWalmart2020","abstract":"Context","accessed":{"date-parts":[["2024",7,9]]},"author":[{"family":"Deivasigamani","given":"Karthik"}],"citation-key":"deivasigamaniRetailGraphWalmart2020","container-title":"Walmart Global Tech Blog","issued":{"date-parts":[["2020",3,13]]},"language":"en","title":"Retail Graph — Walmart’s Product Knowledge Graph","type":"post-weblog","URL":"https://medium.com/walmartglobaltech/retail-graph-walmarts-product-knowledge-graph-6ef7357963bc"},{"id":"deltakonnectRFIDBeiDecathlon2021","abstract":"Seit 2015 nutzt der Sport- und Outdoorartikelhändler Decathlon erfolgreich RFID-Technologie.","accessed":{"date-parts":[["2023",3,7]]},"author":[{"family":"Delta Konnect","given":""}],"citation-key":"deltakonnectRFIDBeiDecathlon2021","container-title":"Delta Konnect GmbH","issued":{"date-parts":[["2021",8,21]]},"language":"de-DE","title":"RFID bei Decathlon: Eine historische Erfolgsgeschichte","title-short":"RFID bei Decathlon","type":"post-weblog","URL":"https://deltakonnect.de/rfid-bei-decathlon-erfolgsgeschichte/"},{"id":"delvaRMLstarDeclarativeMapping2021","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Delva","given":"Thomas"},{"family":"Arenas-Guerrero","given":"Julián"},{"family":"Iglesias-Molina","given":"Ana"},{"family":"Corcho","given":"Oscar"},{"family":"Chaves-Fraga","given":"David"},{"family":"Dimou","given":"Anastasia"}],"citation-key":"delvaRMLstarDeclarativeMapping2021","container-title":"Proceedings of the ISWC 2021 Posters, Demos and Industry Tracks: From Novel Ideas to Industrial Practice (ISWC-Posters-Demos-Industry 2021)","event-title":"ISWC2021, the International Semantic Web Conference","ISSN":"1613-0073","issued":{"date-parts":[["2021"]]},"language":"eng","license":"Creative Commons Attribution 4.0 International Public License (CC-BY 4.0)","publisher":"CEUR","source":"biblio.ugent.be","title":"RML-star : a declarative mapping language for RDF-star generation","title-short":"RML-star","type":"paper-conference","URL":"http://hdl.handle.net/1854/LU-8724424","volume":"2980"},{"id":"demeesterIntegrationFunctionExecutions2017","accessed":{"date-parts":[["2024",7,29]]},"author":[{"family":"De Meester","given":"Ben"},{"family":"Dimou","given":"Anastasia"},{"family":"Kleedorfer","given":"Florian"}],"citation-key":"demeesterIntegrationFunctionExecutions2017","issued":{"date-parts":[["2017"]]},"title":"Integration of function executions within rml | FnO: The Function Ontology","type":"webpage","URL":"https://fno.io/rml/"},{"id":"demeesterOntologySemanticallyDeclare2016","abstract":"Applications built on top of the Semantic Web are emerging as a novel solution in diﬀerent areas, such as decision making and route planning. However, to connect results of these solutions – i.e., the semantically annotated data – with real-world applications, this semantic data needs to be connected to actionable events. A lot of work has been done (both semantically as non-semantically) to describe and deﬁne Web services, but there is still a gap on a more abstract level, i.e., describing interfaces independent of the technology used. In this paper, we present a data model, speciﬁcation, and ontology to semantically declare and describe functions independently of the used technology. This way, we can declare and use actionable events in semantic applications, without restricting ourselves to programming language-dependent implementations. The ontology allows for extensions, and is proposed as a possible solution for semantic applications in various domains.","accessed":{"date-parts":[["2024",7,29]]},"author":[{"family":"De Meester","given":"Ben"},{"family":"Dimou","given":"Anastasia"},{"family":"Verborgh","given":"Ruben"},{"family":"Mannens","given":"Erik"}],"citation-key":"demeesterOntologySemanticallyDeclare2016","container-title":"The Semantic Web","DOI":"10.1007/978-3-319-47602-5_10","editor":[{"family":"Sack","given":"Harald"},{"family":"Rizzo","given":"Giuseppe"},{"family":"Steinmetz","given":"Nadine"},{"family":"Mladenić","given":"Dunja"},{"family":"Auer","given":"Sören"},{"family":"Lange","given":"Christoph"}],"event-place":"Cham","ISBN":"978-3-319-47601-8 978-3-319-47602-5","issued":{"date-parts":[["2016"]]},"language":"en","page":"46-49","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"An Ontology to Semantically Declare and Describe Functions","type":"chapter","URL":"https://link.springer.com/10.1007/978-3-319-47602-5_10","volume":"9989"},{"id":"dengRecommenderSystemsBased2022","abstract":"As a pivotal tool to alleviate the information overload problem, recommender systems aim to predict user's preferred items from millions of candidates by analyzing observed user-item relations. As for alleviating the sparsity and cold start problems encountered by recommender systems, researchers resort to employing side information or knowledge in recommendation as a strategy for uncovering hidden (indirect) user-item relations, aiming to enrich observed information (or data) for recommendation. However, in the face of the high complexity and large scale of side information and knowledge, this strategy relies for efficient implementation on the scalability of recommendation models. Not until after the prevalence of machine learning did graph embedding techniques be a concentration, which can efficiently utilize complex and large-scale data. In light of that, equipping recommender systems with graph embedding techniques has been widely studied these years, appearing to outperform conventional recommendation implemented directly based on graph topological analysis. As the focus, this article retrospects graph embedding-based recommendation from embedding techniques for bipartite graphs, general graphs and knowledge graphs, and proposes a general design pipeline of that. In addition, after comparing several representative graph embedding-based recommendation models with the most common-used conventional recommendation models on simulations, this article manifests that the conventional models can overall outperform the graph embedding-based ones in predicting implicit user-item interactions, revealing the comparative weakness of graph embedding-based recommendation in these tasks. To foster future research, this article proposes suggestions on making a trade-off between graph embedding-based recommendation and conventional recommendation in different tasks, and puts forward open questions.","accessed":{"date-parts":[["2024",8,4]]},"author":[{"family":"Deng","given":"Yue"}],"citation-key":"dengRecommenderSystemsBased2022","container-title":"IEEE Access","container-title-short":"IEEE Access","DOI":"10.1109/ACCESS.2022.3174197","ISSN":"2169-3536","issued":{"date-parts":[["2022"]]},"page":"51587-51633","source":"arXiv.org","title":"Recommender systems based on graph embedding techniques: A comprehensive review","title-short":"Recommender systems based on graph embedding techniques","type":"article-journal","URL":"http://arxiv.org/abs/2109.09587","volume":"10"},{"id":"dingAutomatedConstructionThemespecific2024","abstract":"Despite widespread applications of knowledge graphs (KGs) in various tasks such as question answering and intelligent conversational systems, existing KGs face two major challenges: information granularity and deficiency in timeliness. These hinder considerably the retrieval and analysis of in-context, fine-grained, and up-to-date knowledge from KGs, particularly in highly specialized themes (e.g., specialized scientific research) and rapidly evolving contexts (e.g., breaking news or disaster tracking). To tackle such challenges, we propose a theme-specific knowledge graph (i.e., ThemeKG), a KG constructed from a theme-specific corpus, and design an unsupervised framework for ThemeKG construction (named TKGCon). The framework takes raw theme-specific corpus and generates a high-quality KG that includes salient entities and relations under the theme. Specifically, we start with an entity ontology of the theme from Wikipedia, based on which we then generate candidate relations by Large Language Models (LLMs) to construct a relation ontology. To parse the documents from the theme corpus, we first map the extracted entity pairs to the ontology and retrieve the candidate relations. Finally, we incorporate the context and ontology to consolidate the relations for entity pairs. We observe that directly prompting GPT-4 for theme-specific KG leads to inaccurate entities (such as \"two main types\" as one entity in the query result) and unclear (such as \"is\", \"has\") or wrong relations (such as \"have due to\", \"to start\"). In contrast, by constructing the theme-specific KG step by step, our model outperforms GPT-4 and could consistently identify accurate entities and relations. Experimental results also show that our framework excels in evaluations compared with various KG construction baselines.","accessed":{"date-parts":[["2024",9,27]]},"author":[{"family":"Ding","given":"Linyi"},{"family":"Zhou","given":"Sizhe"},{"family":"Xiao","given":"Jinfeng"},{"family":"Han","given":"Jiawei"}],"citation-key":"dingAutomatedConstructionThemespecific2024","DOI":"10.48550/arXiv.2404.19146","issued":{"date-parts":[["2024",4,29]]},"number":"arXiv:2404.19146","publisher":"arXiv","source":"arXiv.org","title":"Automated Construction of Theme-specific Knowledge Graphs","type":"article","URL":"http://arxiv.org/abs/2404.19146"},{"id":"dingAutomatedConstructionThemespecific2024a","abstract":"Despite widespread applications of knowledge graphs (KGs) in various tasks such as question answering and intelligent conversational systems, existing KGs face two major challenges: information granularity and deficiency in timeliness. These hinder considerably the retrieval and analysis of in-context, fine-grained, and up-to-date knowledge from KGs, particularly in highly specialized themes (e.g., specialized scientific research) and rapidly evolving contexts (e.g., breaking news or disaster tracking). To tackle such challenges, we propose a theme-specific knowledge graph (i.e., ThemeKG), a KG constructed from a theme-specific corpus, and design an unsupervised framework for ThemeKG construction (named TKGCon). The framework takes raw theme-specific corpus and generates a high-quality KG that includes salient entities and relations under the theme. Specifically, we start with an entity ontology of the theme from Wikipedia, based on which we then generate candidate relations by Large Language Models (LLMs) to construct a relation ontology. To parse the documents from the theme corpus, we first map the extracted entity pairs to the ontology and retrieve the candidate relations. Finally, we incorporate the context and ontology to consolidate the relations for entity pairs. We observe that directly prompting GPT-4 for theme-specific KG leads to inaccurate entities (such as \"two main types\" as one entity in the query result) and unclear (such as \"is\", \"has\") or wrong relations (such as \"have due to\", \"to start\"). In contrast, by constructing the theme-specific KG step by step, our model outperforms GPT-4 and could consistently identify accurate entities and relations. Experimental results also show that our framework excels in evaluations compared with various KG construction baselines.","accessed":{"date-parts":[["2024",9,20]]},"author":[{"family":"Ding","given":"Linyi"},{"family":"Zhou","given":"Sizhe"},{"family":"Xiao","given":"Jinfeng"},{"family":"Han","given":"Jiawei"}],"citation-key":"dingAutomatedConstructionThemespecific2024a","DOI":"10.48550/arXiv.2404.19146","issued":{"date-parts":[["2024",4,29]]},"number":"arXiv:2404.19146","publisher":"arXiv","source":"arXiv.org","title":"Automated Construction of Theme-specific Knowledge Graphs","type":"article","URL":"http://arxiv.org/abs/2404.19146"},{"id":"disneyHowChooseGraph2023","abstract":"If you’ve found this article, the chances are you: Need to build a powerful graph or graph analytics application Are debating which is the right datastore","accessed":{"date-parts":[["2024",6,26]]},"author":[{"family":"Disney","given":"Andrew"}],"citation-key":"disneyHowChooseGraph2023","container-title":"Cambridge Intelligence","issued":{"date-parts":[["2023",10,19]]},"language":"en","title":"How to choose a graph database: we compare 6 favorites","title-short":"How to choose a graph database","type":"webpage","URL":"https://cambridge-intelligence.com/choosing-graph-database/"},{"id":"dockerincDockerComposeOverview0100","abstract":"Learn how to use Docker Compose to define and run multi-container applications with this detailed introduction to the tool.","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Docker Inc","given":""}],"citation-key":"dockerincDockerComposeOverview0100","container-title":"Docker Documentation","issued":{"literal":"08:47:37 +0100 +0100"},"language":"en","title":"Docker Compose overview","type":"webpage","URL":"https://docs.docker.com/compose/"},{"id":"DogsVsCats","abstract":"Create an algorithm to distinguish dogs from cats","accessed":{"date-parts":[["2023",1,3]]},"citation-key":"DogsVsCats","language":"en","title":"Dogs vs. Cats","type":"webpage","URL":"https://kaggle.com/competitions/dogs-vs-cats"},{"id":"dongAutoKnowSelfDrivingKnowledge2020","abstract":"Can one build a knowledge graph (KG) for all products in the world? Knowledge graphs have firmly established themselves as valuable sources of information for search and question answering, and it is natural to wonder if a KG can contain information about products offered at online retail sites. There have been several successful examples of generic KGs, but organizing information about products poses many additional challenges, including sparsity and noise of structured data for products, complexity of the domain with millions of product types and thousands of attributes, heterogeneity across large number of categories, as well as large and constantly growing number of products.","accessed":{"date-parts":[["2024",2,25]]},"author":[{"family":"Dong","given":"Xin Luna"},{"family":"He","given":"Xiang"},{"family":"Kan","given":"Andrey"},{"family":"Li","given":"Xian"},{"family":"Liang","given":"Yan"},{"family":"Ma","given":"Jun"},{"family":"Xu","given":"Yifan Ethan"},{"family":"Zhang","given":"Chenwei"},{"family":"Zhao","given":"Tong"},{"family":"Saldana","given":"Gabriel Blanco"},{"family":"Deshpande","given":"Saurabh"},{"family":"Manduca","given":"Alexandre Michetti"},{"family":"Ren","given":"Jay"},{"family":"Singh","given":"Surender Pal"},{"family":"Xiao","given":"Fan"},{"family":"Chang","given":"Haw-Shiuan"},{"family":"Karamanolakis","given":"Giannis"},{"family":"Mao","given":"Yuning"},{"family":"Wang","given":"Yaqing"},{"family":"Faloutsos","given":"Christos"},{"family":"McCallum","given":"Andrew"},{"family":"Han","given":"Jiawei"}],"citation-key":"dongAutoKnowSelfDrivingKnowledge2020","container-title":"Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining","DOI":"10.1145/3394486.3403323","issued":{"date-parts":[["2020",8,23]]},"language":"en","page":"2724-2734","source":"arXiv.org","title":"AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types","title-short":"AutoKnow","type":"paper-conference","URL":"http://arxiv.org/abs/2006.13473"},{"id":"dongKnowledgebasedTrustEstimating2015","abstract":"The quality of web sources has been traditionally evaluated using exogenous signals such as the hyperlink structure of the graph. We propose a new approach that relies on endogenous signals, namely, the correctness of factual information provided by the source. A source that has few false facts is considered to be trustworthy.","accessed":{"date-parts":[["2024",6,27]]},"author":[{"family":"Dong","given":"Xin Luna"},{"family":"Gabrilovich","given":"Evgeniy"},{"family":"Murphy","given":"Kevin"},{"family":"Dang","given":"Van"},{"family":"Horn","given":"Wilko"},{"family":"Lugaresi","given":"Camillo"},{"family":"Sun","given":"Shaohua"},{"family":"Zhang","given":"Wei"}],"citation-key":"dongKnowledgebasedTrustEstimating2015","container-title":"Proceedings of the VLDB Endowment","container-title-short":"Proc. VLDB Endow.","DOI":"10.14778/2777598.2777603","ISSN":"2150-8097","issue":"9","issued":{"date-parts":[["2015",5]]},"language":"en","page":"938-949","source":"DOI.org (Crossref)","title":"Knowledge-based trust: estimating the trustworthiness of web sources","title-short":"Knowledge-based trust","type":"article-journal","URL":"https://dl.acm.org/doi/10.14778/2777598.2777603","volume":"8"},{"id":"dooleyFoodOnHarmonizedFood2018","abstract":"The construction of high capacity data sharing networks to support increasing government and commercial data exchange has highlighted a key roadblock: the content of existing Internet-connected information remains siloed due to a multiplicity of local languages and data dictionaries. This lack of a digital lingua franca is obvious in the domain of human food as materials travel from their wild or farm origin, through processing and distribution chains, to consumers. Well defined, hierarchical vocabulary, connected with logical relationships—in other words, an ontology—is urgently needed to help tackle data harmonization problems that span the domains of food security, safety, quality, production, distribution, and consumer health and convenience. FoodOn (http://foodon.org) is a consortium-driven project to build a comprehensive and easily accessible global farm-to-fork ontology about food, that accurately and consistently describes foods commonly known in cultures from around the world. FoodOn addresses food product terminology gaps and supports food traceability. Focusing on human and domesticated animal food description, FoodOn contains animal and plant food sources, food categories and products, and other facets like preservation processes, contact surfaces, and packaging. Much of FoodOn’s vocabulary comes from transforming LanguaL, a mature and popular food indexing thesaurus, into a World Wide Web Consortium (W3C) OWL Web Ontology Language-formatted vocabulary that provides system interoperability, quality control, and software-driven intelligence. FoodOn compliments other technologies facilitating food traceability, which is becoming critical in this age of increasing globalization of food networks.","accessed":{"date-parts":[["2024",7,4]]},"author":[{"family":"Dooley","given":"Damion M."},{"family":"Griffiths","given":"Emma J."},{"family":"Gosal","given":"Gurinder S."},{"family":"Buttigieg","given":"Pier L."},{"family":"Hoehndorf","given":"Robert"},{"family":"Lange","given":"Matthew C."},{"family":"Schriml","given":"Lynn M."},{"family":"Brinkman","given":"Fiona S. L."},{"family":"Hsiao","given":"William W. L."}],"citation-key":"dooleyFoodOnHarmonizedFood2018","container-title":"npj Science of Food","container-title-short":"npj Sci Food","DOI":"10.1038/s41538-018-0032-6","ISSN":"2396-8370","issue":"1","issued":{"date-parts":[["2018",12,18]]},"language":"en","license":"2018 The Author(s)","page":"23","publisher":"Nature Publishing Group","source":"www.nature.com","title":"FoodOn: a harmonized food ontology to increase global food traceability, quality control and data integration","title-short":"FoodOn","type":"article-journal","URL":"https://www.nature.com/articles/s41538-018-0032-6","volume":"2"},{"id":"dubincoreDublinCore","abstract":"DCMI has been publishing specifications, mostly related to the \"Dublin Core\" vocabularies, for more than twenty years. The full range of recommendations is collected here.","accessed":{"date-parts":[["2024",6,16]]},"author":[{"family":"Dubin Core","given":""}],"citation-key":"dubincoreDublinCore","language":"en","title":"Dublin Core;","type":"webpage","URL":"https://www.dublincore.org/specifications/dublin-core/"},{"id":"ehrlingerDefinitionKnowledgeGraphs2016","abstract":"Recently, the term knowledge graph has been used frequently in research and business, usually in close association with Semantic Web technologies, linked data, large-scale data analytics and cloud computing. Its popularity is clearly inﬂuenced by the introduction of Google’s Knowledge Graph in 2012, and since then the term has been widely used without a deﬁnition. A large variety of interpretations has hampered the evolution of a common understanding of knowledge graphs. Numerous research papers refer to Google’s Knowledge Graph, although no oﬃcial documentation about the used methods exists. The prerequisite for widespread academic and commercial adoption of a concept or technology is a common understanding, based ideally on a deﬁnition that is free from ambiguity. We tackle this issue by discussing and deﬁning the term knowledge graph, considering its history and diversity in interpretations and use. Our goal is to propose a deﬁnition of knowledge graphs that serves as basis for discussions on this topic and contributes to a common vision.","author":[{"family":"Ehrlinger","given":"Lisa"},{"family":"Wöß","given":"Wolfram"}],"citation-key":"ehrlingerDefinitionKnowledgeGraphs2016","issued":{"date-parts":[["2016"]]},"language":"en","page":"4","source":"Zotero","title":"Towards a Deﬁnition of Knowledge Graphs","type":"article-journal"},{"id":"EinfuehrungFunktionsweiseMarkup","abstract":"Google verwendet Markup für strukturierte Daten, um Inhalte zu verstehen. In diesem Leitfaden erfährst du, wie strukturierte Daten funktionieren, welche Formate es gibt und wo du sie auf deiner Website platzieren kannst.","accessed":{"date-parts":[["2024",6,7]]},"citation-key":"EinfuehrungFunktionsweiseMarkup","language":"de","title":"Einführung in die Funktionsweise von Markup für strukturierte Daten | Google Search Central | Dokumentation | Google for Developers","type":"webpage","URL":"https://developers.google.com/search/docs/appearance/structured-data/intro-structured-data?hl=de#why"},{"id":"ElevatingMasterData","accessed":{"date-parts":[["2024",5,31]]},"citation-key":"ElevatingMasterData","title":"Elevating master data management in an organization | McKinsey","type":"webpage","URL":"https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/master-data-management-the-key-to-getting-more-from-your-data"},{"id":"EnergyEfficiencyProgramming2022","abstract":"The complete set of tools for energy consumption analysis of programming languages, using Computer Language Benchmark Game","accessed":{"date-parts":[["2022",10,31]]},"citation-key":"EnergyEfficiencyProgramming2022","genre":"C","issued":{"date-parts":[["2022",10,29]]},"license":"MIT","original-date":{"date-parts":[["2017",8,28]]},"publisher":"Green Software Lab","source":"GitHub","title":"Energy Efficiency in Programming Languages","type":"software","URL":"https://github.com/greensoftwarelab/Energy-Languages"},{"id":"eoffice24MaschinenDeoCalgonitFinish","abstract":"Beseitigt mühelos schlechte Gerüche in Ihrer Spülmaschine - inkl. Easyclip für die Befestigung in Ihrer Spülmaschine - Mit angenehmen Citrus-Duft - Ausreichend für 60 Spülgänge - Maschinen-Deo ...","accessed":{"date-parts":[["2023",3,5]]},"author":[{"family":"eOFFICE24","given":""}],"citation-key":"eoffice24MaschinenDeoCalgonitFinish","container-title":"eOFFICE24","language":"de-DE","title":"Maschinen-Deo Calgonit finish Citrus/Limone","type":"webpage","URL":"https://www.eoffice24.com/maschinen-deo-calgonit-finish-citrus-limone.html"},{"id":"ERPSoftwareMarktanteileAnbieter","abstract":"Die Statistik veranschaulicht die weltweiten Marktanteile der Software-Anbieter von Enterprise-Resource-Planning (ERP).","accessed":{"date-parts":[["2024",8,1]]},"citation-key":"ERPSoftwareMarktanteileAnbieter","container-title":"Statista","language":"de","title":"ERP-Software - Marktanteile der Anbieter weltweit 2017","type":"webpage","URL":"https://de.statista.com/statistik/daten/studie/262342/umfrage/marktanteile-der-anbieter-von-erp-software-weltweit/"},{"id":"ErsterAutonomerREWE","abstract":"Schnelles Einkaufen und kassenloses Bezahlen jetzt auch in Bayerns Landeshauptstadt möglich","accessed":{"date-parts":[["2023",2,13]]},"citation-key":"ErsterAutonomerREWE","language":"de","title":"Erster autonomer REWE Pick&Go-Markt in München eröffnet","type":"document","URL":"https://mediacenter.rewe.de/pressemitteilungen/rewe-pick-and-go-muenchen"},{"id":"etailmentTechnologieFunktioniertAmazon","abstract":"Amazon schafft mit seinem neuen Shop-Konzept Amazon Go den lästigsten Teil beim Einkauf ab: Das Bezahlen an der Kasse. Schlange stehen war gestern. Möglich macht das \"Sensor Fusion\". Doch was verbirgt sich hinter dem Zauberwort? Amazon macht daraus ein Geheimnis. Mit welcher Technik also revolutioniert Amazon den Einkauf im Supermarkt?","accessed":{"date-parts":[["2023",2,13]]},"author":[{"family":"etailment","given":""}],"citation-key":"etailmentTechnologieFunktioniertAmazon","container-title":"etailment.de","language":"de","title":"Technologie: So funktioniert Amazon Go: Die Technik hinter dem Zauberwort „Sensor Fusion“","title-short":"Technologie","type":"webpage","URL":"https://etailment.de/news/stories/Technologie-So-funktioniert-Amazon-Go-Die-Technik-hinter-dem-Zauberwort-Sensor-Fusion-20194"},{"id":"fahrmeirStatistik2016","accessed":{"date-parts":[["2023",1,3]]},"author":[{"family":"Fahrmeir","given":"Ludwig"},{"family":"Heumann","given":"Christian"},{"family":"Künstler","given":"Rita"},{"family":"Pigeot","given":"Iris"},{"family":"Tutz","given":"Gerhard"}],"citation-key":"fahrmeirStatistik2016","collection-title":"Springer-Lehrbuch","DOI":"10.1007/978-3-662-50372-0","event-place":"Berlin, Heidelberg","ISBN":"978-3-662-50371-3 978-3-662-50372-0","issued":{"date-parts":[["2016"]]},"publisher":"Springer","publisher-place":"Berlin, Heidelberg","source":"DOI.org (Crossref)","title":"Statistik","type":"book","URL":"http://link.springer.com/10.1007/978-3-662-50372-0"},{"id":"faltstromURNNamespaceDefinition1999","abstract":"This document lays out general definitions of and mechanisms for establishing URN \"namespaces\". This document specifies an Internet Best Current Practices for the Internet Community, and requests discussion and suggestions for improvements.","accessed":{"date-parts":[["2024",2,20]]},"author":[{"family":"Fältström","given":"Patrik"},{"family":"Iannella","given":"Renato"},{"family":"Gulik","given":"Dirk-Willem","dropping-particle":"van"},{"family":"Daigle","given":"Leslie"}],"citation-key":"faltstromURNNamespaceDefinition1999","DOI":"10.17487/RFC2611","genre":"Request for Comments","issued":{"date-parts":[["1999",6]]},"number":"RFC 2611","number-of-pages":"14","publisher":"Internet Engineering Task Force","source":"IETF","title":"URN Namespace Definition Mechanisms","type":"report","URL":"https://datatracker.ietf.org/doc/rfc2611"},{"id":"farberMicrosoftAcademicKnowledge2019","abstract":"In this paper, we present the Microsoft Academic Knowledge Graph (MAKG), a large RDF data set with over eight billion triples with information about scientific publications and related entities, such as authors, institutions, journals, and fields of study. The data set is licensed under the Open Data Commons Attribution License (ODC-By). By providing the data as RDF dump files as well as a data source in the Linked Open Data cloud with resolvable URIs and links to other data sources, we bring a vast amount of scholarly data to the Web of Data. Furthermore, we provide entity embeddings for all 210 million represented publications. We facilitate a number of use case scenarios, particularly in the field of digital libraries, such as (1) entity-centric exploration of papers, researchers, affiliations, etc.; (2) data integration tasks using RDF as a common data model and links to other data sources; and (3) data analysis and knowledge discovery of scholarly data.","author":[{"family":"Färber","given":"Michael"}],"citation-key":"farberMicrosoftAcademicKnowledge2019","container-title":"The Semantic Web – ISWC 2019","DOI":"10.1007/978-3-030-30796-7_8","editor":[{"family":"Ghidini","given":"Chiara"},{"family":"Hartig","given":"Olaf"},{"family":"Maleshkova","given":"Maria"},{"family":"Svátek","given":"Vojtěch"},{"family":"Cruz","given":"Isabel"},{"family":"Hogan","given":"Aidan"},{"family":"Song","given":"Jie"},{"family":"Lefrançois","given":"Maxime"},{"family":"Gandon","given":"Fabien"}],"event-place":"Cham","ISBN":"978-3-030-30796-7","issued":{"date-parts":[["2019"]]},"language":"en","page":"113-129","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"The Microsoft Academic Knowledge Graph: A Linked Data Source with 8 Billion Triples of Scholarly Data","title-short":"The Microsoft Academic Knowledge Graph","type":"paper-conference"},{"id":"ferramentaProgrammingLanguagesStatic2022","abstract":"Mobile devices, such as smartphones, have become an extension of our body for various reasons, mainly because of mobility, communication convenience, and an extensive \nrange of provided apps. However, being portable, they have several limitations, and the \nmost concerning for users is the limited battery life. Smartphone manufacturers are trying to address this problem by optimizing the hardware and the software, but the concern \nremains. Although it is well known that apps are one of the components of smartphones \nthat consume the most energy, most app developers do not use or even consider using \nstrategies to minimize their apps’ energy consumption. Creating a labeling system that \nclassifies apps based on their energy consumption is a possible solution to drive developers to be more conscious of their apps’ energy consumption. This work aims to develop a \ntechnique to compute a metric for app energy certification. The metric we propose to calculate is WCEC, which represents the energy consumed in the most extreme case of program execution. Typically, this analysis is used in embedded systems, where the energy \nconsumed by the apps must be rigorously determined to avoid inconveniences. Here we \nhave reused the fundamentals of the WCEC analysis to use for Android apps. We address \nour solution to the Android platform since it has the largest worldwide mobile operating \nsystem market share. To perform the WCEC analysis, we take advantage of static analysis \nand the IPET, the techniques commonly used in this context. We have also created a test \nscenario to illustrate a case where our tool can be used. This document aims to explain \nthe fundamentals present in our tool and describe its implementation details.","accessed":{"date-parts":[["2023",7,25]]},"author":[{"family":"Ferramenta","given":"Delcio Kelson Alves"}],"citation-key":"ferramentaProgrammingLanguagesStatic2022","genre":"masterThesis","issued":{"date-parts":[["2022",11,23]]},"language":"eng","license":"openAccess","note":"Accepted: 2023-02-20T16:42:56Z","source":"ubibliorum.ubi.pt","title":"Programming languages and static analysis techniques for software energy certification","type":"thesis","URL":"https://ubibliorum.ubi.pt/handle/10400.6/13107"},{"id":"flavioExtraktionWissenAus2024","accessed":{"date-parts":[["2024",5,8]]},"author":[{"family":"Flavio","given":"Horbach"}],"citation-key":"flavioExtraktionWissenAus2024","genre":"Masterthesis","issued":{"date-parts":[["2024"]]},"language":"de-DE","number-of-pages":"105","publisher":"Hochschule Trier, Umwelt-Campus Birkenfeld","title":"Extraktion von Wissen aus Produktbeschreibungen von Lebensmitteln mittels Natural Language Processing","type":"thesis","URL":"https://bibliothek.umwelt-campus.de/onlinedokumente/UPT/MT2024/MT%202024%20Horbach%20%20Flavio%20Extraktion.pdf"},{"id":"FOODON00002420","accessed":{"date-parts":[["2024",5,14]]},"citation-key":"FOODON00002420","title":"FOODON:00002420","type":"webpage","URL":"https://www.ebi.ac.uk/ols4/ontologies/foodon/properties/http%253A%252F%252Fpurl.obolibrary.org%252Fobo%252FFOODON_00002420?lang=en"},{"id":"FoodOntology","abstract":"A simple vocabulary for describing recipes, ingredients, menus and diets.","accessed":{"date-parts":[["2024",7,1]]},"citation-key":"FoodOntology","language":"en","title":"Food Ontology","type":"webpage","URL":"https://www.bbc.co.uk/ontologies/food-ontology/bbc.com/ontologies/food-ontology/"},{"id":"FoodOntologyVocabularyFood","accessed":{"date-parts":[["2024",3,28]]},"citation-key":"FoodOntologyVocabularyFood","title":"The FoodOntology Vocabulary for Food Products Description","type":"webpage","URL":"https://akswnc7.informatik.uni-leipzig.de/dstreitmatter/archivo/purl.org/foodontology/2020.06.10-203447/foodontology_type=generatedDocu.html"},{"id":"forgwordNoobQuestionsLeveling2018","accessed":{"date-parts":[["2023",2,1]]},"author":[{"family":"Forgword","given":""}],"citation-key":"forgwordNoobQuestionsLeveling2018","container-title":"r/GuildWars","genre":"Reddit Post","issued":{"date-parts":[["2018",12,27]]},"title":"Noob questions - Leveling Gear","type":"post","URL":"www.reddit.com/r/GuildWars/comments/a9yxil/noob_questions_leveling_gear/"},{"id":"franzenAntwortskalenStandardisiertenBefragungen","author":[{"family":"Franzen","given":"Axel"}],"citation-key":"franzenAntwortskalenStandardisiertenBefragungen","language":"de","source":"Zotero","title":"Antwortskalen in standardisierten Befragungen","type":"article-journal"},{"id":"freyEvaluationMetadataRepresentations2019","abstract":"The maintenance and use of metadata such as provenance and time-related information is of increasing importance in the Semantic Web, especially for Big Data applications that work on heterogeneous data from multiple sources and which require high data quality. In an RDF dataset, it is possible to store metadata alongside the actual RDF data and several possible metadata representation models have been proposed. However, there is still no in-depth comparative evaluation of the main representation alternatives on both the conceptual level and the implementation level using different graph backends. In order to help to close this gap, we introduce major use cases and requirements for storing and using diverse kinds of metadata. Based on these requirements, we perform a detailed comparison and benchmark study for different RDF-based metadata representations, including a new approach based on so-called companion properties. The benchmark evaluation considers two datasets and evaluates different representations for three popular RDF stores.","accessed":{"date-parts":[["2024",5,7]]},"author":[{"family":"Frey","given":"Johannes"},{"family":"Müller","given":"Kay"},{"family":"Hellmann","given":"Sebastian"},{"family":"Rahm","given":"Erhard"},{"family":"Vidal","given":"Maria-Esther"}],"citation-key":"freyEvaluationMetadataRepresentations2019","container-title":"Semantic Web","container-title-short":"SW","DOI":"10.3233/SW-180307","editor":[{"family":"Ngonga Ngomo","given":"Axel-Cyrille"},{"family":"Fundulaki","given":"Irini"},{"family":"Krithara","given":"Anastasia"},{"family":"Ngonga Ngomo","given":"Axel-Cyrille"},{"family":"Fundulaki","given":"Irini"},{"family":"Krithara","given":"Anastasia"}],"ISSN":"22104968, 15700844","issue":"2","issued":{"date-parts":[["2019",1,21]]},"language":"en","page":"205-229","source":"DOI.org (Crossref)","title":"Evaluation of metadata representations in RDF stores","type":"article-journal","URL":"https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-180307","volume":"10"},{"id":"frischWebScrapingZur2022","abstract":"Wie können vermeintlich unstrukturierte Daten von Produktseiten eines Online-Shops\ngezielt extrahiert und in ein strukturiertes Format übertragen werden? Im Rahmen die-\nser Arbeit soll diese Frage geklärt werden.\nDazu wurden zuerst 10 Online Shops als Referenz gesucht und genau analysiert. Da-\nbei galt es zu klären, wie Online-Shops aufgebaut sind. Wie kommt der Scraper von\nder Startseite zu den Produktseiten? Und wenn er auf der Produktseite angelangt ist,\nwie kann er die Daten extrahieren? Gibt es Standards oder Muster in den Produktsei-\nten, an denen sich der Scraper orientieren kann?\nBei der Analyse der Shops wurde klar, dass es in den (auf den ersten Blick) unstruktu-\nrierten Daten, doch viele standardisierte Muster und Strukturen gibt, anhand derer ein\neinheitliches Vorgehen entwickelt werden kann, mit dem der zu entwickelnde Scraper\ndie Informationen extrahieren kann.\nZudem soll geklärt werden, wie ein solcher Scraper entwickelt werden kann. Welche\nSchritte muss der Scraper in welcher Reihenfolge ausführen, um von der Startseite des\nOnline-Shops an alle Produktseiten zu kommen? Welche einzelnen Komponenten\nbraucht der Scraper? Diese Fragen sollen durch eine prototypische Umsetzung eines\nWeb-Scrapers mithilfe des Frameworks Scrapy geklärt werden.","author":[{"family":"Frisch","given":"Niclas Michael"}],"citation-key":"frischWebScrapingZur2022","genre":"ba","issued":{"date-parts":[["2022"]]},"publisher":"Hochschule Trier, Umwelt-Campus Birkenfeld","title":"Web Scraping zur Gewinnung von Produktinformationen im Lebensmittelhandel","type":"thesis","URL":"https://bibliothek.umwelt-campus.de/onlinedokumente/UPT/BT2022/BT%202022%20Frisch%20%20Niclas%20Web%20Scraping.pdf"},{"id":"frontoniRealTimeOut2014","abstract":"Out-of-shelf problem is important to solve for retail store since the absence of products on the shelf can lead to a significant reduction of shoppers and a consequent drop on sales. For this purpose, it is necessary to study and to introduce approaches able to establish the lack of products on the shelves and thereby promptly ensuring their repositioning. In this context, the paper investigates the use of artificial intelligence techniques in detecting the out-of-shelf products. Particularly, having sales data, ordering info and product assortment of the store available, we study the development of low cost shelf detector that is based on wireless sensor network, and that can automatically discover out-of-shelf situations on a daily basis for all the stores of a retail chain. The use of an automatic method for detecting products that are not available on the shelf based on sales data would offer an accurate view of the shelf availability, both to retailers and to product suppliers. The tool presented is the first being installed for a long time in a high number of stores and products demonstrating the ability to gather data from there and extract interesting insights. This paper aims to present the hardware infrastructure of an embedded sensor network devoted to real time shelf out-of-stock management and to demonstrate the feasibility and the scalability of the system in providing a lot of data and interesting insights for store team and brand's marketing team.","author":[{"family":"Frontoni","given":"Emanuele"},{"family":"Mancini","given":"Adriano"},{"family":"Zingaretti","given":"Primo"}],"citation-key":"frontoniRealTimeOut2014","container-title":"2014 IEEE/ASME 10th International Conference on Mechatronic and Embedded Systems and Applications (MESA)","DOI":"10.1109/MESA.2014.6935614","issued":{"date-parts":[["2014",9]]},"page":"1–6","title":"Real time out of shelf detection using embedded sensor network","type":"paper-conference"},{"id":"frontoniRealTimeOut2014a","abstract":"Out-of-shelf problem is important to solve for retail store since the absence of products on the shelf can lead to a significant reduction of shoppers and a consequent drop on sales. For this purpose, it is necessary to study and to introduce approaches able to establish the lack of products on the shelves and thereby promptly ensuring their repositioning. In this context, the paper investigates the use of artificial intelligence techniques in detecting the out-of-shelf products. Particularly, having sales data, ordering info and product assortment of the store available, we study the development of low cost shelf detector that is based on wireless sensor network, and that can automatically discover out-of-shelf situations on a daily basis for all the stores of a retail chain. The use of an automatic method for detecting products that are not available on the shelf based on sales data would offer an accurate view of the shelf availability, both to retailers and to product suppliers. The tool presented is the first being installed for a long time in a high number of stores and products demonstrating the ability to gather data from there and extract interesting insights. This paper aims to present the hardware infrastructure of an embedded sensor network devoted to real time shelf out-of-stock management and to demonstrate the feasibility and the scalability of the system in providing a lot of data and interesting insights for store team and brand's marketing team.","author":[{"family":"Frontoni","given":"Emanuele"},{"family":"Mancini","given":"Adriano"},{"family":"Zingaretti","given":"Primo"}],"citation-key":"frontoniRealTimeOut2014a","container-title":"2014 IEEE/ASME 10th International Conference on Mechatronic and Embedded Systems and Applications (MESA)","DOI":"10.1109/MESA.2014.6935614","event-title":"2014 IEEE/ASME 10th International Conference on Mechatronic and Embedded Systems and Applications (MESA)","issued":{"date-parts":[["2014",9]]},"page":"1-6","source":"IEEE Xplore","title":"Real time out of shelf detection using embedded sensor network","type":"paper-conference"},{"id":"gallaunerEnablingClassificationHeavilyoccluded2021","abstract":"5","accessed":{"date-parts":[["2023",3,29]]},"author":[{"family":"Gallauner","given":"Benjamin"},{"family":"Thalhammer","given":"Stefan"},{"family":"Vincze","given":"Markus"}],"citation-key":"gallaunerEnablingClassificationHeavilyoccluded2021","event-title":"OAGM Workshop 2021 Computer Vision and Pattern Analysis Across Domains","issued":{"date-parts":[["2021"]]},"language":"en","note":"Accepted: 2022-08-10T15:15:42Z","page":"5","publisher":"Verlag der Technischen Universität Graz","source":"repositum.tuwien.at","title":"Enabling Classification of Heavily-occluded Objects through Class-agnostic Image Manipulation","type":"paper-conference","URL":"https://repositum.tuwien.at/handle/20.500.12708/77563"},{"id":"gammaDesignPatternsElements2011","citation-key":"gammaDesignPatternsElements2011","collection-title":"Addison-Wesley professional computing series","edition":"39. printing","editor":[{"family":"Gamma","given":"Erich"},{"family":"Helm","given":"Richard"},{"family":"Johnson","given":"Ralph"},{"family":"Vlissides","given":""}],"event-place":"Boston, Mass. Munich","ISBN":"978-0-201-63361-0","issued":{"date-parts":[["2011"]]},"language":"eng","number-of-pages":"395","publisher":"Addison-Wesley","publisher-place":"Boston, Mass. Munich","source":"K10plus ISBN","title":"Design patterns: elements of reusable object-oriented software","title-short":"Design patterns","type":"book"},{"id":"gartnerGartnerDeliveringActionable2024","abstract":"Gartner provides actionable insights, guidance, and tools that enable faster, smarter decisions and stronger performance on an organization’s mission-critical priorities.","accessed":{"date-parts":[["2024",6,23]]},"author":[{"family":"Gartner","given":""}],"citation-key":"gartnerGartnerDeliveringActionable2024","container-title":"Gartner","issued":{"date-parts":[["2024"]]},"language":"en","title":"Gartner | Delivering Actionable, Objective Insight to Executives and Their Teams","type":"webpage","URL":"https://www.gartner.com/en"},{"id":"gartnerGartnerTopData2021","abstract":"Trend 1: Smarter, responsible, scalable #AI. Gartner reveals the top 10 #data & #analytics trends for 2021. Read more. #GartnerDA","accessed":{"date-parts":[["2024",6,23]]},"author":[{"family":"Gartner","given":""}],"citation-key":"gartnerGartnerTopData2021","container-title":"Gartner","issued":{"date-parts":[["2021"]]},"language":"en","title":"Gartner Top Data and Analytics Trends for 2021","type":"webpage","URL":"https://www.gartner.com/smarterwithgartner/gartner-top-10-data-and-analytics-trends-for-2021"},{"id":"gayoRDFEcosystem2018","abstract":"This chapter includes a short overview of the RDF data model and the Turtle notation, as well as some technologies like SPARQL, RDF Schema, and OWL that form part of the RDF ecosystem.","accessed":{"date-parts":[["2024",2,16]]},"author":[{"family":"Gayo","given":"Jose Emilio Labra"},{"family":"Prud’hommeaux","given":"Eric"},{"family":"Boneva","given":"Iovka"},{"family":"Kontokostas","given":"Dimitris"}],"citation-key":"gayoRDFEcosystem2018","collection-title":"Synthesis Lectures on Data, Semantics, and Knowledge","container-title":"Validating RDF Data","DOI":"10.1007/978-3-031-79478-0_2","editor":[{"family":"Gayo","given":"Jose Emilio Labra"},{"family":"Prud’hommeaux","given":"Eric"},{"family":"Boneva","given":"Iovka"},{"family":"Kontokostas","given":"Dimitris"}],"event-place":"Cham","ISBN":"978-3-031-79478-0","issued":{"date-parts":[["2018"]]},"language":"en","page":"9-26","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"The RDF Ecosystem","type":"chapter","URL":"https://doi.org/10.1007/978-3-031-79478-0_2"},{"id":"gayoValidatingRDFData2018","accessed":{"date-parts":[["2024",2,16]]},"author":[{"family":"Gayo","given":"Jose Emilio Labra"},{"family":"Prud’hommeaux","given":"Eric"},{"family":"Boneva","given":"Iovka"},{"family":"Kontokostas","given":"Dimitris"}],"citation-key":"gayoValidatingRDFData2018","collection-title":"Synthesis Lectures on Data, Semantics, and Knowledge","DOI":"10.1007/978-3-031-79478-0","event-place":"Cham","ISBN":"978-3-031-79477-3 978-3-031-79478-0","issued":{"date-parts":[["2018"]]},"language":"en","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Validating RDF Data","type":"book","URL":"https://link.springer.com/10.1007/978-3-031-79478-0"},{"id":"georgeFineGrainedProductClass2015","abstract":"Assistive solutions for a better shopping experience can improve the quality of life of people, in particular also of visually impaired shoppers. We present a system that visually recognizes the ﬁne-grained product classes of items on a shopping list, in shelves images taken with a smartphone in a grocery store. Our system consists of three components: (a) We automatically recognize useful text on product packaging, e.g., product name and brand, and build a mapping of words to product classes based on the largescale GroceryProducts dataset. When the user populates the shopping list, we automatically infer the product class of each entered word. (b) We perform ﬁne-grained product class recognition when the user is facing a shelf. We discover discriminative patches on product packaging to differentiate between visually similar product classes and to increase the robustness against continuous changes in product design. (c) We continuously improve the recognition accuracy through active learning. Our experiments show the robustness of the proposed method against cross-domain challenges, and the scalability to an increasing number of products with minimal re-training.","accessed":{"date-parts":[["2023",1,14]]},"author":[{"family":"George","given":"Marian"},{"family":"Mircic","given":"Dejan"},{"family":"Soros","given":"Gabor"},{"family":"Floerkemeier","given":"Christian"},{"family":"Mattern","given":"Friedemann"}],"citation-key":"georgeFineGrainedProductClass2015","container-title":"2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","DOI":"10.1109/ICCVW.2015.77","event-place":"Santiago","ISBN":"978-1-4673-9711-7","issued":{"date-parts":[["2015",12]]},"language":"en","page":"546–554","publisher":"IEEE","publisher-place":"Santiago","title":"Fine-Grained Product Class Recognition for Assisted Shopping","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7406426/"},{"id":"georgeFineGrainedProductClass2015a","abstract":"Assistive solutions for a better shopping experience can improve the quality of life of people, in particular also of visually impaired shoppers. We present a system that visually recognizes the ﬁne-grained product classes of items on a shopping list, in shelves images taken with a smartphone in a grocery store. Our system consists of three components: (a) We automatically recognize useful text on product packaging, e.g., product name and brand, and build a mapping of words to product classes based on the largescale GroceryProducts dataset. When the user populates the shopping list, we automatically infer the product class of each entered word. (b) We perform ﬁne-grained product class recognition when the user is facing a shelf. We discover discriminative patches on product packaging to differentiate between visually similar product classes and to increase the robustness against continuous changes in product design. (c) We continuously improve the recognition accuracy through active learning. Our experiments show the robustness of the proposed method against cross-domain challenges, and the scalability to an increasing number of products with minimal re-training.","accessed":{"date-parts":[["2023",1,14]]},"author":[{"family":"George","given":"Marian"},{"family":"Mircic","given":"Dejan"},{"family":"Soros","given":"Gabor"},{"family":"Floerkemeier","given":"Christian"},{"family":"Mattern","given":"Friedemann"}],"citation-key":"georgeFineGrainedProductClass2015a","container-title":"2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","DOI":"10.1109/ICCVW.2015.77","event-place":"Santiago","event-title":"2015 IEEE International Conference on Computer Vision: Workshop (ICCVW)","ISBN":"978-1-4673-9711-7","issued":{"date-parts":[["2015",12]]},"language":"en","page":"546-554","publisher":"IEEE","publisher-place":"Santiago","source":"DOI.org (Crossref)","title":"Fine-Grained Product Class Recognition for Assisted Shopping","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7406426/"},{"id":"geziciSystematicLiteratureReview2022","abstract":"There is a widespread demand for Artificial Intelligence (AI) software, specifically Machine Learning (ML). It is getting increasingly popular and being adopted in various applications we use daily. AI-based software quality is different from traditional software quality because it generally addresses distinct and more complex kinds of problems. With the fast advance of AI technologies and related techniques, how to build high-quality AI-based software becomes a very prominent subject. This paper aims at investigating the state of the art on software quality (SQ) for AI-based systems and identifying quality attributes, applied models, challenges, and practices that are reported in the literature. We carried out a systematic literature review (SLR) from 1988 to 2020 to (i) analyze and understand related primary studies and (ii) synthesize limitations and open challenges to drive future research. Our study provides a road map for researchers to understand quality challenges, attributes, and practices in the context of software quality for AI-based software better. From the empirical evidence that we have gathered by this SLR, we suggest future work on this topic be structured under three categories which are Definition/Specification, Design/Evaluation, and Process/Socio-technical.","accessed":{"date-parts":[["2023",6,12]]},"author":[{"family":"Gezici","given":"Bahar"},{"family":"Tarhan","given":"Ayça Kolukısa"}],"citation-key":"geziciSystematicLiteratureReview2022","container-title":"Empirical Software Engineering","container-title-short":"Empir Software Eng","DOI":"10.1007/s10664-021-10105-2","ISSN":"1573-7616","issue":"3","issued":{"date-parts":[["2022",3,17]]},"language":"en","page":"66","source":"Springer Link","title":"Systematic literature review on software quality for AI-based software","type":"article-journal","URL":"https://doi.org/10.1007/s10664-021-10105-2","volume":"27"},{"id":"GIMP","abstract":"GIMP - The GNU Image Manipulation Program: The Free and Open Source Image Editor","accessed":{"date-parts":[["2023",3,27]]},"citation-key":"GIMP","container-title":"GIMP","language":"en","title":"GIMP","type":"webpage","URL":"https://www.gimp.org/"},{"id":"githubNonFoodKGNonfoodProduct2022","abstract":"Non-food product knowledge graph","accessed":{"date-parts":[["2023",3,27]]},"author":[{"family":"github","given":""}],"citation-key":"githubNonFoodKGNonfoodProduct2022","genre":"HTML","issued":{"date-parts":[["2022",7,1]]},"license":"BSD-3-Clause","original-date":{"date-parts":[["2021",3,12]]},"publisher":"K4R-IAI","source":"GitHub","title":"NonFoodKG - A non-food product knowledge graph","type":"software","URL":"https://github.com/K4R-IAI/NonFoodKG"},{"id":"gmbhGPCProduktklassifikationGlobal","abstract":"Die GPC von GS1 (Global Product Classification bzw. standardisierte Produktklassifikation) unterstützt globale Stammdatenmanagement & Beschaffungsprozesse.","accessed":{"date-parts":[["2024",6,10]]},"author":[{"family":"GmbH","given":"GS1 Germany"}],"citation-key":"gmbhGPCProduktklassifikationGlobal","container-title":"GS1 Germany","language":"de","title":"GPC Produktklassifikation: global eindeutig","title-short":"GPC Produktklassifikation","type":"webpage","URL":"https://www.gs1-germany.de/gs1-standards/klassifikation/produktklassifikation-gpc/"},{"id":"gmbhMaschinenDeoCalgonitFinish","abstract":"Beseitigt mühelos schlechte Gerüche in Ihrer Spülmaschine - inkl. Easyclip für die Befestigung in Ihrer Spülmaschine - Mit angenehmen Citrus-Duft - Ausreichend für 60 Spülgänge - Maschinen-Deo ...","accessed":{"date-parts":[["2023",3,5]]},"author":[{"family":"GmbH","given":"eOFFICE24 com Persiehl 1849"}],"citation-key":"gmbhMaschinenDeoCalgonitFinish","language":"de-DE","title":"Maschinen-Deo Calgonit finish Citrus/Limone","type":"document","URL":"https://www.eoffice24.com/maschinen-deo-calgonit-finish-citrus-limone.html"},{"id":"gojareAnalysisDesignSelenium2015","abstract":"Nowadays, number of software system has been implemented as web-based applications. These web applications are very complex. It is very difficult to test such complex web applications. Automation testing uses automation tools to reduce human intervention and repeatable tasks. In this paper we have designed and implemented automation testing framework for testing web applications. This new automation testing framework has been implemented using selenium WebDriver tool. Using this framework tester can easily write their test cases efficiently and in less time. Tester need not to study the selenium webdriver tool in detail. This framework is helpful to developer to analyze their code due to screen shot property of framework. This framework produces the customized test report to tester. It is very easy to maintain and repair the test suite for new release of the application using this framework.","accessed":{"date-parts":[["2023",9,26]]},"author":[{"family":"Gojare","given":"Satish"},{"family":"Joshi","given":"Rahul"},{"family":"Gaigaware","given":"Dhanashree"}],"citation-key":"gojareAnalysisDesignSelenium2015","collection-title":"Big Data, Cloud and Computing Challenges","container-title":"Procedia Computer Science","container-title-short":"Procedia Computer Science","DOI":"10.1016/j.procs.2015.04.038","ISSN":"1877-0509","issued":{"date-parts":[["2015",1,1]]},"page":"341-346","source":"ScienceDirect","title":"Analysis and Design of Selenium WebDriver Automation Testing Framework","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S1877050915005396","volume":"50"},{"id":"gomesMethodAutomaticGeneration","abstract":"This paper presents a method for automatic validation of business process models that uses an algorithm based on the metaheuristic Scatter Search. The business processes models are represented as UML activity diagrams where the business rules are represented as OCL pre and post con-ditions attached to process activities or as OCL invariants associated to a business conceptual model. The actions as-sociated to an activity defined in the process are performed with the help of the USE tool.The method generates and simulates a set ofprocess execution scenarios that garantees a minimum coverage of all possible real scenarios.","author":[{"family":"Gomes","given":"Leandro"},{"family":"Schmitz","given":"Eber"}],"citation-key":"gomesMethodAutomaticGeneration","source":"ResearchGate","title":"A Method for automatic generation of test cases to BPEL Processes","type":"article-journal"},{"id":"GoodRelationsProfessionalWeb","accessed":{"date-parts":[["2024",2,12]]},"citation-key":"GoodRelationsProfessionalWeb","title":"GoodRelations: The Professional Web Vocabulary for E-Commerce","type":"webpage","URL":"http://www.heppnetz.de/projects/goodrelations/"},{"id":"googleIntroducingKnowledgeGraph2012","abstract":"We hope this will give you a more complete picture of your interest, provide smarter search results, and pique your curiosity.","accessed":{"date-parts":[["2024",2,23]]},"author":[{"family":"Google","given":""}],"citation-key":"googleIntroducingKnowledgeGraph2012","container-title":"Google","issued":{"date-parts":[["2012",5,16]]},"language":"en-us","title":"Introducing the Knowledge Graph: things, not strings","title-short":"Introducing the Knowledge Graph","type":"webpage","URL":"https://blog.google/products/search/introducing-knowledge-graph-things-not/"},{"id":"gosainStaticAnalysisSurvey2015","abstract":"Static program analysis has shown tremendous surge from basic compiler optimization technique to becoming a major role player in correctness and verification of software. Because of its rich theoretical background, static analysis is in a good position to help produce quality software. This paper provides an overview of the existing static analysis techniques and tools. Further, it gives a critique of static analysis approach over six attributes, namely precision, efficiency, coverage, modularity, scalability, and automation.","author":[{"family":"Gosain","given":"Anjana"},{"family":"Sharma","given":"Ganga"}],"citation-key":"gosainStaticAnalysisSurvey2015","collection-title":"Advances in Intelligent Systems and Computing","container-title":"Intelligent Computing and Applications","DOI":"10.1007/978-81-322-2268-2_59","editor":[{"family":"Mandal","given":"Durbadal"},{"family":"Kar","given":"Rajib"},{"family":"Das","given":"Swagatam"},{"family":"Panigrahi","given":"Bijaya Ketan"}],"event-place":"New Delhi","ISBN":"978-81-322-2268-2","issued":{"date-parts":[["2015"]]},"language":"en","page":"581-591","publisher":"Springer India","publisher-place":"New Delhi","source":"Springer Link","title":"Static Analysis: A Survey of Techniques and Tools","title-short":"Static Analysis","type":"paper-conference"},{"id":"gosnellPractitionerGuideGraph2020","abstract":"Graph data closes the gap between the way humans and computers view the world. While computers rely on static rows and columns of data, people navigate and reason about life through relationships. This practical guide demonstrates how graph data brings these two approaches together. By working with concepts from graph theory, database schema, distributed systems, and data analysis, you'll arrive at a unique intersection known as graph thinking. Authors Denise Koessler Gosnell and Matthias Broecheler show data engineers, data scientists, and data analysts how to solve complex problems with graph databases. You'll explore templates for building with graph technology, along with examples that demonstrate how teams think about graph data within an application.--","author":[{"family":"Gosnell","given":"Denise"},{"family":"Broecheler","given":"Matthias"}],"call-number":"QA76.9.D3 G683 2020","citation-key":"gosnellPractitionerGuideGraph2020","edition":"First edition","event-place":"Sebastopol, CA","ISBN":"978-1-4920-4407-9","issued":{"date-parts":[["2020"]]},"note":"OCLC: on1104466208","number-of-pages":"397","publisher":"O'Reilly Media, Inc","publisher-place":"Sebastopol, CA","source":"Library of Congress ISBN","title":"The practitioner's guide to graph data","type":"book"},{"id":"gottschalkRemovingEnergyCode","abstract":"Due to the increasing consumer adoption of mobile devices, like smart phones and tablet PCs, saving energy is becoming more and more important. Users desire more functionality and longer battery cycles. While modern mobile computing devices offer hardware optimized for low energy consumption, applications often do not make proper use of energy-saving capabilities. This paper proposes detecting and removing energy-wasteful code using software reengineering services, like code analysis and restructuring, to optimize the energy consumption of mobile devices.","author":[{"family":"Gottschalk","given":"Marion"},{"family":"Joseﬁok","given":"Mirco"},{"family":"Jelschen","given":"Jan"},{"family":"Winter","given":"Andreas"}],"citation-key":"gottschalkRemovingEnergyCode","language":"en","page":"15","source":"Zotero","title":"Removing Energy Code Smells with Reengineering Services","type":"article-journal"},{"id":"gouyWhichProgrammingLanguage","accessed":{"date-parts":[["2022",10,12]]},"author":[{"family":"Gouy","given":"Isaac"}],"citation-key":"gouyWhichProgrammingLanguage","title":"Which programming language is fastest?","type":"webpage","URL":"https://benchmarksgame-team.pages.debian.net/benchmarksgame/"},{"id":"graefeVolcanoOptimizerGenerator1993","abstract":"Emerging database application domains demand not only new functionality but also high performance. To satisfy these two requirements, the Volcano project provides efficient, extensible tools for query and request processing, particularly for object-oriented and scientific database systems. One of these tools is a new optimizer generator. Data model, logical algebra, physical algebra, and optimization rules are translated by the optimizer generator into optimizer source code. Compared with our earlier EXODUS optimizer generator prototype, the search engine is more extensible and powerful; it provides effective support for non-trivial cost models and for physical properties such as sort order. At the same time, it is much more efficient as it combines dynamic programming, which until now had been used only for relational select-project-join optimization, with goal-directed search and branch-andbound pruning. Compared with other rule-based optimization systems, it provides complete data model independence and more natural extensibility.","accessed":{"date-parts":[["2024",7,9]]},"author":[{"family":"Graefe","given":"G."},{"family":"McKenna","given":"W.J."}],"citation-key":"graefeVolcanoOptimizerGenerator1993","container-title":"Proceedings of IEEE 9th International Conference on Data Engineering","DOI":"10.1109/ICDE.1993.344061","event-place":"Vienna, Austria","event-title":"IEEE 9th International Conference on Data Engineering","ISBN":"978-0-8186-3570-0","issued":{"date-parts":[["1993"]]},"language":"en","page":"209-218","publisher":"IEEE Comput. Soc. Press","publisher-place":"Vienna, Austria","source":"DOI.org (Crossref)","title":"The Volcano optimizer generator: extensibility and efficient search","title-short":"The Volcano optimizer generator","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/344061/"},{"id":"grechStaticAnalysisEnergy2015","abstract":"Energy models can be constructed by characterizing the energy consumed when executing each instruction in a processor's instruction set. This can be used to determine how much energy is required to execute a sequence of assembly instructions, without the need to instrument or measure hardware. However, statically analyzing low-level program structures is hard, and the gap between the high-level program structure and the low-level energy models needs to be bridged. We have developed techniques for performing a static analysis on the intermediate compiler representations of a program. Specifically, we target LLVM IR, a representation used by modern compilers, including Clang. Using these techniques we can automatically infer an estimate of the energy consumed when running a function under different platforms and compilers. One of the challenges in doing so is that of determining the energy cost of executing LLVM IR program segments, for which we have developed two different approaches. When this information is used in conjunction with our analysis, we are able to infer energy formulae that characterize the energy consumption for a particular program. This approach can be applied to any languages targeting the LLVM toolchain, including C and XC or architectures such as ARM Cortex-M or XMOS xCORE, with a focus towards embedded platforms. Our techniques are validated on these platforms by comparing the static analysis results to the physical measurements taken from the hardware. Static energy consumption estimation enables energy-aware software development by providing instant feedback to the developer, without requiring simulations or hardware knowledge.","accessed":{"date-parts":[["2023",7,25]]},"author":[{"family":"Grech","given":"Neville"},{"family":"Georgiou","given":"Kyriakos"},{"family":"Pallister","given":"James"},{"family":"Kerrison","given":"Steve"},{"family":"Morse","given":"Jeremy"},{"family":"Eder","given":"Kerstin"}],"citation-key":"grechStaticAnalysisEnergy2015","collection-title":"SCOPES '15","container-title":"Proceedings of the 18th International Workshop on Software and Compilers for Embedded Systems","DOI":"10.1145/2764967.2764974","event-place":"New York, NY, USA","ISBN":"978-1-4503-3593-5","issued":{"date-parts":[["2015",6,1]]},"page":"12–21","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Static analysis of energy consumption for LLVM IR programs","type":"paper-conference","URL":"https://doi.org/10.1145/2764967.2764974"},{"id":"greensoftwarelabEnergyEfficiencyProgramming2022","abstract":"The complete set of tools for energy consumption analysis of programming languages, using Computer Language Benchmark Game","accessed":{"date-parts":[["2022",5,14]]},"author":[{"family":"Greensoftwarelab","given":""}],"citation-key":"greensoftwarelabEnergyEfficiencyProgramming2022","genre":"C","issued":{"date-parts":[["2022",5,9]]},"license":"MIT","original-date":{"date-parts":[["2017",8,28]]},"publisher":"Green Software Lab","source":"GitHub","title":"Energy Efficiency in Programming Languages","type":"software","URL":"https://github.com/greensoftwarelab/Energy-Languages"},{"id":"greenwaldOracleEssentialsOracle2013","abstract":"Written by Oracle insiders, this indispensable guide distills an enormous amount of information about the Oracle Database into one compact volume. Ideal for novice and experienced DBAs, developers, managers, and users, Oracle Essentials walks you through technologies and features in Oracle’s product line, including its architecture, data structures, networking, concurrency, and tuning.Complete with illustrations and helpful hints, this fifth edition provides a valuable one-stop overview of Oracle Database 12c, including an introduction to Oracle and cloud computing. Oracle Essentials provides the conceptual background you need to understand how Oracle truly works.Topics include:A complete overview of Oracle databases and data stores, and Fusion Middleware products and featuresCore concepts and structures in Oracle’s architecture, including pluggable databasesOracle objects and the various datatypes Oracle supportsSystem and database management, including Oracle Enterprise Manager 12cSecurity options, basic auditing capabilities, and options for meeting compliance needsPerformance characteristics of disk, memory, and CPU tuningBasic principles of multiuser concurrencyOracle’s online transaction processing (OLTP)Data warehouses, Big Data, and Oracle’s business intelligence toolsBackup and recovery, and high availability and failover solutions","author":[{"family":"Greenwald","given":"Rick"},{"family":"Stackowiak","given":"Robert"},{"family":"Stern","given":"Associate Fellow Sustainable Development Programme Royal Institute of International Affairs Jonathan"},{"family":"Stern","given":"Jonathan"}],"citation-key":"greenwaldOracleEssentialsOracle2013","ISBN":"978-1-4493-4318-7","issued":{"date-parts":[["2013",9,6]]},"language":"en","number-of-pages":"431","publisher":"O'Reilly Media, Inc.","source":"Google Books","title":"Oracle Essentials: Oracle Database 12c","title-short":"Oracle Essentials","type":"book"},{"id":"gs1germanygmbhDigitalLink","accessed":{"date-parts":[["2024",6,15]]},"author":[{"family":"GS1 Germany GmbH","given":""}],"citation-key":"gs1germanygmbhDigitalLink","title":"Digital Link","type":"webpage","URL":"https://www.gs1.org/standards/gs1-digital-link"},{"id":"gs1germanygmbhGlobalDataModel2024","abstract":"The Global Data Model defines a consistent set of product attributes to harmonise and simplify data exchange between trading partners. Find out more about it here.","accessed":{"date-parts":[["2024",6,15]]},"author":[{"family":"GS1 Germany GmbH","given":""}],"citation-key":"gs1germanygmbhGlobalDataModel2024","issued":{"date-parts":[["2024",4,23]]},"language":"en","title":"Global Data Model","type":"webpage","URL":"https://www.gs1.org/standards/gs1-global-data-model"},{"id":"gs1germanygmbhHomepage2024","abstract":"The Global Language of Business","accessed":{"date-parts":[["2024",6,15]]},"author":[{"family":"GS1 Germany GmbH","given":""}],"citation-key":"gs1germanygmbhHomepage2024","issued":{"date-parts":[["2024",4,23]]},"language":"en","title":"Homepage","type":"webpage","URL":"https://www.gs1.org/"},{"id":"gs1germanygmbhWebVocabulary","accessed":{"date-parts":[["2024",6,15]]},"author":[{"family":"GS1 Germany GmbH","given":""}],"citation-key":"gs1germanygmbhWebVocabulary","title":"Web Vocabulary","type":"webpage","URL":"https://www.gs1.org/voc/"},{"id":"gulabovskaSurveyStaticAnalysis","author":[{"family":"Gulabovska","given":"Hristina"}],"citation-key":"gulabovskaSurveyStaticAnalysis","language":"en","page":"10","source":"Zotero","title":"Survey on Static Analysis Tools of Python Programs","type":"article-journal"},{"id":"hampGermaNetLexicalSemanticNet1997","accessed":{"date-parts":[["2024",7,30]]},"author":[{"family":"Hamp","given":"Birgit"},{"family":"Feldweg","given":"Helmut"}],"citation-key":"hampGermaNetLexicalSemanticNet1997","container-title":"Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications","issued":{"date-parts":[["1997"]]},"source":"ACLWeb","title":"GermaNet - a Lexical-Semantic Net for German","type":"paper-conference","URL":"https://aclanthology.org/W97-0802"},{"id":"handelsblattStartupWirWollen","abstract":"Das Start-up Ubica will mithilfe von Robotern die operativen Kosten in den Läden deutlich senken. Auch Rewe zeigt sich interessiert.","accessed":{"date-parts":[["2023",1,19]]},"author":[{"family":"handelsblatt","given":""}],"citation-key":"handelsblattStartupWirWollen","language":"de","title":"Start-up: „Wir wollen nun in die Skalierung gehen“ – Dieser Roboter fährt nachts durch DM-Filialen","title-short":"Start-up","type":"webpage","URL":"https://www.handelsblatt.com/unternehmen/start-up-wir-wollen-nun-in-die-skalierung-gehen-dieser-roboter-faehrt-nachts-durch-dm-filialen/28501380.html"},{"id":"haoEstimatingMobileApplication2013","abstract":"Optimizing the energy efficiency of mobile applications can greatly increase user satisfaction. However, developers lack viable techniques for estimating the energy consumption of their applications. This paper proposes a new approach that is both lightweight in terms of its developer requirements and provides fine-grained estimates of energy consumption at the code level. It achieves this using a novel combination of program analysis and per-instruction energy modeling. In evaluation, our approach is able to estimate energy consumption to within 10% of the ground truth for a set of mobile applications from the Google Play store. Additionally, it provides useful and meaningful feedback to developers that helps them to understand application energy consumption behavior.","author":[{"family":"Hao","given":"Shuai"},{"family":"Li","given":"Ding"},{"family":"Halfond","given":"William G. J."},{"family":"Govindan","given":"Ramesh"}],"citation-key":"haoEstimatingMobileApplication2013","container-title":"2013 35th International Conference on Software Engineering (ICSE)","DOI":"10.1109/ICSE.2013.6606555","event-title":"2013 35th International Conference on Software Engineering (ICSE)","ISSN":"1558-1225","issued":{"date-parts":[["2013",5]]},"page":"92-101","source":"IEEE Xplore","title":"Estimating mobile application energy consumption using program analysis","type":"paper-conference"},{"id":"harrisCombinedCornerEdge1988","abstract":"The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed.","accessed":{"date-parts":[["2023",9,9]]},"author":[{"family":"Harris","given":"C."},{"family":"Stephens","given":"M."}],"citation-key":"harrisCombinedCornerEdge1988","container-title":"Procedings of the Alvey Vision Conference 1988","DOI":"10.5244/C.2.23","event-place":"Manchester","event-title":"Alvey Vision Conference 1988","issued":{"date-parts":[["1988"]]},"language":"en","page":"23.1-23.6","publisher":"Alvey Vision Club","publisher-place":"Manchester","source":"Semantic Scholar","title":"A Combined Corner and Edge Detector","type":"paper-conference","URL":"http://www.bmva.org/bmvc/1988/avc-88-023.html"},{"id":"hassenImageSharpnessAssessment2013","accessed":{"date-parts":[["2022",5,17]]},"author":[{"family":"Hassen","given":"R."},{"literal":"Zhou Wang"},{"family":"Salama","given":"M. M. A."}],"citation-key":"hassenImageSharpnessAssessment2013","container-title":"IEEE Transactions on Image Processing","container-title-short":"IEEE Trans. on Image Process.","DOI":"10.1109/TIP.2013.2251643","ISSN":"1057-7149, 1941-0042","issue":"7","issued":{"date-parts":[["2013",7]]},"language":"en","page":"2798-2810","source":"DOI.org (Crossref)","title":"Image Sharpness Assessment Based on Local Phase Coherence","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/6476013/","volume":"22"},{"id":"heathLinkedDataEvolving2011","accessed":{"date-parts":[["2024",2,13]]},"author":[{"family":"Heath","given":"Tom"},{"family":"Bizer","given":"Christian"}],"citation-key":"heathLinkedDataEvolving2011","collection-title":"Synthesis Lectures on Data, Semantics, and Knowledge","DOI":"10.1007/978-3-031-79432-2","event-place":"Cham","ISBN":"978-3-031-79431-5 978-3-031-79432-2","issued":{"date-parts":[["2011"]]},"language":"en","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Linked Data: Evolving the Web into a Global Data Space","title-short":"Linked Data","type":"book","URL":"https://link.springer.com/10.1007/978-3-031-79432-2"},{"id":"hellbergEinkaufMitSAP2009","author":[{"family":"Hellberg","given":"Torsten"}],"citation-key":"hellbergEinkaufMitSAP2009","collection-title":"SAP press","edition":"2., aktualisierte und erw. Aufl","event-place":"Bonn","ISBN":"978-3-8362-1394-3","issued":{"date-parts":[["2009"]]},"language":"ger","number-of-pages":"375","publisher":"Galileo Press","publisher-place":"Bonn","source":"K10plus ISBN","title":"Einkauf mit SAP MM: Prozesse, Funktionen, Customizing","title-short":"Einkauf mit SAP MM","type":"book"},{"id":"heppGoodRelationsOntologyDescribing2008","abstract":"A promising application domain for Semantic Web technology is the annotation of products and services offerings on the Web\nso that consumers and enterprises can search for suitable suppliers using products and services ontologies. While there has\nbeen substantial progress in developing ontologies for types of products and services, namely eClassOWL, this alone does not provide the representational means required for e-commerce\non the Semantic Web. Particularly missing is an ontology that allows describing the relationships between (1) Web resources,\n(2) offerings made by means of those Web resources, (3) legal entities, (4) prices, (5) terms and conditions, and the aforementioned\nontologies for products and services (6). For example, we must be able to say that a particular Web site describes an offer\nto sell cell phones of a certain make and model at a certain price, that a piano house offers maintenance for pianos that\nweigh less than 150 kg, or that a car rental company leases out cars of a certain make and model from a set of branches across\nthe country. In this paper, we analyze the complexity of product description on the Semantic Web and define the GoodRelations\nontology that covers the representational needs of typical business scenarios for commodity products and services.","author":[{"family":"Hepp","given":"Martin"}],"citation-key":"heppGoodRelationsOntologyDescribing2008","DOI":"10.1007/978-3-540-87696-0_29","event-title":"Proceedings of the 16th International Conference on Knowledge Engineering and Knowledge Management","ISBN":"978-3-540-87695-3","issued":{"date-parts":[["2008",9,29]]},"page":"329-346","source":"ResearchGate","title":"GoodRelations: An Ontology for Describing Products and Services Offers on the Web","title-short":"GoodRelations","type":"paper-conference","volume":"5268"},{"id":"heppProductTypesOntology","accessed":{"date-parts":[["2024",2,12]]},"author":[{"family":"Hepp","given":"Martin"}],"citation-key":"heppProductTypesOntology","title":"The Product Types Ontology: Use Wikipedia pages for describing products or services with GoodRelations and schema.org","type":"webpage","URL":"http://www.productontology.org/"},{"id":"hitzlerSemanticWebGrundlagen2008","citation-key":"hitzlerSemanticWebGrundlagen2008","collection-title":"eXamen.press","edition":"1. Aufl","editor":[{"family":"Hitzler","given":"Pascal"},{"family":"Krötzsch","given":"Markus"},{"family":"Rudolph","given":"Sebastian"},{"family":"Sure-Vetter","given":"York"}],"event-place":"Berlin Heidelberg","ISBN":"978-3-540-33993-9","issued":{"date-parts":[["2008"]]},"language":"ger","number-of-pages":"277","publisher":"Springer","publisher-place":"Berlin Heidelberg","source":"K10plus ISBN","title":"Semantic Web: Grundlagen","title-short":"Semantic Web","type":"book"},{"id":"hoferConstructionKnowledgeGraphs2023","abstract":"With knowledge graphs (KGs) at the center of numerous applications such as recommender systems and question answering, the need for generalized pipelines to construct and continuously update such KGs is increasing. While the individual steps that are necessary to create KGs from unstructured (e.g. text) and structured data sources (e.g. databases) are mostly wellresearched for their one-shot execution, their adoption for incremental KG updates and the interplay of the individual steps have hardly been investigated in a systematic manner so far. In this work, we first discuss the main graph models for KGs and introduce the major requirements for future KG construction pipelines. Next, we provide an overview of the necessary steps to build highquality KGs, including cross-cutting topics such as metadata management, ontology development, and quality assurance. We then evaluate the state of the art of KG construction w.r.t the introduced requirements for specific popular KGs as well as some recent tools and strategies for KG construction. Finally, we identify areas in need of further research and improvement.","accessed":{"date-parts":[["2024",2,23]]},"author":[{"family":"Hofer","given":"Marvin"},{"family":"Obraczka","given":"Daniel"},{"family":"Saeedi","given":"Alieh"},{"family":"Köpcke","given":"Hanna"},{"family":"Rahm","given":"Erhard"}],"citation-key":"hoferConstructionKnowledgeGraphs2023","issued":{"date-parts":[["2023",10,11]]},"language":"en","number":"arXiv:2302.11509","publisher":"arXiv","source":"arXiv.org","title":"Construction of Knowledge Graphs: State and Challenges","title-short":"Construction of Knowledge Graphs","type":"article","URL":"http://arxiv.org/abs/2302.11509"},{"id":"hoferNewDBpediaRelease2020","abstract":"Since its inception in 2007, DBpedia has been constantly releasing open data in RDF, extracted from various Wikimedia projects using a complex software system called the DBpedia Information Extraction Framework (DIEF). For the past 12 years, the software received a plethora of extensions by the community, which positively affected the size and data quality. Due to the increase in size and complexity, the release process was facing huge delays (from 12 to 17 months cycle), thus impacting the agility of the development. In this paper, we describe the new DBpedia release cycle including our innovative release workflow, which allows development teams (in particular those who publish large, open data) to implement agile, cost-efficient processes and scale up productivity. The DBpedia release workflow has been re-engineered, its new primary focus is on productivity and agility, to address the challenges of size and complexity. At the same time, quality is assured by implementing a comprehensive testing methodology. We run an experimental evaluation and argue that the implemented measures increase agility and allow for cost-effective quality-control and debugging and thus achieve a higher level of maintainability. As a result, DBpedia now publishes regular (i.e. monthly) releases with over 21 billion triples with minimal publishing effort.","author":[{"family":"Hofer","given":"Marvin"},{"family":"Hellmann","given":"Sebastian"},{"family":"Dojchinovski","given":"Milan"},{"family":"Frey","given":"Johannes"}],"citation-key":"hoferNewDBpediaRelease2020","container-title":"Semantic Systems. In the Era of Knowledge Graphs","DOI":"10.1007/978-3-030-59833-4_1","editor":[{"family":"Blomqvist","given":"Eva"},{"family":"Groth","given":"Paul"},{"family":"Boer","given":"Victor","non-dropping-particle":"de"},{"family":"Pellegrini","given":"Tassilo"},{"family":"Alam","given":"Mehwish"},{"family":"Käfer","given":"Tobias"},{"family":"Kieseberg","given":"Peter"},{"family":"Kirrane","given":"Sabrina"},{"family":"Meroño-Peñuela","given":"Albert"},{"family":"Pandit","given":"Harshvardhan J."}],"event-place":"Cham","ISBN":"978-3-030-59833-4","issued":{"date-parts":[["2020"]]},"language":"en","page":"1-18","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"The New DBpedia Release Cycle: Increasing Agility and Efficiency in Knowledge Extraction Workflows","title-short":"The New DBpedia Release Cycle","type":"paper-conference"},{"id":"HowOftenShould","abstract":"SurveyMonkey delivers People Powered Data to organizations around the world. From simple surveys to advanced feedback solutions and enterprise offerings, SurveyMonkey products work for any use case or budget","accessed":{"date-parts":[["2024",3,17]]},"citation-key":"HowOftenShould","container-title":"SurveyMonkey","language":"en-US","title":"How often should you use email reminders?","type":"webpage","URL":"https://www.surveymonkey.com/curiosity/how-often-should-you-use-email-reminders/"},{"id":"HttpBookValidatingrdf","accessed":{"date-parts":[["2024",2,16]]},"citation-key":"HttpBookValidatingrdf","title":"http://book.validatingrdf.com/bookHtml008.html#sec20","type":"webpage","URL":"http://book.validatingrdf.com/bookHtml008.html#sec20"},{"id":"HttpBookValidatingrdfa","accessed":{"date-parts":[["2024",2,16]]},"citation-key":"HttpBookValidatingrdfa","title":"http://book.validatingrdf.com/bookHtml009.html#sec37","type":"webpage","URL":"http://book.validatingrdf.com/bookHtml009.html#sec37"},{"id":"HttpsCeurwsOrg","accessed":{"date-parts":[["2024",7,12]]},"citation-key":"HttpsCeurwsOrg","title":"https://ceur-ws.org/Vol-3637/paper52.pdf","type":"webpage","URL":"https://ceur-ws.org/Vol-3637/paper52.pdf"},{"id":"HumanSignalLabelImg2023","abstract":"LabelImg is now part of the Label Studio community. The popular image annotation tool created by Tzutalin is no longer actively being developed, but you can check out Label Studio, the open source data labeling tool for images, text, hypertext, audio, video and time-series data.","accessed":{"date-parts":[["2023",9,26]]},"citation-key":"HumanSignalLabelImg2023","genre":"Python","issued":{"date-parts":[["2023",9,26]]},"license":"MIT","original-date":{"date-parts":[["2015",9,17]]},"publisher":"HumanSignal","source":"GitHub","title":"HumanSignal/labelImg","type":"software","URL":"https://github.com/HumanSignal/labelImg"},{"id":"IdentifiersOrg","accessed":{"date-parts":[["2024",2,16]]},"citation-key":"IdentifiersOrg","title":"Identifiers.org","type":"webpage","URL":"https://identifiers.org/"},{"id":"iglesias-molinaRMLstar2023","accessed":{"date-parts":[["2024",7,17]]},"author":[{"family":"Iglesias-Molina","given":"Ana"},{"family":"Arenas-Guerrero","given":"Julián"},{"family":"Delva","given":"Thomas"},{"family":"Dimou","given":"Anastasia"},{"family":"Chaves-Fraga","given":"David"}],"citation-key":"iglesias-molinaRMLstar2023","issued":{"date-parts":[["2023",10,5]]},"title":"RML-star","type":"webpage","URL":"https://kg-construct.github.io/rml-star/spec/docs/"},{"id":"ilyasSagaPlatformContinuous2022","abstract":"We introduce Saga, a next-generation knowledge construction and serving platform for powering knowledge-based applications at industrial scale. Saga follows a hybrid batch-incremental design to continuously integrate billions of facts about real-world entities and construct a central knowledge graph that supports multiple production use cases with diverse requirements around data freshness, accuracy, and availability. In this paper, we discuss the unique challenges associated with knowledge graph construction at industrial scale, and review the main components of Saga and how they address these challenges. Finally, we share lessons-learned from a wide array of production use cases powered by Saga.","accessed":{"date-parts":[["2024",6,27]]},"author":[{"family":"Ilyas","given":"Ihab F."},{"family":"Rekatsinas","given":"Theodoros"},{"family":"Konda","given":"Vishnu"},{"family":"Pound","given":"Jeffrey"},{"family":"Qi","given":"Xiaoguang"},{"family":"Soliman","given":"Mohamed"}],"citation-key":"ilyasSagaPlatformContinuous2022","container-title":"Proceedings of the 2022 International Conference on Management of Data","DOI":"10.1145/3514221.3526049","issued":{"date-parts":[["2022",6,10]]},"page":"2259-2272","source":"arXiv.org","title":"Saga: A Platform for Continuous Construction and Serving of Knowledge At Scale","title-short":"Saga","type":"paper-conference","URL":"http://arxiv.org/abs/2204.07309"},{"id":"ImportedHttpsWww","accessed":{"date-parts":[["2024",6,1]]},"citation-key":"ImportedHttpsWww","title":"Imported from https://www.apcjones.com/arrows/ - Arrows","type":"webpage","URL":"https://arrows.app/#/local/id=qPMEMMiz8xKHf94Tpbih"},{"id":"influxdbInfluxDBRealtimeInsights2022","abstract":"Manage all types of time series data in a single, purpose-built database. Optimized for speed in any environment in the cloud, on-premises, or at the edge.","accessed":{"date-parts":[["2024",6,23]]},"author":[{"family":"influxdb","given":""}],"citation-key":"influxdbInfluxDBRealtimeInsights2022","container-title":"InfluxData","issued":{"literal":"Sat, 15 Jan 2022 15:32:09 +0000"},"title":"InfluxDB | Real-time insights at any scale","type":"webpage","URL":"https://www.influxdata.com/home/"},{"id":"iso15936-1:2017ISO1583612017","abstract":"Information and documentation — The Dublin Core metadata element set — Part 1: Core elements","accessed":{"date-parts":[["2024",6,16]]},"author":[{"literal":"ISO 15936-1:2017"}],"citation-key":"iso15936-1:2017ISO1583612017","container-title":"ISO","language":"en","title":"ISO 15836-1:2017","title-short":"ISO 15836-1","type":"webpage","URL":"https://www.iso.org/standard/71339.html"},{"id":"iso15936-2:2019ISO1583622019","abstract":"Information and documentation — The Dublin Core metadata element set — Part 2: DCMI Properties and classes","accessed":{"date-parts":[["2024",6,16]]},"author":[{"literal":"ISO 15936-2:2019"}],"citation-key":"iso15936-2:2019ISO1583622019","container-title":"ISO","language":"en","title":"ISO 15836-2:2019","title-short":"ISO 15836-2","type":"webpage","URL":"https://www.iso.org/standard/71341.html"},{"id":"isoISOIEC25010","accessed":{"date-parts":[["2022",11,1]]},"author":[{"family":"ISO","given":""}],"citation-key":"isoISOIEC25010","title":"ISO/IEC 25010:2011(en), Systems and software engineering — Systems and software Quality Requirements and Evaluation (SQuaRE) — System and software quality models","type":"webpage","URL":"https://www.iso.org/obp/ui/#iso:std:iso-iec:25010:ed-1:v1:en"},{"id":"Jinja2VeryFast2024","citation-key":"Jinja2VeryFast2024","genre":"Python","issued":{"date-parts":[["2024"]]},"license":"OSI Approved :: BSD License","medium":"OS Independent","source":"PyPI","title":"Jinja2 - A very fast and expressive template engine.","title-short":"Jinja2","type":"software","version":"3.1.4"},{"id":"jsm2008HenchmanTierList2021","accessed":{"date-parts":[["2023",2,5]]},"author":[{"family":"jsm2008","given":""}],"citation-key":"jsm2008HenchmanTierList2021","container-title":"r/GuildWars","genre":"Reddit Post","issued":{"date-parts":[["2021",7,29]]},"title":"Henchman tier list (by profession)","type":"post","URL":"www.reddit.com/r/GuildWars/comments/ou0j1f/henchman_tier_list_by_profession/"},{"id":"jundFreiburgGroceriesDataset2016","author":[{"family":"Jund","given":"Philipp"},{"family":"Abdo","given":"Nichola"},{"family":"Eitel","given":"Andreas"},{"family":"Burgard","given":"Wolfram"}],"citation-key":"jundFreiburgGroceriesDataset2016","issued":{"date-parts":[["2016"]]},"title":"The Freiburg Groceries Dataset","type":"document"},{"id":"kanellopoulosCodeQualityEvaluation2010","abstract":"This work proposes a methodology for source code quality and static behaviour evaluation of a software system, based on the standard ISO/IEC-9126. It uses elements automatically derived from source code enhanced with expert knowledge in the form of quality characteristic rankings, allowing software engineers to assign weights to source code attributes. It is flexible in terms of the set of metrics and source code attributes employed, even in terms of the ISO/IEC-9126 characteristics to be assessed. We applied the methodology to two case studies, involving five open source and one proprietary system. Results demonstrated that the methodology can capture software quality trends and express expert perceptions concerning system quality in a quantitative and systematic manner.","accessed":{"date-parts":[["2022",11,1]]},"author":[{"family":"Kanellopoulos","given":"Yiannis"},{"family":"Antonellis","given":"Panos"},{"family":"Antoniou","given":"Dimitris"},{"family":"Makris","given":"Christos"},{"family":"Theodoridis","given":"Evangelos"},{"family":"Tjortjis","given":"Christos"},{"family":"Tsirakis","given":"Nikos"}],"citation-key":"kanellopoulosCodeQualityEvaluation2010","container-title":"International Journal of Software Engineering & Applications","container-title-short":"IJSEA","DOI":"10.5121/ijsea.2010.1302","ISSN":"09762221","issue":"3","issued":{"date-parts":[["2010",7,26]]},"language":"en","page":"17-36","source":"DOI.org (Crossref)","title":"Code Quality Evaluation Methodology Using The ISO/IEC 9126 Standard","type":"article-journal","URL":"http://www.airccse.org/journal/ijsea/papers/0710ijsea2.pdf","volume":"1"},{"id":"kernSustainableSoftwareProducts2018","abstract":"Many authors have proposed criteria to assess the ‘‘environmental friendliness’’ or ‘‘sustainability’’ of\nsoftware products. However, a causal model that links observable properties of a software product to\nconditions of it being green or (more general) sustainable is still missing. Such a causal model is necessary\nbecause software products are intangible goods and, as such, only have indirect effects on the physical\nworld. In particular, software products are not subject to any wear and tear, they can be copied without\ngreat effort, and generate no waste or emissions when being disposed of. Viewed in isolation, software\nseems to be a perfectly sustainable type of product. In real life, however, software products with the same\nor similar functionality can differ substantially in the burden they place on natural resources, especially\nif the sequence of released versions and resulting hardware obsolescence is taken into account. In this\narticle,wepresent a model describing the causal chains from software products to their impacts on natural\nresources, including energy sources, from a life-cycle perspective.Wefocus on (i) the demands of software\nfor hardware capacities (local, remote, and in the connecting network) and the resulting hardware energy\ndemand, (ii) the expectations of users regarding such demands and how these affect hardware operating\nlife, and (iii) the autonomy of users in managing their software use with regard to resource efficiency.\nWe propose a hierarchical set of criteria and indicators to assess these impacts. We demonstrate the\napplication of this set of criteria, including the definition of standard usage scenarios for chosen categories\nof software products. We further discuss the practicability of this type of assessment, its acceptability for\nseveral stakeholders and potential consequences for the eco-labeling of software products and sustainable\nsoftware design.","author":[{"family":"Kern","given":"Eva"},{"family":"Hilty","given":"Lorenz"},{"family":"Guldner","given":"Achim"},{"family":"Maksimov","given":"Yuliyan"},{"family":"Filler","given":"Andreas"},{"family":"Gröger","given":"Jens"},{"family":"Naumann","given":"Stefan"}],"citation-key":"kernSustainableSoftwareProducts2018","container-title":"Future Generation Computer Systems","container-title-short":"Future Generation Computer Systems","DOI":"10.1016/j.future.2018.02.044","issued":{"date-parts":[["2018",4,1]]},"page":"199-210","source":"ResearchGate","title":"Sustainable software products—Towards assessment criteria for resource and energy efficiency","type":"article-journal","volume":"2018"},{"id":"kimStaticProgramAnalysis2016","abstract":"A major drawback of mobile devices is limited battery life. Apps that use graphics are especially energy greedy and developers must invest significant effort to make such apps energy efficient. We propose a novel static optimization technique for eliminating drawing commands to produce energy-efficient apps. The key insight we exploit is that the static analysis is able to predict future behavior of the app, and we give three exemplars that demonstrate the value of this approach. Firstly, loop invariant texture analysis identifies repetitive texture transfers in the render loop so that they can be moved out of the loop and performed just once. Secondly, packing identifies images that are drawn together and therefore can be combined into a larger image to eliminate overhead associated with multiple smaller images. Finally, identical frames detection uses a combination of static and dynamic analysis to identify frames that are identical to the previous frame and therefore do not have to be drawn. We implemented the technique against LibGDX, an Android game engine, and evaluated it using open source projects. Our experiments indicate savings up to 44% of the total energy consumption of the device.","author":[{"family":"Kim","given":"Chang Hwan Peter"},{"family":"Kroening","given":"Daniel"},{"family":"Kwiatkowska","given":"Marta"}],"citation-key":"kimStaticProgramAnalysis2016","container-title":"2016 IEEE 24th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS)","DOI":"10.1109/MASCOTS.2016.28","event-title":"2016 IEEE 24th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS)","ISSN":"2375-0227","issued":{"date-parts":[["2016",9]]},"page":"115-124","source":"IEEE Xplore","title":"Static Program Analysis for Identifying Energy Bugs in Graphics-Intensive Mobile Apps","type":"paper-conference"},{"id":"kluyverJupyterNotebooksPublishing2016","abstract":"It is increasingly necessary for researchers in all fields to write computer code, and in order to reproduce research results, it is important that this code is published. We present Jupyter notebooks, a document format for publishing code, results and explanations in a form that is both readable and executable. We discuss various tools and use cases for notebook documents.","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Kluyver","given":"Thomas"},{"family":"Ragan-Kelley","given":"Benjain"},{"family":"Pérez","given":"Fernando"},{"family":"Granger","given":"Brian"},{"family":"Bussonnier","given":"Matthias"},{"family":"Frederic","given":"Jonathan"},{"family":"Kelley","given":"Kyle"},{"family":"Hamrick","given":"Jessica"},{"family":"Grout","given":"Jason"},{"family":"Corlay","given":"Sylvain"},{"family":"Ivanov","given":"Paul"},{"family":"Avila","given":"Damián"},{"family":"Abdalla","given":"Safia"},{"family":"Willing","given":"Carol"},{"literal":"Jupyter Development Team"}],"citation-key":"kluyverJupyterNotebooksPublishing2016","DOI":"10.3233/978-1-61499-649-1-87","issued":{"date-parts":[["2016",6,1]]},"note":"ADS Bibcode: 2016ppap.book...87K","page":"87-90","source":"NASA ADS","title":"Jupyter Notebooks—a publishing format for reproducible computational workflows","type":"book","URL":"https://ui.adsabs.harvard.edu/abs/2016ppap.book...87K"},{"id":"KnowledgeGraph2024","abstract":"In knowledge representation and reasoning, a knowledge graph is a knowledge base that uses a graph-structured data model or topology to represent and operate on data. Knowledge graphs are often used to store interlinked descriptions of entities –  objects, events, situations or abstract concepts –  while also encoding the semantics or relationships underlying these entities.Since the development of the Semantic Web, knowledge graphs have often been associated with linked open data projects, focusing on the connections between concepts and entities. They are also historically associated with and used by search engines such as Google, Bing, Yext and Yahoo; knowledge-engines and question-answering services such as WolframAlpha, Apple's Siri, and Amazon Alexa; and social networks such as LinkedIn and Facebook.\nRecent developments in data science and machine learning, particularly in graph neural networks and representation learning, have broadened the scope of knowledge graphs beyond their traditional use in search engines and recommender systems. They are increasingly used in scientific research, with notable applications in fields such as genomics, proteomics, and systems biology.","accessed":{"date-parts":[["2024",2,27]]},"citation-key":"KnowledgeGraph2024","container-title":"Wikipedia","issued":{"date-parts":[["2024",1,4]]},"language":"en","license":"Creative Commons Attribution-ShareAlike License","note":"Page Version ID: 1193493935","source":"Wikipedia","title":"Knowledge graph","type":"entry-encyclopedia","URL":"https://en.wikipedia.org/w/index.php?title=Knowledge_graph&oldid=1193493935"},{"id":"kochSiameseNeuralNetworks","abstract":"The process of learning good features for machine learning applications can be very computationally expensive and may prove difﬁcult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classiﬁcation tasks.","author":[{"family":"Koch","given":"Gregory"},{"family":"Zemel","given":"Richard"},{"family":"Salakhutdinov","given":"Ruslan"}],"citation-key":"kochSiameseNeuralNetworks","language":"en","title":"Siamese Neural Networks for One-shot Image Recognition","type":"article-journal"},{"id":"kochSiameseNeuralNetworksa","abstract":"The process of learning good features for machine learning applications can be very computationally expensive and may prove difﬁcult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classiﬁcation tasks.","author":[{"family":"Koch","given":"Gregory"},{"family":"Zemel","given":"Richard"},{"family":"Salakhutdinov","given":"Ruslan"}],"citation-key":"kochSiameseNeuralNetworksa","language":"en","source":"Zotero","title":"Siamese Neural Networks for One-shot Image Recognition","type":"article-journal"},{"id":"kpmgSupplyChainVisibility2022","author":[{"family":"KPMG","given":""}],"citation-key":"kpmgSupplyChainVisibility2022","issued":{"date-parts":[["2022"]]},"language":"en","source":"Zotero","title":"Supply chain visibility in the digital age","type":"article-journal"},{"id":"krosnickPromisesPitfallsUsing2023","abstract":"ChatGPT and other publicly available large language models (LLMs) put AI into the hands of everyday computer users, offering the possibility of automating computer tasks. One candidate task is web scraping. We informally experimented with ChatGPT to explore the potential promises and pitfalls of using it for scraping data from web user interfaces. We share our observations and considerations for future human-LLM web scraping systems.","author":[{"family":"Krosnick","given":"Rebecca"},{"family":"Oney","given":"Steve"}],"citation-key":"krosnickPromisesPitfallsUsing2023","issued":{"date-parts":[["2023"]]},"language":"en","source":"Zotero","title":"Promises and Pitfalls of Using LLMs for Scraping Web UIs","type":"article-journal"},{"id":"kumpel2023productkg","author":[{"family":"Kümpel","given":"Michaela"},{"family":"Beetz","given":"Michael"}],"citation-key":"kumpel2023productkg","issued":{"date-parts":[["2023"]]},"title":"ProductKG: a product knowledge graph for user assistance in daily activities.","type":"paper-conference"},{"id":"kumpelNonFoodKGNonFoodFood","accessed":{"date-parts":[["2022",12,9]]},"author":[{"family":"Kümpel","given":"Michaela"}],"citation-key":"kumpelNonFoodKGNonFoodFood","title":"NonFoodKG: A Non-Food to Food Product Knowledge Graph for Consumer Applications in Retail Environments \\textbar www.semantic-web-journal.net","type":"document","URL":"https://www.semantic-web-journal.net/content/nonfoodkg-non-food-food-product-knowledge-graph-consumer-applications-retail-environments"},{"id":"kumpelNonFoodKGNonFoodFooda","accessed":{"date-parts":[["2022",12,9]]},"author":[{"family":"Kümpel","given":"Michaela"}],"citation-key":"kumpelNonFoodKGNonFoodFooda","title":"NonFoodKG: A Non-Food to Food Product Knowledge Graph for Consumer Applications in Retail Environments | www.semantic-web-journal.net","type":"webpage","URL":"https://www.semantic-web-journal.net/content/nonfoodkg-non-food-food-product-knowledge-graph-consumer-applications-retail-environments"},{"id":"kumpelProductKGProductKnowledge","abstract":"The Web offers plenty of product information that is valuable for supporting decision processes. Research on Web knowledge acquisition and the Semantic Web has led to the creation of many domain ontologies and Web applications. What still is lacking is a connection of such knowledge to the real world. If object information is linked to environment information, users can get better, more personalised support in their daily activities like shopping or cooking since this enables them to link information about leftover products in the fridge to recipe information or a health profile to products the user is looking at in the store. It has been shown that semantic Digital Twins can successfully link object to environment information that can be used by agents like smartphone or service robot. Such semantic Digital Twins can offer even more services to users if they are connected to product information from the Web.","author":[{"family":"Kümpel","given":"Michaela"},{"family":"Beetz","given":"Michael"}],"citation-key":"kumpelProductKGProductKnowledge","language":"en","source":"Zotero","title":"ProductKG: A Product Knowledge Graph for User Assistance in Daily Activities","type":"article-journal"},{"id":"kumpelProductKGProductKnowledgea","abstract":"The Web offers plenty of product information that is valuable for supporting decision processes. Research on Web knowledge acquisition and the Semantic Web has led to the creation of many domain ontologies and Web applications. What still is lacking is a connection of such knowledge to the real world. If object information is linked to environment information, users can get better, more personalised support in their daily activities like shopping or cooking since this enables them to link information about leftover products in the fridge to recipe information or a health profile to products the user is looking at in the store. It has been shown that semantic Digital Twins can successfully link object to environment information that can be used by agents like smartphone or service robot. Such semantic Digital Twins can offer even more services to users if they are connected to product information from the Web.","author":[{"family":"Kümpel","given":"Michaela"},{"family":"Beetz","given":"Michael"}],"citation-key":"kumpelProductKGProductKnowledgea","language":"en","source":"Zotero","title":"ProductKG: A Product Knowledge Graph for User Assistance in Daily Activities","type":"article-journal"},{"id":"leavittWillNoSQLDatabases2010","abstract":"Organizations that collect large amounts of unstructured data are increasingly turning to nonrelational databases, now frequently called NoSQL databases.","accessed":{"date-parts":[["2024",7,9]]},"author":[{"family":"Leavitt","given":"Neal"}],"citation-key":"leavittWillNoSQLDatabases2010","container-title":"Computer","DOI":"10.1109/MC.2010.58","ISSN":"1558-0814","issue":"2","issued":{"date-parts":[["2010",2]]},"page":"12-14","source":"IEEE Xplore","title":"Will NoSQL Databases Live Up to Their Promise?","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/5410700","volume":"43"},{"id":"lehmannDBpediaLargescaleMultilingual2015","abstract":"The DBpedia community project extracts structured, multilingual knowledge from Wikipedia and makes it freely available on the Web using Semantic Web and Linked Data technologies. The project extracts knowledge from 111 different language editions of Wikipedia. The largest DBpedia knowledge base which is extracted from the English edition of Wikipedia consists of over 400 million facts that describe 3.7 million things. The DBpedia knowledge bases that are extracted from the other 110 Wikipedia editions together consist of 1.46 billion facts and describe 10 million additional things. The DBpedia project maps Wikipedia infoboxes from 27 different language editions to a single shared ontology consisting of 320 classes and 1,650 properties. The mappings are created via a world-wide crowd-sourcing effort and enable knowledge from the different Wikipedia editions to be combined. The project publishes releases of all DBpedia knowledge bases for download and provides SPARQL query access to 14 out of the 111 language editions via a global network of local DBpedia chapters. In addition to the regular releases, the project maintains a live knowledge base which is updated whenever a page in Wikipedia changes. DBpedia sets 27 million RDF links pointing into over 30 external data sources and thus enables data from these sources to be used together with DBpedia data. Several hundred data sets on the Web publish RDF links pointing to DBpedia themselves and make DBpedia one of the central interlinking hubs in the Linked Open Data (LOD) cloud. In this system report, we give an overview of the DBpedia community project, including its architecture, technical implementation, maintenance, internationalisation, usage statistics and applications.","accessed":{"date-parts":[["2024",2,12]]},"author":[{"family":"Lehmann","given":"Jens"},{"family":"Isele","given":"Robert"},{"family":"Jakob","given":"Max"},{"family":"Jentzsch","given":"Anja"},{"family":"Kontokostas","given":"Dimitris"},{"family":"Mendes","given":"Pablo N."},{"family":"Hellmann","given":"Sebastian"},{"family":"Morsey","given":"Mohamed"},{"family":"Van Kleef","given":"Patrick"},{"family":"Auer","given":"Sören"},{"family":"Bizer","given":"Christian"}],"citation-key":"lehmannDBpediaLargescaleMultilingual2015","container-title":"Semantic Web","DOI":"10.3233/SW-140134","ISSN":"15700844","issue":"2","issued":{"date-parts":[["2015"]]},"language":"en","page":"167-195","source":"DOI.org (Crossref)","title":"DBpedia – A large-scale, multilingual knowledge base extracted from Wikipedia","type":"article-journal","URL":"https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-140134","volume":"6"},{"id":"LevenshteinDistance2023","abstract":"In information theory, linguistics, and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. It is named after the Soviet mathematician Vladimir Levenshtein, who considered this distance in 1965.Levenshtein distance may also be referred to as edit distance, although that term may also denote a larger family of distance metrics known collectively as edit distance.: 32  It is closely related to pairwise string alignments.","accessed":{"date-parts":[["2023",10,4]]},"citation-key":"LevenshteinDistance2023","container-title":"Wikipedia","issued":{"date-parts":[["2023",9,29]]},"language":"en","license":"Creative Commons Attribution-ShareAlike License","note":"Page Version ID: 1177712548","source":"Wikipedia","title":"Levenshtein distance","type":"entry-encyclopedia","URL":"https://en.wikipedia.org/w/index.php?title=Levenshtein_distance&oldid=1177712548"},{"id":"LevenshteinModuleLevenshtein","accessed":{"date-parts":[["2023",3,27]]},"citation-key":"LevenshteinModuleLevenshtein","title":"Levenshtein module — Levenshtein 0.20.9 documentation","type":"webpage","URL":"https://maxbachmann.github.io/Levenshtein/levenshtein.html#ratio"},{"id":"liAliMeKGDomainKnowledge2020","abstract":"Pre-sales customer service is of importance to E-commerce platforms as it contributes to optimizing customers’ buying process. To better serve users, we propose AliMe KG, a domain knowledge graph in E-commerce that captures user problems, points of interests (POI), item information and relations thereof. It helps to understand user needs, answer pre-sales questions and generate explanation texts. We applied AliMe KG to several online business scenarios such as shopping guide, question answering over properties and recommendation reason generation, and gained positive results. In the paper, we systematically introduce how we construct domain knowledge graph from free text, and demonstrate its business value with several applications. Our experience shows that mining structured knowledge from free text in vertical domain is practicable, and can be of substantial value in industrial settings.","accessed":{"date-parts":[["2024",2,4]]},"author":[{"family":"Li","given":"Feng-Lin"},{"family":"Chen","given":"Hehong"},{"family":"Xu","given":"Guohai"},{"family":"Qiu","given":"Tian"},{"family":"Ji","given":"Feng"},{"family":"Zhang","given":"Ji"},{"family":"Chen","given":"Haiqing"}],"citation-key":"liAliMeKGDomainKnowledge2020","container-title":"Proceedings of the 29th ACM International Conference on Information & Knowledge Management","DOI":"10.1145/3340531.3412685","event-place":"Virtual Event Ireland","event-title":"CIKM '20: The 29th ACM International Conference on Information and Knowledge Management","ISBN":"978-1-4503-6859-9","issued":{"date-parts":[["2020",10,19]]},"language":"en","page":"2581-2588","publisher":"ACM","publisher-place":"Virtual Event Ireland","source":"DOI.org (Crossref)","title":"AliMeKG: Domain Knowledge Graph Construction and Application in E-commerce","title-short":"AliMeKG","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3340531.3412685"},{"id":"lindenbergerLightGlueLocalFeature2023","abstract":"We introduce LightGlue, a deep neural network that learns to match local features across images. We revisit multiple design decisions of SuperGlue, the state of the art in sparse matching, and derive simple but effective improvements. Cumulatively, they make LightGlue more efficient – in terms of both memory and computation, more accurate, and much easier to train. One key property is that LightGlue is adaptive to the difficulty of the problem: the inference is much faster on image pairs that are intuitively easy to match, for example because of a larger visual overlap or limited appearance change. This opens up exciting prospects for deploying deep matchers in latency-sensitive applications like 3D reconstruction. The code and trained models are publicly available at github.com/cvg/LightGlue.","accessed":{"date-parts":[["2023",9,9]]},"author":[{"family":"Lindenberger","given":"Philipp"},{"family":"Sarlin","given":"Paul-Edouard"},{"family":"Pollefeys","given":"Marc"}],"citation-key":"lindenbergerLightGlueLocalFeature2023","issued":{"date-parts":[["2023",6,23]]},"language":"en","number":"arXiv:2306.13643","publisher":"arXiv","source":"arXiv.org","title":"LightGlue: Local Feature Matching at Light Speed","title-short":"LightGlue","type":"article","URL":"http://arxiv.org/abs/2306.13643"},{"id":"lindenbergerLightGlueLocalFeature2023a","abstract":"We introduce LightGlue, a deep neural network that learns to match local features across images. We revisit multiple design decisions of SuperGlue, the state of the art in sparse matching, and derive simple but effective improvements. Cumulatively, they make LightGlue more efficient – in terms of both memory and computation, more accurate, and much easier to train. One key property is that LightGlue is adaptive to the difficulty of the problem: the inference is much faster on image pairs that are intuitively easy to match, for example because of a larger visual overlap or limited appearance change. This opens up exciting prospects for deploying deep matchers in latency-sensitive applications like 3D reconstruction. The code and trained models are publicly available at github.com/cvg/LightGlue.","accessed":{"date-parts":[["2023",9,9]]},"author":[{"family":"Lindenberger","given":"Philipp"},{"family":"Sarlin","given":"Paul-Edouard"},{"family":"Pollefeys","given":"Marc"}],"citation-key":"lindenbergerLightGlueLocalFeature2023a","issued":{"date-parts":[["2023",6,23]]},"language":"en","number":"arXiv:2306.13643","publisher":"arXiv","source":"arXiv.org","title":"LightGlue: Local Feature Matching at Light Speed","title-short":"LightGlue","type":"article","URL":"http://arxiv.org/abs/2306.13643"},{"id":"LinkedOpenData2023","abstract":"Linked Open Data (LOD) bezeichnet im World Wide Web frei verfügbare Daten, die per Uniform Resource Identifier (URI) identifiziert sind und darüber direkt per HTTP abgerufen werden können und ebenfalls per URI auf andere Daten verweisen. Auch die Semantik und Verknüpfung der kodierten Datenpunkte ist über Technologien wie das Resource Description Framework (RDF) und darauf aufbauende Standards wie die Web Ontology Language (OWL) und die Abfragesprache SPARQL maschinell les- und verarbeitbar. \nLinked Open Data folgt damit den Prinzipien des Semantic Web. Die miteinander verknüpften Daten ergeben ein weltweites Netz, das auch als „Linked [Open] Data Cloud“, „Giant Global Graph“ oder „Knowledge Graph“ bezeichnet wird. Dort, wo der Schwerpunkt weniger auf der freien Nutzbarkeit der Daten wie bei freien Inhalten liegt (Open Data), ist auch die Bezeichnung Linked Data üblich.","accessed":{"date-parts":[["2024",2,17]]},"citation-key":"LinkedOpenData2023","container-title":"Wikipedia","issued":{"date-parts":[["2023",10,10]]},"language":"de","license":"Creative Commons Attribution-ShareAlike License","note":"Page Version ID: 238032371","source":"Wikipedia","title":"Linked Open Data","type":"entry-encyclopedia","URL":"https://de.wikipedia.org/w/index.php?title=Linked_Open_Data&oldid=238032371"},{"id":"liqatInferringEnergyBounds2018","abstract":"The ever increasing number and complexity of energy-bound devices (such as the ones used in Internet of Things applications, smart phones, and mission critical systems) pose an important challenge on techniques to optimize their energy consumption and to verify that they will perform their function within the available energy budget. In this work we address this challenge from the software point of view and propose a novel approach to estimating accurate parametric bounds on the energy consumed by program executions that are practical for their application to energy verification and optimization. Our approach divides a program into basic (branchless) blocks and performs a best effort modeling to estimate upper and lower bounds on the energy consumption for each block using an evolutionary algorithm. Then it combines the obtained values according to the program control flow, using a safe static analysis, to infer functions that give both upper and lower bounds on the energy consumption of the whole program and its procedures as functions on input data sizes. We have tested our approach on (C-like) embedded programs running on the XMOS hardware platform. However, our method is general enough to be applied to other microprocessor architectures and programming languages. The bounds obtained by our prototype implementation on a set of benchmarks were always safe and quite accurate. This supports our hypothesis that our approach offers a good compromise between safety and accuracy, and can be applied in practice for energy verification and optimization.","author":[{"family":"Liqat","given":"Umer"},{"family":"Banković","given":"Zorana"},{"family":"Lopez-Garcia","given":"Pedro"},{"family":"Hermenegildo","given":"Manuel V."}],"citation-key":"liqatInferringEnergyBounds2018","collection-title":"Lecture Notes in Computer Science","container-title":"Logic-Based Program Synthesis and Transformation","DOI":"10.1007/978-3-319-94460-9_4","editor":[{"family":"Fioravanti","given":"Fabio"},{"family":"Gallagher","given":"John P."}],"event-place":"Cham","ISBN":"978-3-319-94460-9","issued":{"date-parts":[["2018"]]},"language":"en","page":"54-72","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"Inferring Energy Bounds via Static Program Analysis and Evolutionary Modeling of Basic Blocks","type":"paper-conference"},{"id":"liStaticAnalysisAndroid2017","abstract":"Context\nStatic analysis exploits techniques that parse program source code or bytecode, often traversing program paths to check some program properties. Static analysis approaches have been proposed for different tasks, including for assessing the security of Android apps, detecting app clones, automating test cases generation, or for uncovering non-functional issues related to performance or energy. The literature thus has proposed a large body of works, each of which attempts to tackle one or more of the several challenges that program analyzers face when dealing with Android apps.\nObjective\nWe aim to provide a clear view of the state-of-the-art works that statically analyze Android apps, from which we highlight the trends of static analysis approaches, pinpoint where the focus has been put, and enumerate the key aspects where future researches are still needed.\nMethod\nWe have performed a systematic literature review (SLR) which involves studying 124 research papers published in software engineering, programming languages and security venues in the last 5 years (January 2011–December 2015). This review is performed mainly in five dimensions: problems targeted by the approach, fundamental techniques used by authors, static analysis sensitivities considered, android characteristics taken into account and the scale of evaluation performed.\nResults\nOur in-depth examination has led to several key findings: 1) Static analysis is largely performed to uncover security and privacy issues; 2) The Soot framework and the Jimple intermediate representation are the most adopted basic support tool and format, respectively; 3) Taint analysis remains the most applied technique in research approaches; 4) Most approaches support several analysis sensitivities, but very few approaches consider path-sensitivity; 5) There is no single work that has been proposed to tackle all challenges of static analysis that are related to Android programming; and 6) Only a small portion of state-of-the-art works have made their artifacts publicly available.\nConclusion\nThe research community is still facing a number of challenges for building approaches that are aware altogether of implicit-Flows, dynamic code loading features, reflective calls, native code and multi-threading, in order to implement sound and highly precise static analyzers.","accessed":{"date-parts":[["2023",7,25]]},"author":[{"family":"Li","given":"Li"},{"family":"Bissyandé","given":"Tegawendé F."},{"family":"Papadakis","given":"Mike"},{"family":"Rasthofer","given":"Siegfried"},{"family":"Bartel","given":"Alexandre"},{"family":"Octeau","given":"Damien"},{"family":"Klein","given":"Jacques"},{"family":"Traon","given":"Le"}],"citation-key":"liStaticAnalysisAndroid2017","container-title":"Information and Software Technology","container-title-short":"Information and Software Technology","DOI":"10.1016/j.infsof.2017.04.001","ISSN":"0950-5849","issued":{"date-parts":[["2017",8,1]]},"language":"en","page":"67-95","source":"ScienceDirect","title":"Static analysis of android apps: A systematic literature review","title-short":"Static analysis of android apps","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0950584917302987","volume":"88"},{"id":"ListWikipediasMeta","accessed":{"date-parts":[["2024",6,18]]},"citation-key":"ListWikipediasMeta","language":"en","title":"List of Wikipedias - Meta","type":"webpage","URL":"https://meta.wikimedia.org/wiki/List_of_Wikipedias"},{"id":"liuPlanogramComplianceChecking2016","abstract":"In this paper, a novel method for automatic planogram compliance checking in retail chains is proposed without requiring product template images for training. Product layout is extracted from an input image by means of unsupervised recurring pattern detection and matched via graph matching with the expected product layout specified by a planogram to measure the level of compliance. A divide and conquer strategy is employed to improve the speed. Specifically, the input image is divided into several regions based on the planogram. Recurring patterns are detected in each region respectively and then merged together to estimate the product layout. Experimental results on real data have verified the efficacy of the proposed method. Compared with a template-based method, higher accuracies are achieved by the proposed method over a wide range of products.","accessed":{"date-parts":[["2023",2,15]]},"author":[{"family":"Liu","given":"Song"},{"family":"Li","given":"Wanqing"},{"family":"Davis","given":"Stephen"},{"family":"Ritz","given":"Christian"},{"family":"Tian","given":"Hongda"}],"citation-key":"liuPlanogramComplianceChecking2016","container-title":"IEEE MultiMedia","DOI":"10.1109/MMUL.2016.19","ISSN":"1070-986X","issue":"2","issued":{"date-parts":[["2016",4]]},"language":"en","page":"54–63","title":"Planogram Compliance Checking Based on Detection of Recurring Patterns","type":"article-journal","URL":"http://arxiv.org/abs/1602.06647","volume":"23"},{"id":"liuPlanogramComplianceChecking2016a","abstract":"In this paper, a novel method for automatic planogram compliance checking in retail chains is proposed without requiring product template images for training. Product layout is extracted from an input image by means of unsupervised recurring pattern detection and matched via graph matching with the expected product layout specified by a planogram to measure the level of compliance. A divide and conquer strategy is employed to improve the speed. Specifically, the input image is divided into several regions based on the planogram. Recurring patterns are detected in each region respectively and then merged together to estimate the product layout. Experimental results on real data have verified the efficacy of the proposed method. Compared with a template-based method, higher accuracies are achieved by the proposed method over a wide range of products.","accessed":{"date-parts":[["2023",2,15]]},"author":[{"family":"Liu","given":"Song"},{"family":"Li","given":"Wanqing"},{"family":"Davis","given":"Stephen"},{"family":"Ritz","given":"Christian"},{"family":"Tian","given":"Hongda"}],"citation-key":"liuPlanogramComplianceChecking2016a","container-title":"IEEE MultiMedia","container-title-short":"IEEE MultiMedia","DOI":"10.1109/MMUL.2016.19","ISSN":"1070-986X","issue":"2","issued":{"date-parts":[["2016",4]]},"language":"en","page":"54-63","source":"arXiv.org","title":"Planogram Compliance Checking Based on Detection of Recurring Patterns","type":"article-journal","URL":"http://arxiv.org/abs/1602.06647","volume":"23"},{"id":"liuRelationalMemoryAugmented2022","abstract":"We present a memory-augmented approach to condition an autoregressive language model on a knowledge graph. We represent the graph as a collection of relation triples and retrieve relevant relations for a given context to improve text generation. Experiments on WikiText-103, WMT19, and enwik8 English datasets demonstrate that our approach produces a better language model in terms of perplexity and bits per character. We also show that relational memory improves coherence, is complementary to token-based memory, and enables causal interventions. Our model provides a simple yet effective way to combine an autoregressive language model and a knowledge graph for more coherent and logical generation.","accessed":{"date-parts":[["2024",2,12]]},"author":[{"family":"Liu","given":"Qi"},{"family":"Yogatama","given":"Dani"},{"family":"Blunsom","given":"Phil"}],"citation-key":"liuRelationalMemoryAugmented2022","issued":{"date-parts":[["2022",1,24]]},"language":"en","number":"arXiv:2201.09680","publisher":"arXiv","source":"arXiv.org","title":"Relational Memory Augmented Language Models","type":"article","URL":"http://arxiv.org/abs/2201.09680"},{"id":"LostTreasureGuild","accessed":{"date-parts":[["2023",2,1]]},"citation-key":"LostTreasureGuild","language":"en","title":"Lost Treasure - Guild Wars Wiki (GWW)","type":"webpage","URL":"https://wiki.guildwars.com/wiki/Lost_Treasure"},{"id":"loweDistinctiveImageFeatures2004","abstract":"This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.","accessed":{"date-parts":[["2023",3,7]]},"author":[{"family":"Lowe","given":"David G."}],"citation-key":"loweDistinctiveImageFeatures2004","container-title":"International Journal of Computer Vision","DOI":"10.1023/B:VISI.0000029664.99615.94","ISSN":"1573-1405","issue":"2","issued":{"date-parts":[["2004",11]]},"language":"en","page":"91–110","title":"Distinctive Image Features from Scale-Invariant Keypoints","type":"article-journal","URL":"https://doi.org/10.1023/B:VISI.0000029664.99615.94","volume":"60"},{"id":"loweDistinctiveImageFeatures2004a","abstract":"This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.","accessed":{"date-parts":[["2023",3,7]]},"author":[{"family":"Lowe","given":"David G."}],"citation-key":"loweDistinctiveImageFeatures2004a","container-title":"International Journal of Computer Vision","container-title-short":"International Journal of Computer Vision","DOI":"10.1023/B:VISI.0000029664.99615.94","ISSN":"1573-1405","issue":"2","issued":{"date-parts":[["2004",11,1]]},"language":"en","page":"91-110","source":"Springer Link","title":"Distinctive Image Features from Scale-Invariant Keypoints","type":"article-journal","URL":"https://doi.org/10.1023/B:VISI.0000029664.99615.94","volume":"60"},{"id":"loweObjectRecognitionLocal1999","abstract":"An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds.","author":[{"family":"Lowe","given":"D.G."}],"citation-key":"loweObjectRecognitionLocal1999","container-title":"Proceedings of the Seventh IEEE International Conference on Computer Vision","DOI":"10.1109/ICCV.1999.790410","issued":{"date-parts":[["1999",9]]},"page":"1150–1157 vol.2","title":"Object recognition from local scale-invariant features","type":"paper-conference","volume":"2"},{"id":"loweObjectRecognitionLocal1999a","abstract":"An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds.","author":[{"family":"Lowe","given":"D.G."}],"citation-key":"loweObjectRecognitionLocal1999a","container-title":"Proceedings of the Seventh IEEE International Conference on Computer Vision","DOI":"10.1109/ICCV.1999.790410","event-title":"Proceedings of the Seventh IEEE International Conference on Computer Vision","issued":{"date-parts":[["1999",9]]},"page":"1150-1157 vol.2","source":"IEEE Xplore","title":"Object recognition from local scale-invariant features","type":"paper-conference","volume":"2"},{"id":"lueddersFragebogenUndLeitfadenkonstruktion2016","author":[{"family":"Lüdders","given":"Lisa"}],"citation-key":"lueddersFragebogenUndLeitfadenkonstruktion2016","collection-title":"Methodenbuch","edition":"1. Auflage","event-place":"Bremen","ISBN":"978-3-943001-24-2","issued":{"date-parts":[["2016"]]},"language":"ger","number-of-pages":"167","publisher":"Apollon University Press","publisher-place":"Bremen","source":"K10plus ISBN","title":"Fragebogen- und Leitfadenkonstruktion: ein Handbuch für Studium und Berufspraxis","title-short":"Fragebogen- und Leitfadenkonstruktion","type":"book"},{"id":"luGraphbasedMultilingualProduct2021","accessed":{"date-parts":[["2024",2,4]]},"author":[{"family":"Lu","given":"Hanqing"},{"family":"Hu","given":"Youna"},{"family":"Zhao","given":"Tong"},{"family":"Wu","given":"Tony"},{"family":"Song","given":"Yiwei"},{"family":"Yin","given":"Bing"}],"citation-key":"luGraphbasedMultilingualProduct2021","container-title":"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers","DOI":"10.18653/v1/2021.naacl-industry.19","event-place":"Online","event-title":"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers","issued":{"date-parts":[["2021"]]},"language":"en","page":"146-153","publisher":"Association for Computational Linguistics","publisher-place":"Online","source":"DOI.org (Crossref)","title":"Graph-based Multilingual Product Retrieval in E-Commerce Search","type":"paper-conference","URL":"https://www.aclweb.org/anthology/2021.naacl-industry.19"},{"id":"luoAliCoCoAlibabaEcommerce2020","abstract":"One of the ultimate goals of e-commerce platforms is to satisfy various shopping needs for their customers. Much efforts are devoted to creating taxonomies or ontologies in e-commerce towards this goal. However, user needs in e-commerce are still not well defined, and none of the existing ontologies has the enough depth and breadth for universal user needs understanding. The semantic gap in-between prevents shopping experience from being more intelligent. In this paper, we propose to construct a large-scale e-commerce cognitive concept net named \"AliCoCo\", which is practiced in Alibaba, the largest Chinese e-commerce platform in the world. We formally define user needs in e-commerce, then conceptualize them as nodes in the net. We present details on how AliCoCo is constructed semi-automatically and its successful, ongoing and potential applications in e-commerce.","accessed":{"date-parts":[["2024",7,11]]},"author":[{"family":"Luo","given":"Xusheng"},{"family":"Liu","given":"Luxin"},{"family":"Yang","given":"Yonghua"},{"family":"Bo","given":"Le"},{"family":"Cao","given":"Yuanpeng"},{"family":"Wu","given":"Jinhang"},{"family":"Li","given":"Qiang"},{"family":"Yang","given":"Keping"},{"family":"Zhu","given":"Kenny Q."}],"citation-key":"luoAliCoCoAlibabaEcommerce2020","DOI":"10.48550/arXiv.2003.13230","issued":{"date-parts":[["2020",3,30]]},"number":"arXiv:2003.13230","publisher":"arXiv","source":"arXiv.org","title":"AliCoCo: Alibaba E-commerce Cognitive Concept Net","title-short":"AliCoCo","type":"article","URL":"http://arxiv.org/abs/2003.13230"},{"id":"LuxonScavengerGuild","accessed":{"date-parts":[["2023",2,1]]},"citation-key":"LuxonScavengerGuild","language":"en","title":"Luxon Scavenger - Guild Wars Wiki (GWW)","type":"webpage","URL":"https://wiki.guildwars.com/wiki/Luxon_Scavenger"},{"id":"MacroSpaceManagement","accessed":{"date-parts":[["2023",3,12]]},"citation-key":"MacroSpaceManagement","title":"Macro Space Management","type":"document","URL":"http://www.cms-svetovanje.si/en/services-and-knowledge/macro-space-management"},{"id":"madnickOverviewFrameworkData2009","abstract":"Awareness of data and information quality issues has grown rapidly in light of the critical role played by the quality of information in our data-intensive, knowledge-based economy. Research in the past two decades has produced a large body of data quality knowledge and has expanded our ability to solve many data and information quality problems. In this article, we present an overview of the evolution and current landscape of data and information quality research. We introduce a framework to characterize the research along two dimensions: topics and methods. Representative papers are cited for purposes of illustrating the issues addressed and the methods used. We also identify and discuss challenges to be addressed in future research.","accessed":{"date-parts":[["2024",6,27]]},"author":[{"family":"Madnick","given":"Stuart E."},{"family":"Wang","given":"Richard Y."},{"family":"Lee","given":"Yang W."},{"family":"Zhu","given":"Hongwei"}],"citation-key":"madnickOverviewFrameworkData2009","container-title":"J. Data and Information Quality","DOI":"10.1145/1515693.1516680","ISSN":"1936-1955","issue":"1","issued":{"date-parts":[["2009",6,1]]},"page":"2:1–2:22","source":"ACM Digital Library","title":"Overview and Framework for Data and Information Quality Research","type":"article-journal","URL":"https://doi.org/10.1145/1515693.1516680","volume":"1"},{"id":"ManagingFieldTeam","accessed":{"date-parts":[["2023",3,27]]},"citation-key":"ManagingFieldTeam","language":"en-US","title":"Managing a field team, merchandising and store audit. Retail execution software. – Free Merchandising App for CPG manufacturer, BTL and Marketing agency, broker/distributor","type":"post-weblog","URL":"https://www.visitbasis.com/"},{"id":"maoOctetOnlineCatalog2020","abstract":"Taxonomies have found wide applications in various domains, especially online for item categorization, browsing, and search. Despite the prevalent use of online catalog taxonomies, most of them in practice are maintained by humans, which is labor-intensive and difficult to scale. While taxonomy construction from scratch is considerably studied in the literature, how to effectively enrich existing incomplete taxonomies remains an open yet important research question. Taxonomy enrichment not only requires the robustness to deal with emerging terms but also the consistency between existing taxonomy structure and new term attachment. In this paper, we present a self-supervised end-to-end framework, Octet, for Online Catalog Taxonomy EnrichmenT. Octet leverages heterogeneous information unique to online catalog taxonomies such as user queries, items, and their relations to the taxonomy nodes while requiring no other supervision than the existing taxonomies. We propose to distantly train a sequence labeling model for term extraction and employ graph neural networks (GNNs) to capture the taxonomy structure as well as the query-item-taxonomy interactions for term attachment. Extensive experiments in different online domains demonstrate the superiority of Octet over state-of-the-art methods via both automatic and human evaluations. Notably, Octet enriches an online catalog taxonomy in production to 2 times larger in the open-world evaluation.","accessed":{"date-parts":[["2024",2,12]]},"author":[{"family":"Mao","given":"Yuning"},{"family":"Zhao","given":"Tong"},{"family":"Kan","given":"Andrey"},{"family":"Zhang","given":"Chenwei"},{"family":"Dong","given":"Xin Luna"},{"family":"Faloutsos","given":"Christos"},{"family":"Han","given":"Jiawei"}],"citation-key":"maoOctetOnlineCatalog2020","container-title":"Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining","DOI":"10.1145/3394486.3403274","issued":{"date-parts":[["2020",8,23]]},"page":"2247-2257","source":"arXiv.org","title":"Octet: Online Catalog Taxonomy Enrichment with Self-Supervision","title-short":"Octet","type":"paper-conference","URL":"http://arxiv.org/abs/2006.10276"},{"id":"marzilianoNoReferencePerceptualBlur2002","abstract":"We present a no-reference blur metric for images and video. The blur metric is based on the analysis of the spread of the edges in an image. Its perceptual significance is validated through subjective experiments. The novel metric is near real-time, has low computational complexity and is shown to perform well over a range of image content. Potential applications include optimization of source coding, network resource management and autofocus of an image capturing device.","author":[{"family":"Marziliano","given":"Pina"},{"family":"Dufaux","given":"Frederic"},{"family":"Winkler","given":"Stefan"},{"family":"Ebrahimi","given":"Touradj"}],"citation-key":"marzilianoNoReferencePerceptualBlur2002","DOI":"10.1109/ICIP.2002.1038902","event-title":"International Conference on Image Processing, Rochester, NY","ISBN":"978-0-7803-7622-9","issued":{"date-parts":[["2002",2,1]]},"page":"III-57","source":"ResearchGate","title":"A No-Reference Perceptual Blur Metric","type":"paper-conference","volume":"53"},{"id":"Mck_retailops2020_fullissuergbhyperlinks011620Pdf","accessed":{"date-parts":[["2024",2,21]]},"citation-key":"Mck_retailops2020_fullissuergbhyperlinks011620Pdf","title":"mck_retail-ops-2020_fullissue-rgb-hyperlinks-011620.pdf","type":"webpage","URL":"https://www.mckinsey.com/~/media/mckinsey/industries/retail/our%20insights/future%20of%20retail%20operations%20winning%20in%20a%20digital%20era/mck_retail-ops-2020_fullissue-rgb-hyperlinks-011620.pdf"},{"id":"mckinneyPythonDataAnalysis2022","abstract":"Get the definitive handbook for manipulating, processing, cleaning, and crunching datasets in Python. Updated for Python 3.10 and pandas 1.4, the third edition of this hands-on guide is packed with practical case studies that show you how to solve a broad set of data analysis problems effectively. You'll learn the latest versions of pandas, NumPy, and Jupyter in the process.  Written by Wes McKinney, the creator of the Python pandas project, this book is a practical, modern introduction to data science tools in Python. It's ideal for analysts new to Python and for Python programmers new to data science and scientific computing. Data files and related material are available on GitHub. Use the Jupyter notebook and IPython shell for exploratory computing Learn basic and advanced features in NumPy Get started with data analysis tools in the pandas library Use flexible tools to load, clean, transform, merge, and reshape data Create informative visualizations with matplotlib Apply the pandas groupby facility to slice, dice, and summarize datasets Analyze and manipulate regular and irregular time series data Learn how to solve real-world data analysis problems with thorough, detailed examples","author":[{"family":"McKinney","given":"Wes"}],"citation-key":"mckinneyPythonDataAnalysis2022","edition":"3rd edition","event-place":"Beijing Boston Farnham Sebastopol Tokyo","ISBN":"978-1-0981-0403-0","issued":{"date-parts":[["2022",9,20]]},"language":"English","number-of-pages":"579","publisher":"O'Reilly Media","publisher-place":"Beijing Boston Farnham Sebastopol Tokyo","source":"Amazon","title":"Python for Data Analysis: Data Wrangling with pandas, NumPy, and Jupyter","title-short":"Python for Data Analysis","type":"book"},{"id":"mckinsey&companyElevatingMasterData2024","accessed":{"date-parts":[["2024",7,8]]},"author":[{"family":"McKinsey & Company","given":""}],"citation-key":"mckinsey&companyElevatingMasterData2024","issued":{"date-parts":[["2024",5]]},"title":"Elevating master data management in an organization | McKinsey","type":"webpage","URL":"https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/master-data-management-the-key-to-getting-more-from-your-data"},{"id":"mediacenterErsterAutonomerREWE","abstract":"Schnelles Einkaufen und kassenloses Bezahlen jetzt auch in Bayerns Landeshauptstadt möglich","accessed":{"date-parts":[["2023",2,13]]},"author":[{"family":"mediacenter","given":""}],"citation-key":"mediacenterErsterAutonomerREWE","container-title":"REWE Presse","language":"de","title":"Erster autonomer REWE Pick&Go-Markt in München eröffnet","type":"webpage","URL":"https://mediacenter.rewe.de/pressemitteilungen/rewe-pick-and-go-muenchen"},{"id":"merkelDockerLightweightLinux2014","abstract":"Docker promises the ability to package applications and their dependencies into lightweight containers that move easily between different distros, start up quickly and are isolated from each other.","author":[{"family":"Merkel","given":"Dirk"}],"citation-key":"merkelDockerLightweightLinux2014","container-title":"Linux J.","ISSN":"1075-3583","issue":"239","issued":{"date-parts":[["2014",3,1]]},"page":"2:2","source":"ACM Digital Library","title":"Docker: lightweight Linux containers for consistent development and deployment","title-short":"Docker","type":"article-journal","volume":"2014"},{"id":"metralabsProfessionelleMobileServiceRoboter","abstract":"MetraLabs entwickelt mobile Roboterplattformen und Servicerobotikanwendungen für den Einzelhandel, für Forschungseinrichtungen und die Industrie.","accessed":{"date-parts":[["2022",12,6]]},"author":[{"family":"metralabs","given":""}],"citation-key":"metralabsProfessionelleMobileServiceRoboter","language":"de-DE","title":"Professionelle mobile Service-Roboter - MetraLabs","type":"post-weblog","URL":"https://www.metralabs.com/"},{"id":"millerWordNetLexicalDatabase1995","abstract":"Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].","accessed":{"date-parts":[["2024",7,30]]},"author":[{"family":"Miller","given":"George A."}],"citation-key":"millerWordNetLexicalDatabase1995","container-title":"Commun. ACM","DOI":"10.1145/219717.219748","ISSN":"0001-0782","issue":"11","issued":{"date-parts":[["1995",11,1]]},"page":"39–41","source":"ACM Digital Library","title":"WordNet: a lexical database for English","title-short":"WordNet","type":"article-journal","URL":"https://dl.acm.org/doi/10.1145/219717.219748","volume":"38"},{"id":"MongoDBDatenplattformFuer","abstract":"Mit einer Entwicklerdatenplattform, die auf der führenden modernen Datenbank basiert, bringen Sie Ihre Ideen schneller auf den Markt. Verwenden Sie für Ihre Transaktions-, Such-, Analyse- und Mobilanwendungen eine gemeinsame Abfrageoberfläche, die auf den von Entwicklern bevorzugten Datenmodellen beruht.","accessed":{"date-parts":[["2024",6,14]]},"citation-key":"MongoDBDatenplattformFuer","container-title":"MongoDB","language":"de","title":"MongoDB: Die Datenplattform Für Entwickler","title-short":"MongoDB","type":"webpage","URL":"https://www.mongodb.com/de-de"},{"id":"monteiroUltraprocessedFoodsWhat2019","abstract":"The present commentary contains a clear and simple guide designed to identify ultra-processed foods. It responds to the growing interest in ultra-processed foods among policy makers, academic researchers, health professionals, journalists and consumers concerned to devise policies, investigate dietary patterns, advise people, prepare media coverage, and when buying food and checking labels in shops or at home. Ultra-processed foods are defined within the NOVA classification system, which groups foods according to the extent and purpose of industrial processing. Processes enabling the manufacture of ultra-processed foods include the fractioning of whole foods into substances, chemical modifications of these substances, assembly of unmodified and modified food substances, frequent use of cosmetic additives and sophisticated packaging. Processes and ingredients used to manufacture ultra-processed foods are designed to create highly profitable (low-cost ingredients, long shelf-life, emphatic branding), convenient (ready-to-consume), hyper-palatable products liable to displace all other NOVA food groups, notably unprocessed or minimally processed foods. A practical way to identify an ultra-processed product is to check to see if its list of ingredients contains at least one item characteristic of the NOVA ultra-processed food group, which is to say, either food substances never or rarely used in kitchens (such as high-fructose corn syrup, hydrogenated or interesterified oils, and hydrolysed proteins), or classes of additives designed to make the final product palatable or more appealing (such as flavours, flavour enhancers, colours, emulsifiers, emulsifying salts, sweeteners, thickeners, and anti-foaming, bulking, carbonating, foaming, gelling and glazing agents).","author":[{"family":"Monteiro","given":"Carlos A."},{"family":"Cannon","given":"Geoffrey"},{"family":"Levy","given":"Renata B."},{"family":"Moubarac","given":"Jean-Claude"},{"family":"Louzada","given":"Maria Lc"},{"family":"Rauber","given":"Fernanda"},{"family":"Khandpur","given":"Neha"},{"family":"Cediel","given":"Gustavo"},{"family":"Neri","given":"Daniela"},{"family":"Martinez-Steele","given":"Euridice"},{"family":"Baraldi","given":"Larissa G."},{"family":"Jaime","given":"Patricia C."}],"citation-key":"monteiroUltraprocessedFoodsWhat2019","container-title":"Public Health Nutrition","container-title-short":"Public Health Nutr","DOI":"10.1017/S1368980018003762","ISSN":"1475-2727","issue":"5","issued":{"date-parts":[["2019",4]]},"language":"eng","page":"936-941","PMCID":"PMC10260459","PMID":"30744710","source":"PubMed","title":"Ultra-processed foods: what they are and how to identify them","title-short":"Ultra-processed foods","type":"article-journal","volume":"22"},{"id":"moorthyApplyingImageProcessing2015","abstract":"Lack of availability of goods and/or the improper positioning of products on the shelves of a retail store can result in loss of sales to a retailer. Visual audits are undertaken by the retailer's staff and the staff of the FMCG product companies, (whose products are stocked in the retail shelves), to discover out-of-stock and misplaced products in a retailer's shelf. In this paper, a method of automating the process of manual inspection has been described. The paper also demonstrates that by applying image processing techniques (available in MATLAB 2013a), it is possible to identify and count the front-facing products, as well as detect void spaces on the shelf. Images from a video stream (such as from a security camera) can also be analyzed to count the number of facings of a specific product on a shelf and identify if they are placed face-up, as should be the case. The image processing approach proposed in the paper will primarily enable proper positioning of products on the shelf the front row. While that may seem as a limitation for inventory counting, it is actually an important parameter for product manufacturers who usually rent shelf space and positions at a premium and mandate the retailers to place specific products at specific shelves. The incremental change that the paper proposes is to extend the use of feature extraction in image processing to highlight incorrect placement and positioning of items on the shelves. The implemented solution does not require significant additional infrastructure costs.","accessed":{"date-parts":[["2023",1,13]]},"author":[{"family":"Moorthy","given":"Rahul"},{"family":"Behera","given":"Swikriti"},{"family":"Verma","given":"Saurav"},{"family":"Bhargave","given":"Shreyas"},{"family":"Ramanathan","given":"Prasad"}],"citation-key":"moorthyApplyingImageProcessing2015","collection-title":"WCI '15","container-title":"Proceedings of the Third International Symposium on Women in Computing and Informatics","DOI":"10.1145/2791405.2791533","event-place":"New York, NY, USA","ISBN":"978-1-4503-3361-0","issued":{"date-parts":[["2015",8]]},"page":"451–457","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","title":"Applying Image Processing for Detecting On-Shelf Availability and Product Positioning in Retail Stores","type":"paper-conference","URL":"https://doi.org/10.1145/2791405.2791533"},{"id":"moorthyApplyingImageProcessing2015a","abstract":"Lack of availability of goods and/or the improper positioning of products on the shelves of a retail store can result in loss of sales to a retailer. Visual audits are undertaken by the retailer's staff and the staff of the FMCG product companies, (whose products are stocked in the retail shelves), to discover out-of-stock and misplaced products in a retailer's shelf. In this paper, a method of automating the process of manual inspection has been described. The paper also demonstrates that by applying image processing techniques (available in MATLAB 2013a), it is possible to identify and count the front-facing products, as well as detect void spaces on the shelf. Images from a video stream (such as from a security camera) can also be analyzed to count the number of facings of a specific product on a shelf and identify if they are placed face-up, as should be the case. The image processing approach proposed in the paper will primarily enable proper positioning of products on the shelf the front row. While that may seem as a limitation for inventory counting, it is actually an important parameter for product manufacturers who usually rent shelf space and positions at a premium and mandate the retailers to place specific products at specific shelves. The incremental change that the paper proposes is to extend the use of feature extraction in image processing to highlight incorrect placement and positioning of items on the shelves. The implemented solution does not require significant additional infrastructure costs.","accessed":{"date-parts":[["2023",1,13]]},"author":[{"family":"Moorthy","given":"Rahul"},{"family":"Behera","given":"Swikriti"},{"family":"Verma","given":"Saurav"},{"family":"Bhargave","given":"Shreyas"},{"family":"Ramanathan","given":"Prasad"}],"citation-key":"moorthyApplyingImageProcessing2015a","collection-title":"WCI '15","container-title":"Proceedings of the Third International Symposium on Women in Computing and Informatics","DOI":"10.1145/2791405.2791533","event-place":"New York, NY, USA","ISBN":"978-1-4503-3361-0","issued":{"date-parts":[["2015",8,10]]},"page":"451–457","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Applying Image Processing for Detecting On-Shelf Availability and Product Positioning in Retail Stores","type":"paper-conference","URL":"https://doi.org/10.1145/2791405.2791533"},{"id":"morganoskyConsumerResponseOnline2000","abstract":"Reports a preliminary assessment of consumer response to and demand for online food retail channels. Data were collected from 243 US consumers who currently buy their groceries online. The majority of online users were younger than 55 years of age, female, and reported annual incomes of $70,000 or more. Over 70 percent reported convenience and saving time as their primary reasons for buying groceries online but 15 percent cited physical or constraint issues that made it difficult for them to shop at grocery stores. Of the respondents, 19 percent bought all of their groceries online. Also reports demographic and online shopping variables that are significantly related to the primary reason for shopping online, willingness to buy all grocery items online, perception of time spent shopping online vs in the store, and experience with online grocery shopping.","accessed":{"date-parts":[["2024",2,15]]},"author":[{"family":"Morganosky","given":"Michelle A."},{"family":"Cude","given":"Brenda J."}],"citation-key":"morganoskyConsumerResponseOnline2000","container-title":"International Journal of Retail & Distribution Management","DOI":"10.1108/09590550010306737","ISSN":"0959-0552","issue":"1","issued":{"date-parts":[["2000",2,1]]},"language":"en","page":"17-26","source":"DOI.org (Crossref)","title":"Consumer response to online grocery shopping","type":"article-journal","URL":"https://www.emerald.com/insight/content/doi/10.1108/09590550010306737/full/html","volume":"28"},{"id":"morganoskyConsumerResponseOnline2000a","abstract":"Reports a preliminary assessment of consumer response to and demand for online food retail channels. Data were collected from 243 US consumers who currently buy their groceries online. The majority of online users were younger than 55 years of age, female, and reported annual incomes of $70,000 or more. Over 70 percent reported convenience and saving time as their primary reasons for buying groceries online but 15 percent cited physical or constraint issues that made it difficult for them to shop at grocery stores. Of the respondents, 19 percent bought all of their groceries online. Also reports demographic and online shopping variables that are significantly related to the primary reason for shopping online, willingness to buy all grocery items online, perception of time spent shopping online vs in the store, and experience with online grocery shopping.","accessed":{"date-parts":[["2024",2,15]]},"author":[{"family":"Morganosky","given":"Michelle A."},{"family":"Cude","given":"Brenda J."}],"citation-key":"morganoskyConsumerResponseOnline2000a","container-title":"International Journal of Retail & Distribution Management","DOI":"10.1108/09590550010306737","ISSN":"0959-0552","issue":"1","issued":{"date-parts":[["2000",2,1]]},"language":"en","page":"17-26","source":"DOI.org (Crossref)","title":"Consumer response to online grocery shopping","type":"article-journal","URL":"https://www.emerald.com/insight/content/doi/10.1108/09590550010306737/full/html","volume":"28"},{"id":"naumannEcolabelBlueAngel2021","abstract":"Energy consumption induced through information technology, i.e. hardware and software, is constantly increasing. In this article, we present the “Blue Angel”, a label that evaluates and classifies the resource and energy efficiency of software. In particular, the process by which the label was developed is presented. We also describe the components of the Blue Angel: Energy and resource efficiency of the software product, hardware useful life and user autonomy. Finally, we give an outlook on the possibilities for expansion, since the first version of the label focuses especially on desktop software, mainly because of the complex measurement issues. According to evaluations from expert workshops and interviews, this is the best starting point for reliable results and certifications.","author":[{"family":"Naumann","given":"Stefan"},{"family":"Guldner","given":"Achim"},{"family":"Kern","given":"Eva"}],"citation-key":"naumannEcolabelBlueAngel2021","collection-title":"Progress in IS","container-title":"Advances and New Trends in Environmental Informatics","DOI":"10.1007/978-3-030-61969-5_6","editor":[{"family":"Kamilaris","given":"Andreas"},{"family":"Wohlgemuth","given":"Volker"},{"family":"Karatzas","given":"Kostas"},{"family":"Athanasiadis","given":"Ioannis N."}],"event-place":"Cham","ISBN":"978-3-030-61969-5","issued":{"date-parts":[["2021"]]},"language":"en","page":"79-89","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"The Eco-label Blue Angel for Software—Development and Components","type":"paper-conference"},{"id":"naumannEcolabelBlueAngel2021a","abstract":"Energy consumption induced through information technology, i.e. hardware and software, is constantly increasing. In this article, we present the “Blue Angel”, a label that evaluates and classifies the resource and energy efficiency of software. In particular, the process by which the label was developed is presented. We also describe the components of the Blue Angel: Energy and resource efficiency of the software product, hardware useful life and user autonomy. Finally, we give an outlook on the possibilities for expansion, since the first version of the label focuses especially on desktop software, mainly because of the complex measurement issues. According to evaluations from expert workshops and interviews, this is the best starting point for reliable results and certifications.","author":[{"family":"Naumann","given":"Stefan"},{"family":"Guldner","given":"Achim"},{"family":"Kern","given":"Eva"}],"citation-key":"naumannEcolabelBlueAngel2021a","container-title":"Advances and New Trends in Environmental Informatics","editor":[{"family":"Kamilaris","given":"Andreas"},{"family":"Wohlgemuth","given":"Volker"},{"family":"Karatzas","given":"Kostas"},{"family":"Athanasiadis","given":"Ioannis N."}],"event-place":"Cham","ISBN":"978-3-030-61969-5","issued":{"date-parts":[["2021"]]},"page":"79–89","publisher":"Springer International Publishing","publisher-place":"Cham","title":"The Eco-label Blue Angel for Software—Development and Components","type":"paper-conference"},{"id":"naumannGREENSOFTModelReference2011","abstract":"The resource and power consumption of ICT is still increasing, but also the benefits of ICT, e.g. in finding more efficient solutions for environmental problems. To date, it is not clear, whether the resource and energy savings through ICT overbalance the resource and energy consumption by ICT, or not. Up to now, manifold efforts of Green IT address the environmental aspects of sustainability considering computer hardware. However, there is still a lack of models, descriptions or realizations in the area of computer software and software process models. In our contribution, we first propose definitions of the terms “Green and Sustainable Software” and “Green and Sustainable Software Engineering”, then we outline a conceptual reference model, the GREENSOFT Model. This model includes a cradle-to-grave product life cycle model for software products, sustainability metrics and criteria for software, software engineering extensions for sustainably sound software design and development, as well as appropriate guidance.","accessed":{"date-parts":[["2023",8,1]]},"author":[{"family":"Naumann","given":"Stefan"},{"family":"Dick","given":"Markus"},{"family":"Kern","given":"Eva"},{"family":"Johann","given":"Timo"}],"citation-key":"naumannGREENSOFTModelReference2011","container-title":"Sustainable Computing: Informatics and Systems","DOI":"10.1016/j.suscom.2011.06.004","ISSN":"2210-5379","issue":"1","issued":{"date-parts":[["2011"]]},"language":"English","page":"294-304","source":"www.infona.pl","title":"The GREENSOFT Model: A reference model for green and sustainable software and its engineering","title-short":"The GREENSOFT Model","type":"article-journal","URL":"https://www.infona.pl//resource/bwmeta1.element.elsevier-3b37d000-5ab1-3a2c-9a4a-984eea87349f","volume":"4"},{"id":"neo4jBoltDriverPython","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Neo4j","given":""}],"citation-key":"neo4jBoltDriverPython","genre":"Python","license":"OSI Approved :: Apache Software License","medium":"OS Independent","source":"PyPI","title":"Bolt driver for Python","title-short":"neo4j","type":"software","URL":"https://neo4j.com/","version":"5.22.0"},{"id":"neo4jdocuNeo4jDocumentationNeo4j2024","abstract":"Neo4j documentation - Neo4j Documentation","accessed":{"date-parts":[["2024",6,25]]},"author":[{"family":"neo4jdocu","given":""}],"citation-key":"neo4jdocuNeo4jDocumentationNeo4j2024","container-title":"Neo4j Graph Data Platform","issued":{"date-parts":[["2024"]]},"language":"en","title":"Neo4j documentation - Neo4j Documentation","type":"webpage","URL":"https://neo4j.com/docs/docs/"},{"id":"neo4jDownloadNeo4jDesktop","abstract":"Experience Neo4j 5 on your desktop. Get started with the free graph database download today and avoid the costs of self-hosted deployment.","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Neo4j","given":""}],"citation-key":"neo4jDownloadNeo4jDesktop","container-title":"Graph Database & Analytics","language":"en","title":"Download Neo4j Desktop","type":"webpage","URL":"https://neo4j.com/download/"},{"id":"neo4jNeo4jGraphDatabase2024","abstract":"Connect data as it's stored with Neo4j. Perform powerful, complex queries at scale and speed with our graph data platform.","accessed":{"date-parts":[["2024",6,26]]},"author":[{"family":"Neo4j","given":""}],"citation-key":"neo4jNeo4jGraphDatabase2024","container-title":"Graph Database & Analytics","issued":{"date-parts":[["2024"]]},"language":"en","title":"Neo4j Graph Database & Analytics – The Leader in Graph Databases","type":"webpage","URL":"https://neo4j.com/"},{"id":"neosemanticsNeosemanticsN10sNeo4j2024","abstract":"neosemantics (n10s): Neo4j RDF & Semantics toolkit - Neo4j Labs","accessed":{"date-parts":[["2024",6,26]]},"author":[{"family":"neosemantics","given":""}],"citation-key":"neosemanticsNeosemanticsN10sNeo4j2024","container-title":"Neo4j Graph Data Platform","issued":{"date-parts":[["2024"]]},"language":"en","title":"neosemantics (n10s): Neo4j RDF & Semantics toolkit - Neo4j Labs","title-short":"neosemantics (n10s)","type":"webpage","URL":"https://neo4j.com/labs/neosemantics/"},{"id":"neosemanticsNeosemanticsReferenceNeosemantics2024","abstract":"Neosemantics Reference - Neosemantics","accessed":{"date-parts":[["2024",7,16]]},"author":[{"family":"Neosemantics","given":""}],"citation-key":"neosemanticsNeosemanticsReferenceNeosemantics2024","container-title":"Neo4j Graph Data Platform","issued":{"date-parts":[["2024"]]},"language":"en","title":"Neosemantics Reference - Neosemantics","type":"webpage","URL":"https://neo4j.com/labs//neosemantics/5.14/reference/"},{"id":"nickelReviewRelationalMachine2016","abstract":"Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be \"trained\" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination.","accessed":{"date-parts":[["2024",7,12]]},"author":[{"family":"Nickel","given":"Maximilian"},{"family":"Murphy","given":"Kevin"},{"family":"Tresp","given":"Volker"},{"family":"Gabrilovich","given":"Evgeniy"}],"citation-key":"nickelReviewRelationalMachine2016","container-title":"Proceedings of the IEEE","container-title-short":"Proc. IEEE","DOI":"10.1109/JPROC.2015.2483592","ISSN":"0018-9219, 1558-2256","issue":"1","issued":{"date-parts":[["2016",1]]},"page":"11-33","source":"arXiv.org","title":"A Review of Relational Machine Learning for Knowledge Graphs","type":"article-journal","URL":"http://arxiv.org/abs/1503.00759","volume":"104"},{"id":"NonFoodKGNonfoodProduct2022","abstract":"Non-food product knowledge graph","accessed":{"date-parts":[["2023",3,27]]},"citation-key":"NonFoodKGNonfoodProduct2022","issued":{"date-parts":[["2022",7]]},"license":"BSD-3-Clause","publisher":"K4R-IAI","title":"NonFoodKG - A non-food product knowledge graph","type":"document","URL":"https://github.com/K4R-IAI/NonFoodKG"},{"id":"noyIndustryscaleKnowledgeGraphs2019","abstract":"This article looks at the knowledge graphs of five diverse tech companies, comparing the similarities and differences in their respective experiences of building and using the graphs, and discussing the challenges that all knowledge-driven enterprises face today. The collection of knowledge graphs discussed here covers the breadth of applications, from search, to product descriptions, to social networks.","accessed":{"date-parts":[["2024",2,4]]},"author":[{"family":"Noy","given":"Natasha"},{"family":"Gao","given":"Yuqing"},{"family":"Jain","given":"Anshu"},{"family":"Narayanan","given":"Anant"},{"family":"Patterson","given":"Alan"},{"family":"Taylor","given":"Jamie"}],"citation-key":"noyIndustryscaleKnowledgeGraphs2019","container-title":"Queue","container-title-short":"Queue","DOI":"10.1145/3329781.3332266","ISSN":"1542-7730","issue":"2","issued":{"date-parts":[["2019",4,1]]},"page":"Pages 20:48–Pages 20:75","source":"ACM Digital Library","title":"Industry-scale Knowledge Graphs: Lessons and Challenges: Five diverse technology companies show how it’s done","title-short":"Industry-scale Knowledge Graphs","type":"article-journal","URL":"https://dl.acm.org/doi/10.1145/3329781.3332266","volume":"17"},{"id":"noyIndustryscaleKnowledgeGraphs2019a","abstract":"Five diverse technology companies show how it's done.","accessed":{"date-parts":[["2024",7,16]]},"author":[{"family":"Noy","given":"Natasha"},{"family":"Gao","given":"Yuqing"},{"family":"Jain","given":"Anshu"},{"family":"Narayanan","given":"Anant"},{"family":"Patterson","given":"Alan"},{"family":"Taylor","given":"Jamie"}],"citation-key":"noyIndustryscaleKnowledgeGraphs2019a","container-title":"Commun. ACM","DOI":"10.1145/3331166","ISSN":"0001-0782","issue":"8","issued":{"date-parts":[["2019",7,24]]},"page":"36–43","source":"ACM Digital Library","title":"Industry-scale knowledge graphs: lessons and challenges","title-short":"Industry-scale knowledge graphs","type":"article-journal","URL":"https://doi.org/10.1145/3331166","volume":"62"},{"id":"OkfnbrasilSchemaOrgWikidataMapOntology","accessed":{"date-parts":[["2024",2,12]]},"citation-key":"OkfnbrasilSchemaOrgWikidataMapOntology","title":"okfn-brasil/schemaOrg-Wikidata-Map: Ontology alignment between Schema.Org, Wikidata, and DBpedia","type":"webpage","URL":"https://github.com/okfn-brasil/schemaOrg-Wikidata-Map"},{"id":"olivesatgatechImageQualityAssessment2020","abstract":"A Fundamental course in digital image processing for senior-level and graduate-level students. \n\nThis lecture is about Image Quality Assessment (IQA)\n\nOutline: \n- Introduction\n- Subjective versus Objective Quality Assessment\n- Objective Quality Assessment\n       o Classes\n        o MSE and PSNR \n         o SSIM\n               - Background \n               - Calculations\n- Framework for Error Sensitivity Measures \n- Examples of HVS-based measures \n         o PSNR-HVS\n         o PSNR-HVS-M\n         o FSIM\n\nObjectives: \n- Learn the Fundamentals of Image Quality Assessment \n- Be familiar with the concept of quality assessment \n- Learn the components of an effective objective quality measure\n- Be familiar with the various methods to run subjective tests \n- Learn the various methodologies to validate quality measures \n\nAffiliation: \nOLIVES at the Georgia Institute of Technology.\n[Group website]: https://ghassanalregib.info/ \n\n#georgiatech #imageprocessing #IQA #Image_Quality #image_quality_assessment #quality_measure #MSE #PSNR #SSIM","accessed":{"date-parts":[["2022",5,27]]},"citation-key":"olivesatgatechImageQualityAssessment2020","dimensions":"1:10:31","director":[{"literal":"OLIVES at GATECH"}],"issued":{"date-parts":[["2020",2,10]]},"source":"YouTube","title":"Image Quality Assessment","type":"motion_picture","URL":"https://www.youtube.com/watch?v=IoRJP9LDAHk"},{"id":"OntologyInformationScience2024","abstract":"In information science, an ontology encompasses a representation, formal naming, and definitions of the categories, properties, and relations between the concepts, data, or entities that pertain to one, many, or all domains of discourse. More simply, an ontology is a way of showing the properties of a subject area and how they are related, by defining a set of terms and relational expressions that represent the entities in that subject area. The field which studies ontologies so conceived is sometimes referred to as applied ontology.\nEvery academic discipline or field, in creating its terminology, thereby lays the groundwork for an ontology. Each uses ontological assumptions to frame explicit theories, research and applications. Improved ontologies may improve problem solving within that domain, interoperability of data systems, and discoverability of data. Translating research papers within every field is a problem made easier when experts from different countries maintain a controlled vocabulary of jargon between each of their languages. For instance, the definition and ontology of economics is a primary concern in Marxist economics, but also in other subfields of economics. An example of economics relying on information science occurs in cases where a simulation or model is intended to enable economic decisions, such as determining what capital assets are at risk and by how much (see risk management).\nWhat ontologies in both information science and philosophy have in common is the attempt to represent entities, including both objects and events, with all their interdependent properties and relations, according to a system of categories. In both fields, there is considerable work on problems of ontology engineering (e.g., Quine and Kripke in philosophy, Sowa and Guarino in information science), and debates concerning to what extent normative ontology is possible (e.g., foundationalism and coherentism in philosophy, BFO and Cyc in artificial intelligence).\nApplied ontology is considered by some as a successor to prior work in philosophy. However many current efforts are more concerned with establishing controlled vocabularies of narrow domains than with philosophical first principles, or with questions such as the mode of existence of fixed essences or whether enduring objects (e.g., perdurantism and endurantism) may be ontologically more primary than processes. Artificial intelligence has retained considerable attention regarding applied ontology in subfields like natural language processing within machine translation and knowledge representation, but ontology editors are being used often in a range of fields, including biomedical informatics, industry. Such efforts often use ontology editing tools such as Protégé.\n\nOntology is a branch of philosophy and intersects areas such as metaphysics, epistemology, and philosophy of language, as it considers how knowledge, language, and perception relate to the nature of reality. Metaphysics deals with questions like \"what exists?\" and \"what is the nature of reality?\". One of five traditional branches of philosophy, metaphysics is concerned with exploring existence through properties, entities and relations such as those between particulars and universals, intrinsic and extrinsic properties, or essence and existence. Metaphysics has been an ongoing topic of discussion since recorded history.\n\nThe compound word ontology combines onto-, from the Greek ὄν, on (gen. ὄντος, ontos), i.e. \"being; that which is\", which is the present participle of the verb εἰμί, eimí, i.e. \"to be, I am\", and -λογία, -logia, i.e. \"logical discourse\", see classical compounds for this type of word formation.\nWhile the etymology is Greek, the oldest extant record of the word itself, the Neo-Latin form ontologia, appeared in 1606 in the work Ogdoas Scholastica by Jacob Lorhard (Lorhardus) and in 1613 in the Lexicon philosophicum by Rudolf Göckel (Goclenius).\nThe first occurrence in English of ontology as recorded by the OED (Oxford English Dictionary, online edition, 2008) came in Archeologia Philosophica Nova or New Principles of Philosophy by Gideon Harvey.\n\nSince the mid-1970s, researchers in the field of artificial intelligence (AI) have recognized that knowledge engineering is the key to building large and powerful AI systems. AI researchers argued that they could create new ontologies as computational models that enable certain kinds of automated reasoning, which was only marginally successful. In the 1980s, the AI community began to use the term ontology to refer to both a theory of a modeled world and a component of knowledge-based systems. In particular, David Powers introduced the word ontology to AI to refer to real world or robotic grounding, publishing in 1990 literature reviews emphasizing grounded ontology in association with the call for papers for a AAAI Summer Symposium Machine Learning of Natural Language and Ontology, with an expanded version published in SIGART Bulletin and included as a preface to the proceedings. Some researchers, drawing inspiration from philosophical ontologies, viewed computational ontology as a kind of applied philosophy.\n\nIn 1993, the widely cited web page and paper \"Toward Principles for the Design of Ontologies Used for Knowledge Sharing\" by Tom Gruber used ontology as a technical term in computer science closely related to earlier idea of semantic networks and taxonomies. Gruber introduced the term as a specification of a conceptualization: An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents. This definition is consistent with the usage of ontology as set of concept definitions, but more general. And it is a different sense of the word than its use in philosophy.\nAttempting to distance ontologies from taxonomies and similar efforts in knowledge modeling that rely on classes and inheritance, Gruber stated (1993): Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but ontologies need not be limited to these forms. Ontologies are also not limited to conservative definitions — that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world. To specify a conceptualization, one needs to state axioms that do constrain the possible interpretations for the defined terms.\nAs refinement of Gruber's definition Feilmayr and Wöß (2016) stated: \"An ontology is a formal, explicit specification of a shared conceptualization that is characterized by high semantic expressiveness required for increased complexity.\"\n\nContemporary ontologies share many structural similarities, regardless of the language in which they are expressed. Most ontologies describe individuals (instances), classes (concepts), attributes and relations.\n\nA domain ontology (or domain-specific ontology) represents concepts which belong to a realm of the world, such as biology or politics. Each domain ontology typically models domain-specific definitions of terms. For example, the word card has many different meanings. An ontology about the domain of poker would model the \"playing card\" meaning of the word, while an ontology about the domain of computer hardware would model the \"punched card\" and \"video card\" meanings.\nSince domain ontologies are written by different people, they represent concepts in very specific and unique ways, and are often incompatible within the same project. As systems that rely on domain ontologies expand, they often need to merge domain ontologies by hand-tuning each entity or using a combination of software merging and hand-tuning. This presents a challenge to the ontology designer. Different ontologies in the same domain arise due to different languages, different intended usage of the ontologies, and different perceptions of the domain (based on cultural background, education, ideology, etc.).\nAt present, merging ontologies that are not developed from a common upper ontology is a largely manual process and therefore time-consuming and expensive. Domain ontologies that use the same upper ontology to provide a set of basic elements with which to specify the meanings of the domain ontology entities can be merged with less effort. There are studies on generalized techniques for merging ontologies, but this area of research is still ongoing, and it is a recent event to see the issue sidestepped by having multiple domain ontologies using the same upper ontology like the OBO Foundry.\n\nAn upper ontology (or foundation ontology) is a model of the commonly shared relations and objects that are generally applicable across a wide range of domain ontologies. It usually employs a core glossary that overarches the terms and associated object descriptions as they are used in various relevant domain ontologies.\nStandardized upper ontologies available for use include BFO, BORO method, Dublin Core, GFO, Cyc, SUMO, UMBEL, and DOLCE. WordNet has been considered an upper ontology by some and has been used as a linguistic tool for learning domain ontologies.\n\nThe Gellish ontology is an example of a combination of an upper and a domain ontology.\n\nA survey of ontology visualization methods is presented by Katifori et al. An updated survey of ontology visualization methods and tools was published by Dudás et al. The most established ontology visualization methods, namely indented tree and graph visualization are evaluated by Fu et al. A visual language for ontologies represented in OWL is specified by the Visual Notation for OWL Ontologies (VOWL).\n\nOntology engineering (also called ontology building)  is a set of tasks related to the development of ontologies for a particular domain. It is a subfield of knowledge engineering that studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tools and languages that support them.\nOntology engineering aims to make explicit the knowledge contained in software applications, and organizational procedures for a particular domain. Ontology engineering offers a direction for overcoming semantic obstacles, such as those related to the definitions of business terms and software classes. Known challenges with ontology engineering include:\n\nEnsuring the ontology is current with domain knowledge and term use\nProviding sufficient specificity and concept coverage for the domain of interest, thus minimizing the content completeness problem\nEnsuring the ontology can support its use cases\n\nOntology editors are applications designed to assist in the creation or manipulation of ontologies. It is common for ontology editors to use one or more ontology languages.\nAspects of ontology editors include: visual navigation possibilities within the knowledge model, inference engines and information extraction; support for modules; the import and export of foreign knowledge representation languages for ontology matching; and the support of meta-ontologies such as OWL-S, Dublin Core, etc.\n\nOntology learning is the automatic or semi-automatic creation of ontologies, including extracting a domain's terms from natural language text. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.  Information extraction and text mining have been explored to automatically link ontologies to documents, for example in the context of the BioCreative challenges.\n\nEpistemological assumptions, which in research asks \"What do you know? or \"How do you know it?\", creates the foundation researchers use when approaching a certain topic or area for potential research. As epistemology is directly linked to knowledge and how we come about accepting certain truths, individuals conducting academic research must understand what allows them to begin theory building. Simply, epistemological assumptions force researchers to question how they arrive at the knowledge they have.\n\nAn ontology language is a formal language used to encode an ontology. There are a number of such languages for ontologies, both proprietary and standards-based:\n\nCommon Algebraic Specification Language is a general logic-based specification language developed within the IFIP working group 1.3 \"Foundations of System Specifications\" and is a de facto standard language for software specifications. It is now being applied to ontology specifications in order to provide modularity and structuring mechanisms.\nCommon logic is ISO standard 24707, a specification of a family of ontology languages that can be accurately translated into each other.\nThe Cyc project has its own ontology language called CycL, based on first-order predicate calculus with some higher-order extensions.\nDOGMA (Developing Ontology-Grounded Methods and Applications) adopts the fact-oriented modeling approach to provide a higher level of semantic stability.\nThe Gellish language includes rules for its own extension and thus integrates an ontology with an ontology language.\nIDEF5 is a software engineering method to develop and maintain usable, accurate, domain ontologies.\nKIF is a syntax for first-order logic that is based on S-expressions.  SUO-KIF is a derivative version supporting the Suggested Upper Merged Ontology.\nMOF and UML are standards of the OMG\nOlog is a category theoretic approach to ontologies, emphasizing translations between ontologies using functors.\nOBO, a language used for biological and biomedical ontologies.\nOntoUML is an ontologically well-founded profile of UML for conceptual modeling of domain ontologies.\nOWL is a language for making ontological statements, developed as a follow-on from RDF and RDFS, as well as earlier ontology language projects including OIL, DAML, and DAML+OIL. OWL is intended to be used over the World Wide Web, and all its elements (classes, properties and individuals) are defined as RDF resources, and identified by URIs.\nRule Interchange Format (RIF) and F-Logic combine ontologies and rules.\nSemantic Application Design Language (SADL) captures a subset of the expressiveness of OWL, using an English-like language entered via an Eclipse Plug-in.\nSBVR (Semantics of Business Vocabularies and Rules) is an OMG standard adopted in industry to build ontologies.\nTOVE Project, TOronto Virtual Enterprise project\n\nArabic Ontology, a linguistic ontology for Arabic, which can be used as an Arabic Wordnet but with ontologically-clean content.\nAURUM - Information Security Ontology, An ontology for information security knowledge sharing, enabling users to collaboratively understand and extend the domain knowledge body. It may serve as a basis for automated information security risk and compliance management.\nBabelNet, a very large multilingual semantic network and ontology, lexicalized in many languages\nBasic Formal Ontology, a formal upper ontology designed to support scientific research\nBioPAX, an ontology for the exchange and interoperability of biological pathway (cellular processes) data\nBMO, an e-Business Model Ontology based on a review of enterprise ontologies and business model literature\nSSBMO, a Strongly Sustainable Business Model Ontology based on a review of the systems based natural and social science literature (including business).  Includes critique of and significant extensions to the Business Model Ontology (BMO).\nCCO and GexKB, Application Ontologies (APO) that integrate diverse types of knowledge with the Cell Cycle Ontology (CCO) and the Gene Expression Knowledge Base (GexKB)\nCContology (Customer Complaint Ontology), an e-business ontology to support online customer complaint management\nCIDOC Conceptual Reference Model, an ontology for cultural heritage\nCOSMO, a Foundation Ontology (current version in OWL) that is designed to contain representations of all of the primitive concepts needed to logically specify the meanings of any domain entity.  It is intended to serve as a basic ontology that can be used to translate among the representations in other ontologies or databases.  It started as a merger of the basic elements of the OpenCyc and SUMO ontologies, and has been supplemented with other ontology elements (types, relations) so as to include representations of all of the words in the Longman dictionary defining vocabulary.\nComputer Science Ontology, an automatically generated ontology of research topics in the field of computer science\nCyc, a large Foundation Ontology for formal representation of the universe of discourse\nDisease Ontology, designed to facilitate the mapping of diseases and associated conditions to particular medical codes\nDOLCE, a Descriptive Ontology for Linguistic and Cognitive Engineering\nDrammar, ontology of drama\nDublin Core, a simple ontology for documents and publishing\nFinancial Industry Business Ontology (FIBO),  a business conceptual ontology for the financial industry\nFoundational, Core and Linguistic Ontologies\nFoundational Model of Anatomy, an ontology for human anatomy\nFriend of a Friend, an ontology for describing persons, their activities and their relations to other people and objects\nGene Ontology for genomics\nGellish English dictionary, an ontology that includes a dictionary and taxonomy that includes an upper ontology and a lower ontology that focusses on industrial and business applications in engineering, technology and procurement.\nGeopolitical ontology, an ontology describing geopolitical information created by Food and Agriculture Organization(FAO). The geopolitical ontology includes names in multiple languages (English, French, Spanish, Arabic, Chinese, Russian and Italian); maps standard coding systems (UN, ISO, FAOSTAT, AGROVOC, etc.); provides relations among territories (land borders, group membership, etc.); and tracks historical changes. In addition, FAO provides web services of geopolitical ontology and a module maker to download modules of the geopolitical ontology into different formats (RDF, XML, and EXCEL). See more information at FAO Country Profiles.\nGAO (General Automotive Ontology) - an ontology for the automotive industry that includes 'car' extensions\nGOLD, General Ontology for Linguistic Description\nGUM (Generalized Upper Model), a linguistically motivated ontology for mediating between clients systems and natural language technology\nIDEAS Group, a formal ontology for enterprise architecture being developed by the Australian, Canadian, UK and U.S. Defence Depts.\nLinkbase, a formal representation of the biomedical domain, founded upon Basic Formal Ontology.\nLPL, Landmark Pattern Language\nNCBO Bioportal, biological and biomedical ontologies and associated tools to search, browse and visualise\nNIFSTD Ontologies from the Neuroscience Information Framework: a modular set of ontologies for the neuroscience domain.\nOBO-Edit, an ontology browser for most of the Open Biological and Biomedical Ontologies\nOBO Foundry, a suite of interoperable reference ontologies in biology and biomedicine\nOMNIBUS Ontology, an ontology of learning, instruction, and instructional design\nOntology for Biomedical Investigations, an open-access, integrated ontology of biological and clinical investigations\nONSTR, Ontology for Newborn Screening Follow-up and Translational Research, Newborn Screening Follow-up Data Integration Collaborative, Emory University, Atlanta.\nPlant Ontology for plant structures and growth/development stages, etc.\nPOPE, Purdue Ontology for Pharmaceutical Engineering\nPRO, the Protein Ontology of the Protein Information Resource, Georgetown University\nProbOnto, knowledge base and ontology of probability distributions.\nProgram abstraction taxonomy\nProtein Ontology for proteomics\nRXNO Ontology, for name reactions in chemistry\nSCDO, the Sickle Cell Disease Ontology, facilitates data sharing and collaborations within the SDC community, amongst other applications (see list on SCDO website).\nSequence Ontology, for representing genomic feature types found on biological sequences\nSNOMED CT (Systematized Nomenclature of Medicine—Clinical Terms)\nSuggested Upper Merged Ontology, a formal upper ontology\nSystems Biology Ontology (SBO), for computational models in biology\nSWEET, Semantic Web for Earth and Environmental Terminology\nSSN/SOSA, The Semantic Sensor Network Ontology (SSN) and Sensor, Observation, Sample, and Actuator Ontology (SOSA) are W3C Recommendation  and OGC Standards for describing sensors and their observations.\nThoughtTreasure ontology\nTIME-ITEM, Topics for Indexing Medical Education\nUberon, representing animal anatomical structures\nUMBEL, a lightweight reference structure of 20,000 subject concept classes and their relationships derived from OpenCyc\nWordNet, a lexical reference system\nYAMATO, Yet Another More Advanced Top-level Ontology\nYSO - General Finnish Ontology\nThe W3C Linking Open Data community project coordinates attempts to converge different ontologies into worldwide Semantic Web.\n\nThe development of ontologies has led to the emergence of services providing lists or directories of ontologies called ontology libraries.\nThe following are libraries of human-selected ontologies.\n\nCOLORE is an open repository of first-order ontologies in Common Logic with formal links between ontologies in the repository.\nDAML Ontology Library maintains a legacy of ontologies in DAML.\nOntology Design Patterns portal is a wiki repository of reusable components and practices for ontology design, and also maintains a list of exemplary ontologies.\nProtégé Ontology Library contains a set of OWL, Frame-based and other format ontologies.\nSchemaWeb is a directory of RDF schemata expressed in RDFS, OWL and DAML+OIL.\nThe following are both directories and search engines.\n\nOBO Foundry is a suite of interoperable reference ontologies in biology and biomedicine.\nBioportal (ontology repository of NCBO)\nOntoSelect Ontology Library offers similar services for RDF/S, DAML and OWL ontologies.\nOntaria is a \"searchable and browsable directory of semantic web data\" with a focus on RDF vocabularies with OWL ontologies. (NB Project \"on hold\" since 2004).\nSwoogle is a directory and search engine for all RDF resources available on the Web, including ontologies.\nOpen Ontology Repository initiative\nROMULUS is a foundational ontology repository aimed at improving semantic interoperability. Currently there are three foundational ontologies in the repository: DOLCE, BFO and GFO.\n\nIn general, ontologies can be used beneficially in several fields.\n\nEnterprise applications. A more concrete example is SAPPHIRE (Health care) or Situational Awareness and Preparedness for Public Health Incidences and Reasoning Engines which is a semantics-based health information system capable of tracking and evaluating situations and occurrences that may affect public health.\nGeographic information systems bring together data from different sources and benefit therefore from ontological metadata which helps to connect the semantics of the data.\nDomain-specific ontologies are extremely important in biomedical research, which requires named entity disambiguation of various biomedical terms and abbreviations that have the same string of characters but represent different biomedical concepts. For example, CSF can represent Colony Stimulating Factor or Cerebral Spinal Fluid, both of which are represented by the same term, CSF, in biomedical literature. This is why a large number of public ontologies are related to the life sciences. Life science data science tools that fail to implement these types of biomedical ontologies will not be able to accurately determine causal relationships between concepts.\n\nRelated philosophical concepts\nAlphabet of human thought\nCharacteristica universalis\nInteroperability\nLevel of measurement\nMetalanguage\nNatural semantic metalanguage\n\nOberle, D.; Guarino, N.; Staab, S. (2009). \"What is an Ontology?\" (PDF). Handbook on Ontologies. pp. 1–17. doi:10.1007/978-3-540-92673-3_0. ISBN 978-3-540-70999-2. S2CID 8522608.\nFensel, D.; van Harmelen, F.; Horrocks, I.; McGuinness, D.L.; Patel-Schneider, P.F. (2001). \"OIL: an ontology infrastructure for the Semantic Web\". IEEE Intelligent Systems. 16 (2): 38–45. doi:10.1109/5254.920598.\nGangemi, A.; Presutti, V. \"Ontology Design Patterns\" (PDF). Staab & Studer 2009.\nGolemati, M.; Katifori, A.; Vassilakis, C.; Lepouras, G.; Halatsis, C. (2007). \"Creating an Ontology for the User Profile# Method and Applications\" (PDF). Proceedings of the First IEEE International Conference on Research Challenges in Information Science (RCIS), Morocco 2007. CiteSeerX 10.1.1.74.9399. Archived from the original (PDF) on 2008-12-17.\nMizoguchi, R. (2004). \"Tutorial on ontological engineering: Part 3: Advanced course of ontological engineering\" (PDF). New Gener Comput. 22: 193–220. doi:10.1007/BF03040960. S2CID 23747079. Archived from the original (PDF) on 2013-03-09. Retrieved 2009-06-08.\nGruber, T. R. (1993). \"A translation approach to portable ontology specifications\" (PDF). Knowledge Acquisition. 5 (2): 199–220. CiteSeerX 10.1.1.101.7493. doi:10.1006/knac.1993.1008. S2CID 15709015.\nMaedche, A.; Staab, S. (2001). \"Ontology learning for the Semantic Web\". IEEE Intelligent Systems. 16 (2): 72–79. doi:10.1109/5254.920602. S2CID 1411149.\nNoy, Natalya F.; McGuinness, Deborah L. (March 2001). \"Ontology Development 101: A Guide to Creating Your First Ontology\". Stanford Knowledge Systems Laboratory Technical Report KSL-01-05, Stanford Medical Informatics Technical Report SMI-2001-0880. Archived from the original on 2010-07-14.\nChaminda Abeysiriwardana, Prabath; Kodituwakku, Saluka R (2012). \"Ontology Based Information Extraction for Disease Intelligence\". International Journal of Research in Computer Science. 2 (6): 7–19. arXiv:1211.3497. Bibcode:2012arXiv1211.3497C. doi:10.7815/ijorcs.26.2012.051. S2CID 11297019.\nRazmerita, L.; Angehrn, A.; Maedche, A. (2003). \"Ontology-Based User Modeling for Knowledge Management Systems\". User Modeling 2003. Lecture Notes in Computer Science. Vol. 2702. Springer. pp. 213–7. CiteSeerX 10.1.1.102.4591. doi:10.1007/3-540-44963-9_29. ISBN 3-540-44963-9.\nSoylu, A.; De Causmaecker, Patrick (2009). \"Merging model driven and ontology driven system development approaches pervasive computing perspective\". Proceedings of the 24th International Symposium on Computer and Information Sciences. pp. 730–5. doi:10.1109/ISCIS.2009.5291915. ISBN 978-1-4244-5021-3. S2CID 2267593.\nSmith, B. (2008). \"Ontology (Science)\". In Eschenbach, C.; Gruninger, M. (eds.). Formal Ontology in Information Systems, Proceedings of FOIS 2008. ISO Press. pp. 21–35. CiteSeerX 10.1.1.681.2599.\nStaab, S.; Studer, R., eds. (2009). \"What is an Ontology?\". Handbook on Ontologies (2nd ed.). Springer. pp. 1–17. doi:10.1007/978-3-540-92673-3_0. ISBN 978-3-540-92673-3. S2CID 8522608.\nUschold, Mike; Gruninger, M. (1996). \"Ontologies: Principles, Methods and Applications\". Knowledge Engineering Review. 11 (2): 93–136. CiteSeerX 10.1.1.111.5903. doi:10.1017/S0269888900007797. S2CID 2618234.\nPidcock, W. \"What are the differences between a vocabulary, a taxonomy, a thesaurus, an ontology, and a meta-model?\". Archived from the original on 2009-10-14.\nYudelson, M.; Gavrilova, T.; Brusilovsky, P. (2005). \"Towards User Modeling Meta-ontology\". User Modeling 2005. Lecture Notes in Computer Science. Vol. 3538. Springer. pp. 448–452. CiteSeerX 10.1.1.86.7079. doi:10.1007/11527886_62. ISBN 978-3-540-31878-1.\nMovshovitz-Attias, Dana; Cohen, William W. (2012). \"Bootstrapping Biomedical Ontologies for Scientific Text using NELL\" (PDF). Proceedings of the 2012 Workshop on Biomedical Natural Language Processing. Association for Computational Linguistics. pp. 11–19. CiteSeerX 10.1.1.376.2874.\n\nKnowledge Representation at Open Directory Project\nLibrary of ontologies\nGoPubMed using Ontologies for searching\nONTOLOG (a.k.a. \"Ontolog Forum\") - an Open, International, Virtual Community of Practice on Ontology, Ontological Engineering and Semantic Technology\nUse of Ontologies in Natural Language Processing\nOntology Summit - an annual series of events (first started in 2006) that involves the ontology community and communities related to each year's theme chosen for the summit.\nStandardization of Ontologies","accessed":{"date-parts":[["2024",7,27]]},"citation-key":"OntologyInformationScience2024","container-title":"Wikipedia","issued":{"date-parts":[["2024",7,11]]},"language":"en","license":"Creative Commons Attribution-ShareAlike License","note":"Page Version ID: 1233975174","source":"Wikipedia","title":"Ontology (information science)","type":"entry-encyclopedia","URL":"https://en.wikipedia.org/w/index.php?title=Ontology_(information_science)&oldid=1233975174"},{"id":"opdahlKnowledgeGraphsNaturalLanguage2020","abstract":"Emergency-relevant data comes in many varieties. It can be high volume and high velocity, and reaction times are critical, calling for efficient and powerful techniques for data analysis and management. Knowledge graphs represent data in a rich, flexible, and uniform way that is well matched with the needs of emergency management. They build on existing standards, resources, techniques, and tools for semantic data and computing. This chapter explains the most important semantic technologies and how they support knowledge graphs. We proceed to discuss their benefits and challenges and give examples of relevant semantic data sources and vocabularies. Natural-language texts -- in particular those collected from social media such as Twitter -- is a type of data source that poses particular analysis challenges. We therefore include an overview of techniques for processing natural-language texts.","author":[{"family":"Opdahl","given":"Andreas"}],"citation-key":"opdahlKnowledgeGraphsNaturalLanguage2020","issued":{"date-parts":[["2020",12,15]]},"source":"ResearchGate","title":"Knowledge Graphs and Natural-Language Processing","type":"book"},{"id":"openaiPricing2024","abstract":"Simple and flexible. Only pay for what you use.","accessed":{"date-parts":[["2024",8,1]]},"author":[{"family":"Openai","given":""}],"citation-key":"openaiPricing2024","issued":{"date-parts":[["2024"]]},"language":"en-US","title":"Pricing","type":"webpage","URL":"https://openai.com/api/pricing/"},{"id":"OpenCVIntroductionSIFT","accessed":{"date-parts":[["2023",3,7]]},"citation-key":"OpenCVIntroductionSIFT","title":"OpenCV: Introduction to SIFT (Scale-Invariant Feature Transform)","type":"document","URL":"https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html"},{"id":"OpenCVIntroductionSIFTa","accessed":{"date-parts":[["2023",3,7]]},"citation-key":"OpenCVIntroductionSIFTa","title":"OpenCV: Introduction to SIFT (Scale-Invariant Feature Transform)","type":"webpage","URL":"https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html"},{"id":"opencypherOpenCypherOpenCypher","accessed":{"date-parts":[["2024",6,25]]},"author":[{"family":"opencypher","given":""}],"citation-key":"opencypherOpenCypherOpenCypher","title":"openCypher · openCypher","type":"webpage","URL":"https://opencypher.org/"},{"id":"oxigraphHomepage","abstract":"SPARQL graph database. Contribute to oxigraph/oxigraph development by creating an account on GitHub.","accessed":{"date-parts":[["2024",7,10]]},"author":[{"family":"Oxigraph","given":""}],"citation-key":"oxigraphHomepage","container-title":"GitHub","language":"en","title":"Homepage","type":"webpage","URL":"https://github.com/oxigraph/oxigraph/"},{"id":"oxigraphOxigraphBenchREADME","abstract":"SPARQL graph database. Contribute to oxigraph/oxigraph development by creating an account on GitHub.","accessed":{"date-parts":[["2024",7,10]]},"author":[{"family":"Oxigraph","given":""}],"citation-key":"oxigraphOxigraphBenchREADME","container-title":"GitHub","language":"en","title":"oxigraph/bench/README.md at main · oxigraph/oxigraph","type":"webpage","URL":"https://github.com/oxigraph/oxigraph/blob/main/bench/README.md"},{"id":"palombaImpactCodeSmells2019","abstract":"Context. The demand for green software design is steadily growing higher especially in the context of mobile devices, where the computation is often limited by battery life. Previous studies found how wrong programming solutions have a strong impact on the energy consumption. Objective. Despite the efforts spent so far, only a little knowledge on the influence of code smells, i.e.,symptoms of poor design or implementation choices, on the energy consumption of mobile applications is available. Method. To provide a wider overview on the relationship between smells and energy efficiency, in this paper we conducted a large-scale empirical study on the influence of 9 Android-specific code smells on the energy consumption of 60 Android apps. In particular, we focus our attention on the design flaws that are theoretically supposed to be related to non-functional attributes of source code, such as performance and energy consumption. Results. The results of the study highlight that methods affected by four code smell types, i.e.,Internal Setter, Leaking Thread, Member Ignoring Method, and Slow Loop, consume up to 87 times more than methods affected by other code smells. Moreover, we found that refactoring these code smells reduces energy consumption in all of the situations. Conclusions. Based on our findings, we argue that more research aimed at designing automatic refactoring approaches and tools for mobile apps is needed.","accessed":{"date-parts":[["2023",7,25]]},"author":[{"family":"Palomba","given":"Fabio"},{"family":"Di Nucci","given":"Dario"},{"family":"Panichella","given":"Annibale"},{"family":"Zaidman","given":"Andy"},{"family":"De Lucia","given":"Andrea"}],"citation-key":"palombaImpactCodeSmells2019","container-title":"Information and Software Technology","container-title-short":"Information and Software Technology","DOI":"10.1016/j.infsof.2018.08.004","ISSN":"0950-5849","issued":{"date-parts":[["2019",1,1]]},"language":"en","page":"43-55","source":"ScienceDirect","title":"On the impact of code smells on the energy consumption of mobile applications","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0950584918301678","volume":"105"},{"id":"PapersCodePaper","abstract":"Paper tables with annotated results for Product Knowledge Graph Embedding for E-commerce","accessed":{"date-parts":[["2024",7,9]]},"citation-key":"PapersCodePaper","language":"en","title":"Papers with Code - Paper tables with annotated results for Product Knowledge Graph Embedding for E-commerce","type":"webpage","URL":"https://paperswithcode.com/paper/product-knowledge-graph-embedding-for-e/review/"},{"id":"paralleldotsShelfArrangementKPIs","abstract":"Shelf arrangement and planogram KPIs are often considered the gold standard for retail visibility execution, which is why their compliance is integral to creating a Perfect Store.","accessed":{"date-parts":[["2023",2,15]]},"author":[{"family":"paralleldots","given":""}],"citation-key":"paralleldotsShelfArrangementKPIs","language":"en","title":"Shelf Arrangement KPIs and Planogram Compliance through Image Recognition","type":"webpage","URL":"https://blog.paralleldots.com//blog/shelf-arrangement-kpis-and-planogram-compliance-through-image-recognition"},{"id":"pasinOntospy","abstract":"Python library and command-line interface for inspecting and visualizing RDF models aka ontologies.","accessed":{"date-parts":[["2024",7,22]]},"author":[{"family":"Pasin","given":"Michele"}],"citation-key":"pasinOntospy","container-title":"Ontospy","language":"en-US","title":"Ontospy","type":"webpage","URL":"http://lambdamusic.github.io/Ontospy/"},{"id":"pauwelsKnowledgeGraphsLinked2022","abstract":"The built environment in the Industry 4.0 is digitizing rapidly and producing great amounts of data. These data are of different kinds and follow specific structures and logics, depending on what they are used for and where they come from. This includes vendor-neutral and exchangeable files, such as the Industry Foundation Classes (IFC), that capture building and infrastructure data in neutral and exchangeable files; knowledge graphs and linked data that store building data in web-based graph databases; time-series databases that store sensor data values (big data; data lakes); diverse standard file formats to represent 2D and 3D geometry; and property servers as well as product databases. This chapter will particularly investigate knowledge graphs and linked data. These data technologies promise to represent any data on the web and link them all together. This chapter will explain the most basic parts of the technology, while omitting technical detail. It gives an overview of how these technologies are used in the AEC and smart buildings domains. The chapter ends with a discussion on applicability and limitations of the documented technologies and an outlook into the future.","accessed":{"date-parts":[["2024",2,17]]},"author":[{"family":"Pauwels","given":"Pieter"},{"family":"Costin","given":"Aaron"},{"family":"Rasmussen","given":"Mads Holten"}],"citation-key":"pauwelsKnowledgeGraphsLinked2022","collection-title":"Structural Integrity","container-title":"Industry 4.0 for the Built Environment: Methodologies, Technologies and Skills","DOI":"10.1007/978-3-030-82430-3_7","editor":[{"family":"Bolpagni","given":"Marzia"},{"family":"Gavina","given":"Rui"},{"family":"Ribeiro","given":"Diogo"}],"event-place":"Cham","ISBN":"978-3-030-82430-3","issued":{"date-parts":[["2022"]]},"language":"en","page":"157-183","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"Knowledge Graphs and Linked Data for the Built Environment","type":"chapter","URL":"https://doi.org/10.1007/978-3-030-82430-3_7"},{"id":"PDFSoftwareBased","accessed":{"date-parts":[["2022",10,12]]},"citation-key":"PDFSoftwareBased","title":"(PDF) Software based Estimation of Software induced Energy Dissipation with powerstat","type":"webpage","URL":"https://www.researchgate.net/publication/328861723_Software_based_Estimation_of_Software_induced_Energy_Dissipation_with_powerstat"},{"id":"pedersenImageQualityMetrics2011","abstract":"The aim of our research is to assess image quality of prints without the involvement of human observers. The printing industry is continuously moving forward as new products and technologies are introduced to the market. The need to assess the quality is increased with this growth, for example, to verify that these technology advancements lead to higher quality prints. In this thesis we describe the work carried out to use image quality metrics for the evaluation of printed images. The intended behavior of such metrics is to measure or predict image quality as human observers would perceive it. Following an introduction and background on quality assessment, we introduce the concept of image quality metrics. Existing image quality metrics are classified and a survey of them is given, to show how they are constructed and their differences. Following the survey, a new image quality metric, the Spatial Hue Angle MEtric (SHAME) is proposed, which accounts for two key aspects of image quality, namely region of interest and the human visual system. The evaluation of image quality metrics against the percept is a key aspect for ensuring that the metrics can substitute or assist human observers in the assessment of quality. Therefore, existing evaluation methods are presented and analyzed, revealing the need for a new method to assess the overall performance of image quality metrics. For that reason, a new method to evaluate overall performance is proposed, based on the rank order method. Using existing methods and the new evaluation method, a set of commonly used metrics are evaluated with a set of public databases. These databases contain digital images, with a range of different distortions and quality issues, and have quality ratings from human observers. The knowledge gathered from the evaluation of image quality metrics on the existing databases was then applied to the assessment of printed images. Since the metrics require digital images as input, a framework to digitize printed images is proposed. Using this framework a set of metrics is evaluated against human observers, which shows that none of the existing metrics predict overall image quality adequately. These findings lead us to break overall image quality into parts, more precisely quality attributes. Based on existing quality attributes a manageable set of six color printing quality attributes is proposed. The final set included: sharpness, color, lightness, contrast, artifacts, and physical. Through two experimental validation procedures these attributes are found to be a good foundation for the evaluation of color prints. The image quality metrics are then used to describe each of the proposed quality attributes separately to find the most appropriate metrics to measure the quality of each attribute. Two experiments with human observers were carried out, which acted as the basis for the evaluation and selection of metrics. The results show that for some attributes, such as sharpness, suitable metrics can be found, but additional work is needed to find metrics that correlate well with the percept for all of the attributes. An area that may be improved with the use of image quality metrics is the reduction of quality values to a more manageable number (pooling), usually a single quality value. We have investigated the impact of pooling strategies on the performance of image quality metrics. This investigation shows that the performance is linked to the metric, and that the parameters for the pooling strategies are very important. Even with the effort spent on pooling strategies none of the evaluated metrics performed well for the color quality attribute. This lead to a proposal for a new image quality metric designed for the color attribute, Total Variation of Difference (TVD) metric, which applies a spatial filtering to simulate the human visual system before quality is calculated. A comparison against the state of the art metrics shows an increased performance for the new metric. \nLastly, we gather the work carried out in this thesis to develop a practical tool for the printing industry to assess the quality of prints using image quality metrics, named the Quality Assistant. The Quality Assistant consists of all functions needed to evaluate quality, including: a test image suite, the framework for digitizing the prints, a set of image quality metrics, and visualization tools. Through the work carried out in this thesis we have shown the applicability of image quality metrics for the evaluation of printing workflows.","accessed":{"date-parts":[["2022",5,10]]},"author":[{"family":"Pedersen","given":"Marius"}],"citation-key":"pedersenImageQualityMetrics2011","issued":{"date-parts":[["2011"]]},"publisher":"University of Oslo","title":"Image quality metrics for the evaluation of printing workflows","type":"thesis","URL":"https://core.ac.uk/download/pdf/30839624.pdf"},{"id":"pedersenImageQualityMetrics2011a","abstract":"Image quality metrics have become more and more popular in the image processing community. However, so far, no one has been able to deﬁne an image quality metric well correlated with the percept for overall image quality. One of the causes is that image quality is multi-dimensional and complex. One approach to bridge the gap between perceived and calculated image quality is to reduce the complexity of image quality, by breaking the overall quality into a set of quality attributes. In our research we have presented a set of quality attributes built on existing attributes from the literature. The six proposed quality attributes are: sharpness, color, lightness, artifacts, contrast, and physical. This set keeps the dimensionality to a minimum. An experiment validated the quality attributes as suitable for image quality evaluation.","accessed":{"date-parts":[["2022",5,10]]},"author":[{"family":"Pedersen","given":"Marius"},{"family":"Bonnier","given":"Nicolas"},{"family":"Hardeberg","given":"Jon Y."},{"family":"Albregtsen","given":"Fritz"}],"citation-key":"pedersenImageQualityMetrics2011a","DOI":"10.1117/12.876472","editor":[{"family":"Farnand","given":"Susan P."},{"family":"Gaykema","given":"Frans"}],"event-place":"San Francisco Airport, California, USA","event-title":"IS&T/SPIE Electronic Imaging","issued":{"date-parts":[["2011",1,23]]},"language":"en","page":"786702","publisher-place":"San Francisco Airport, California, USA","source":"DOI.org (Crossref)","title":"Image quality metrics for the evaluation of print quality","type":"paper-conference","URL":"http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.876472"},{"id":"pengGraphRetrievalAugmentedGeneration2024","abstract":"Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable success in addressing the challenges of Large Language Models (LLMs) without necessitating retraining. By referencing an external knowledge base, RAG refines LLM outputs, effectively mitigating issues such as ``hallucination'', lack of domain-specific knowledge, and outdated information. However, the complex structure of relationships among different entities in databases presents challenges for RAG systems. In response, GraphRAG leverages structural information across entities to enable more precise and comprehensive retrieval, capturing relational knowledge and facilitating more accurate, context-aware responses. Given the novelty and potential of GraphRAG, a systematic review of current technologies is imperative. This paper provides the first comprehensive overview of GraphRAG methodologies. We formalize the GraphRAG workflow, encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and training methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. Finally, we explore future research directions to inspire further inquiries and advance progress in the field. In order to track recent progress in this field, we set up a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.","accessed":{"date-parts":[["2024",10,4]]},"author":[{"family":"Peng","given":"Boci"},{"family":"Zhu","given":"Yun"},{"family":"Liu","given":"Yongchao"},{"family":"Bo","given":"Xiaohe"},{"family":"Shi","given":"Haizhou"},{"family":"Hong","given":"Chuntao"},{"family":"Zhang","given":"Yan"},{"family":"Tang","given":"Siliang"}],"citation-key":"pengGraphRetrievalAugmentedGeneration2024","DOI":"10.48550/arXiv.2408.08921","issued":{"date-parts":[["2024",9,10]]},"number":"arXiv:2408.08921","publisher":"arXiv","source":"arXiv.org","title":"Graph Retrieval-Augmented Generation: A Survey","title-short":"Graph Retrieval-Augmented Generation","type":"article","URL":"http://arxiv.org/abs/2408.08921"},{"id":"PEPStyleGuide","accessed":{"date-parts":[["2022",11,5]]},"citation-key":"PEPStyleGuide","title":"PEP 8 – Style Guide for Python Code | peps.python.org","type":"webpage","URL":"https://peps.python.org/pep-0008/"},{"id":"perozziDeepWalkOnlineLearning2014","abstract":"We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Perozzi","given":"Bryan"},{"family":"Al-Rfou","given":"Rami"},{"family":"Skiena","given":"Steven"}],"citation-key":"perozziDeepWalkOnlineLearning2014","container-title":"Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining","DOI":"10.1145/2623330.2623732","issued":{"date-parts":[["2014",8,24]]},"page":"701-710","source":"arXiv.org","title":"DeepWalk: Online Learning of Social Representations","title-short":"DeepWalk","type":"paper-conference","URL":"http://arxiv.org/abs/1403.6652"},{"id":"PractitionerGuideGraph","abstract":"Graph data closes the gap between the way humans and computers view the world. While computers rely on static rows and columns of data, people navigate and reason about life … - Selection from The Practitioner's Guide to Graph Data [Book]","accessed":{"date-parts":[["2024",6,23]]},"citation-key":"PractitionerGuideGraph","ISBN":"9781492044079","language":"en","title":"The Practitioner's Guide to Graph Data [Book]","type":"webpage","URL":"https://www.oreilly.com/library/view/the-practitioners-guide/9781492044062/"},{"id":"PraxishandbuchEinkaufMit","abstract":"So bewältigen Sie Ihre Aufgaben im Einkauf mit der SAP-Materialwirtschaft (MM)! Von der Bestellanforderung bis zur Rechnungsprüfung erläutert Ihnen...","accessed":{"date-parts":[["2024",7,14]]},"citation-key":"PraxishandbuchEinkaufMit","container-title":"Rheinwerk Verlag GmbH","language":"de","title":"Praxishandbuch Einkauf mit SAP ERP","type":"webpage","URL":"https://www.rheinwerk-verlag.de/praxishandbuch-einkauf-mit-sap-erp/"},{"id":"Presentation2011","abstract":"Spirals is conducting research activities in the domains of distributed systems and software engineering. Spirals aims at introducing more automation in the adaptation mechanisms of software systems, in particular, transitioning from adaptive systems to self-adaptive systems. In this context, Spirals targets three properties: self-healing, self-optimization, and self-protection. With self-healing, Spirals aims at studying and tailoring data ...","accessed":{"date-parts":[["2022",10,12]]},"citation-key":"Presentation2011","container-title":"Spirals","issued":{"date-parts":[["2011",7,21]]},"language":"en-US","title":"Presentation","type":"post-weblog","URL":"https://team.inria.fr/spirals/"},{"id":"ProductSchemaOrg","accessed":{"date-parts":[["2024",2,12]]},"citation-key":"ProductSchemaOrg","title":"Product - Schema.org Type","type":"webpage","URL":"https://schema.org/Product"},{"id":"ProfessionelleMobileServiceRoboter","abstract":"MetraLabs entwickelt mobile Roboterplattformen und Servicerobotikanwendungen für den Einzelhandel, für Forschungseinrichtungen und die Industrie.","accessed":{"date-parts":[["2022",12,6]]},"citation-key":"ProfessionelleMobileServiceRoboter","language":"de-DE","title":"Professionelle mobile Service-Roboter - MetraLabs","type":"document","URL":"https://www.metralabs.com/"},{"id":"pylintPylint160dev2022","accessed":{"date-parts":[["2022",10,23]]},"author":[{"family":"Pylint","given":"Pylint"}],"citation-key":"pylintPylint160dev2022","issued":{"date-parts":[["2022",11,10]]},"title":"Pylint 2.16.0-dev documentation","type":"software","URL":"https://pylint.pycqa.org/en/latest/"},{"id":"pymongoPythonDriverMongoDB2024","accessed":{"date-parts":[["2024",7,18]]},"author":[{"family":"Pymongo","given":""}],"citation-key":"pymongoPythonDriverMongoDB2024","genre":"Python","issued":{"date-parts":[["2024"]]},"license":"OSI Approved :: Apache Software License","medium":"MacOS :: MacOS X, Microsoft :: Windows, POSIX","source":"PyPI","title":"Python driver for MongoDB","title-short":"pymongo","type":"software","URL":"https://www.mongodb.org","version":"4.8.0"},{"id":"pyoxigraphHomepage","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Pyoxigraph","given":""}],"citation-key":"pyoxigraphHomepage","title":"Homepage","type":"webpage","URL":"https://pyoxigraph.readthedocs.io/en/stable/"},{"id":"PyRAPL2022","abstract":"a library to measure the python energy consumption of python code","accessed":{"date-parts":[["2022",10,12]]},"citation-key":"PyRAPL2022","genre":"Python","issued":{"date-parts":[["2022",10,9]]},"license":"MIT","original-date":{"date-parts":[["2019",9,17]]},"publisher":"PowerAPI","source":"GitHub","title":"PyRAPL","type":"software","URL":"https://github.com/powerapi-ng/pyRAPL"},{"id":"PythonDataAnalysis","accessed":{"date-parts":[["2023",1,3]]},"citation-key":"PythonDataAnalysis","language":"en","title":"Python for Data Analysis, 3E","type":"webpage","URL":"https://wesmckinney.com/book/"},{"id":"pythonHomepage2024","abstract":"The official home of the Python Programming Language","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Python","given":""}],"citation-key":"pythonHomepage2024","container-title":"Python.org","issued":{"date-parts":[["2024",7,12]]},"language":"en","title":"Homepage","type":"webpage","URL":"https://www.python.org/"},{"id":"quaunautGuideNewestUs2012","accessed":{"date-parts":[["2023",2,5]]},"author":[{"family":"quaunaut","given":""}],"citation-key":"quaunautGuideNewestUs2012","container-title":"r/GuildWars","genre":"Reddit Post","issued":{"date-parts":[["2012",3,21]]},"title":"A Guide for the Newest of Us","type":"post","URL":"www.reddit.com/r/GuildWars/comments/r6ab5/a_guide_for_the_newest_of_us/"},{"id":"QueryingSPARQLRdflib","accessed":{"date-parts":[["2024",3,4]]},"citation-key":"QueryingSPARQLRdflib","title":"Querying with SPARQL — rdflib 7.0.0 documentation","type":"webpage","URL":"https://rdflib.readthedocs.io/en/stable/intro_to_sparql.html"},{"id":"QuickerPlanogramCompliance","accessed":{"date-parts":[["2023",2,28]]},"citation-key":"QuickerPlanogramCompliance","language":"en-US","title":"Quicker planogram compliance checks with Image recognition in VisitBasis retail audit mobile app – Managing a field team, merchandising and store audit. Retail execution software.","type":"document","URL":"https://www.visitbasis.com/image-recognition/image-recognition-in-retail/"},{"id":"R2RMLRDBRDF","accessed":{"date-parts":[["2024",3,21]]},"citation-key":"R2RMLRDBRDF","title":"R2RML: RDB to RDF Mapping Language","type":"webpage","URL":"https://www.w3.org/TR/r2rml/"},{"id":"raithelQuantitativeForschungPraxiskurs2008","abstract":"Alle angehenden Sozialwissenschaftlerinnen und Sozialwissenschaftler benötigen grundlegende Kenntnisse und praxisorientierte Übung in den Methoden quantitativer Sozialforschung. Dieses anwendungsorientierte Lehrbuch führt in den erforderlichen Schritten in die Grundfragen empirischer Sozialforschung ein und erläutert den Forschungsprozess in allen Phasen seiner Entwicklung. Im Zentrum steht die sozialwissenschaftliche Praxis, die Analyse und Übersetzung von Daten mit SPSS","author":[{"family":"Raithel","given":"Jürgen"}],"citation-key":"raithelQuantitativeForschungPraxiskurs2008","collection-title":"Lehrbuch","edition":"2., durchgesehene Auflage","event-place":"Wiesbaden","ISBN":"978-3-531-16181-5","issued":{"date-parts":[["2008"]]},"language":"ger","number-of-pages":"214","publisher":"VS Verlag für Sozialwissenschaften","publisher-place":"Wiesbaden","source":"K10plus ISBN","title":"Quantitative Forschung: ein Praxiskurs","title-short":"Quantitative Forschung","type":"book"},{"id":"ramalhoFluentPythonClear2022","abstract":"Don't waste time bending Python to fit patterns you've learned in other languages. Python's simplicity lets you become productive quickly, but often this means you aren't using everything the language has to offer. With the updated edition of this hands-on guide, you'll learn how to write effective, modern Python 3 code by leveraging its best ideas. Discover and apply idiomatic Python 3 features beyond your past experience. Author Luciano Ramalho guides you through Python's core language features and libraries and teaches you how to make your code shorter, faster, and more readable. Complete with major updates throughout, this new edition features five parts that work as five short books within the book: Data structures: Sequences, dicts, sets, Unicode, and data classesFunctions as objects: First-class functions, related design patterns, and type hints in function declarationsObject-oriented idioms: Composition, inheritance, mixins, interfaces, operator overloading, protocols, and more static typesControl flow: Context managers, generators, coroutines, async/await, and thread/process poolsMetaprogramming: Properties, attribute descriptors, class decorators, and new class metaprogramming hooks that replace or simplify metaclasses","author":[{"family":"Ramalho","given":"Luciano"}],"citation-key":"ramalhoFluentPythonClear2022","edition":"2nd Edition","event-place":"Beijing China ; Boston [MA]","ISBN":"978-1-4920-5635-5","issued":{"date-parts":[["2022",5,10]]},"language":"Englisch","number-of-pages":"983","publisher":"O'Reilly Media","publisher-place":"Beijing China ; Boston [MA]","source":"Amazon","title":"Fluent Python: Clear, Concise, and Effective Programming","title-short":"Fluent Python","type":"book"},{"id":"RDFConceptsAbstract","accessed":{"date-parts":[["2024",3,17]]},"citation-key":"RDFConceptsAbstract","title":"RDF 1.1 Concepts and Abstract Syntax","type":"webpage","URL":"https://www.w3.org/TR/rdf11-concepts/"},{"id":"RDFConceptsAbstracta","accessed":{"date-parts":[["2024",6,26]]},"citation-key":"RDFConceptsAbstracta","title":"RDF 1.2 Concepts and Abstract Syntax","type":"webpage","URL":"https://www.w3.org/TR/rdf12-concepts/"},{"id":"RDFMappingLanguage","accessed":{"date-parts":[["2024",2,24]]},"citation-key":"RDFMappingLanguage","title":"RDF Mapping Language (RML)","type":"webpage","URL":"https://rml.io/specs/rml/"},{"id":"RDFPrimera","accessed":{"date-parts":[["2024",2,26]]},"citation-key":"RDFPrimera","title":"RDF 1.1 Primer","type":"webpage","URL":"https://www.w3.org/TR/rdf11-primer/"},{"id":"reddyShoppingQueriesDataset2022","abstract":"Improving the quality of search results can significantly enhance users experience and engagement with search engines. In spite of several recent advancements in the fields of machine learning and data mining, correctly classifying items for a particular user search query has been a long standing challenge, which still has a large room for improvement. This paper introduces the “Shopping Queries Dataset”, a large dataset of difficult Amazon search queries and results, publicly released with the aim of fostering research in improving the quality of search results. The dataset contains around 130 thousand unique queries and 2.6 million manually labeled (query,product) relevance judgements. The dataset is multilingual with queries in English, Japanese, and Spanish. The Shopping Queries Dataset is being used in one of the KDDCup’22 challenges. In this paper, we describe the dataset and present three evaluation tasks along with baseline results: (i) ranking the results list, (ii) classifying product results into relevance categories, and (iii) identifying substitute products for a given query. We anticipate that this data will become the gold standard for future research in the topic of product search.","accessed":{"date-parts":[["2024",2,15]]},"author":[{"family":"Reddy","given":"Chandan K."},{"family":"Màrquez","given":"Lluís"},{"family":"Valero","given":"Fran"},{"family":"Rao","given":"Nikhil"},{"family":"Zaragoza","given":"Hugo"},{"family":"Bandyopadhyay","given":"Sambaran"},{"family":"Biswas","given":"Arnab"},{"family":"Xing","given":"Anlu"},{"family":"Subbian","given":"Karthik"}],"citation-key":"reddyShoppingQueriesDataset2022","issued":{"date-parts":[["2022",6,14]]},"language":"en","number":"arXiv:2206.06588","publisher":"arXiv","source":"arXiv.org","title":"Shopping Queries Dataset: A Large-Scale ESCI Benchmark for Improving Product Search","title-short":"Shopping Queries Dataset","type":"article","URL":"http://arxiv.org/abs/2206.06588"},{"id":"redisRedisEchtzeitDatenplattform","abstract":"Die beliebtesteEchtzeit‑Datenplattform der Welt Redis Enterprise ist einfach die beste Version von Redis, der beliebtesten Datenbank der Welt. Sie bietet unübertroffene Leistung, Skalierbarkeit, Innovation und Kosteneffizienz in Clouds, On-Prem und bei hybriden Implementierungen. Führende Unternehmen bauen mit Redis Enterprise Das ultimative Redis Erfahrung Optimale Unterstützung für Anwendungen der Digitalen Wirtschaft","accessed":{"date-parts":[["2024",6,23]]},"author":[{"family":"redis","given":""}],"citation-key":"redisRedisEchtzeitDatenplattform","container-title":"Redis","language":"de","title":"Redis - Die Echtzeit-Datenplattform","type":"webpage","URL":"https://redis.io/de/"},{"id":"regulationeuno1169/2011EuropeanBankingUnion2011","accessed":{"date-parts":[["2024",6,1]]},"author":[{"family":"REGULATION (EU) No 1169/2011","given":""}],"citation-key":"regulationeuno1169/2011EuropeanBankingUnion2011","DOI":"10.5040/9781509909568","issued":{"date-parts":[["2011"]]},"language":"en","publisher":"Nomos Verlagsgesellschaft","source":"DOI.org (Crossref)","title":"The European Banking Union : A Compendium","title-short":"The European Banking Union","type":"document","URL":"http://www.bloomsburycollections.com/book/the-european-banking-union-a-compendium"},{"id":"reimersSentenceBERTSentenceEmbeddings2019","abstract":"BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.","accessed":{"date-parts":[["2023",9,26]]},"author":[{"family":"Reimers","given":"Nils"},{"family":"Gurevych","given":"Iryna"}],"citation-key":"reimersSentenceBERTSentenceEmbeddings2019","DOI":"10.48550/arXiv.1908.10084","issued":{"date-parts":[["2019",8,27]]},"number":"arXiv:1908.10084","publisher":"arXiv","source":"arXiv.org","title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks","title-short":"Sentence-BERT","type":"article","URL":"http://arxiv.org/abs/1908.10084"},{"id":"rekikAnalysisImpactRFID2008","abstract":"This paper considers the situation of a retail store subject to inventory inaccuracies stemming from execution problems. We assume that inventory inaccuracies are introduced by misplacement type errors that occur within the store, i.e. the whole quantity of products that is ordered and received from the supplier is not available on shelf to satisfy consumers’ demand either because the replenishment process from the backroom to shelves is prone to errors (e.g. products are lost during this transfer, products are forbidden in the backroom, products are put on other shelves than where they should be, etc.) or products are misplaced on other shelves by consumers during their visit to the store. We consider a Newsvendor model that captures this issue in a simple way: For a given quantity of products ordered from the supplier, only a random fraction is available for sales. We compare three approaches. In the first approach, the retailer is unaware of errors in the store. In the second approach, the retailer is aware of errors and optimizes its operations by taking into account this issue. The third approach deals with the case where the retailer deploys an advanced automatic identification technology (such as the Auto-ID system based on the Radio Frequency Identification (RFID) technology) to eliminate errors. In particular, we provide insights on the relative benefit of implementing the RFID technology (moving from approaches 2 to 3) compared to the benefit of optimizing the system in presence of inaccuracies (moving from approaches 1 to 2). We also provide an analytical expression of the cost of the RFID tag which makes its deployment cost effective.","accessed":{"date-parts":[["2023",1,13]]},"author":[{"family":"Rekik","given":"Yacine"},{"family":"Sahin","given":"Evren"},{"family":"Dallery","given":"Yves"}],"citation-key":"rekikAnalysisImpactRFID2008","collection-title":"Special Section on Recent Developments in the Design, Control, Planning and Scheduling of Productive Systems","container-title":"International Journal of Production Economics","DOI":"10.1016/j.ijpe.2006.08.024","ISSN":"0925-5273","issue":"1","issued":{"date-parts":[["2008",3]]},"language":"en","page":"264–278","title":"Analysis of the impact of the RFID technology on reducing product misplacement errors at retail stores","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0925527307001454","volume":"112"},{"id":"rekikAnalysisImpactRFID2008a","abstract":"This paper considers the situation of a retail store subject to inventory inaccuracies stemming from execution problems. We assume that inventory inaccuracies are introduced by misplacement type errors that occur within the store, i.e. the whole quantity of products that is ordered and received from the supplier is not available on shelf to satisfy consumers’ demand either because the replenishment process from the backroom to shelves is prone to errors (e.g. products are lost during this transfer, products are forbidden in the backroom, products are put on other shelves than where they should be, etc.) or products are misplaced on other shelves by consumers during their visit to the store. We consider a Newsvendor model that captures this issue in a simple way: For a given quantity of products ordered from the supplier, only a random fraction is available for sales. We compare three approaches. In the first approach, the retailer is unaware of errors in the store. In the second approach, the retailer is aware of errors and optimizes its operations by taking into account this issue. The third approach deals with the case where the retailer deploys an advanced automatic identification technology (such as the Auto-ID system based on the Radio Frequency Identification (RFID) technology) to eliminate errors. In particular, we provide insights on the relative benefit of implementing the RFID technology (moving from approaches 2 to 3) compared to the benefit of optimizing the system in presence of inaccuracies (moving from approaches 1 to 2). We also provide an analytical expression of the cost of the RFID tag which makes its deployment cost effective.","accessed":{"date-parts":[["2023",1,13]]},"author":[{"family":"Rekik","given":"Yacine"},{"family":"Sahin","given":"Evren"},{"family":"Dallery","given":"Yves"}],"citation-key":"rekikAnalysisImpactRFID2008a","collection-title":"Special Section on Recent Developments in the Design, Control, Planning and Scheduling of Productive Systems","container-title":"International Journal of Production Economics","container-title-short":"International Journal of Production Economics","DOI":"10.1016/j.ijpe.2006.08.024","ISSN":"0925-5273","issue":"1","issued":{"date-parts":[["2008",3,1]]},"language":"en","page":"264-278","source":"ScienceDirect","title":"Analysis of the impact of the RFID technology on reducing product misplacement errors at retail stores","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0925527307001454","volume":"112"},{"id":"rekikExecutionErrorsRetail2008","abstract":"This article considers the situation of a supply chain consisting of a manufacturer and a retail store which faces an uncertainty not only in consumer demand but also in inventory records. Among execution errors that induce an uncertainty in inventory records are undetected supplier unreliabilities, unrecorded item movements (either during the receiving process or within the store), theft, damaged products, etc. In our work, we assume that such inventory inaccuracies are introduced by misplacement-type errors that occur within the store, i.e. the whole quantity of products that is received from the manufacturer is not available on shelf to satisfy consumers’ demand either because within the store, the replenishment process from the backroom to shelves is prone to errors (e.g. products are lost during this transfer, some of the products are forbidden in the backroom, other products are put on the wrong shelves, etc…) or products are moved and put on other shelves by consumers during their visit to the store. The framework we consider to model the misplaced products issue is a decentralised Newsvendor model. Within this setting, we analyse four scenarii. Each scenario can be characterised by (i) whether the manufacturer and the retailer are aware of misplacement errors that occur in the store or not (they are not aware of misplacement errors or even that they know the existence of errors, they choose to ignore them) (ii) whether there is a coordination between actors or not. Based on these scenarii, our aim is to evaluate the benefit of having information on errors and optimising the system by taking this information into account as well as the benefit of coordinating the channel.","accessed":{"date-parts":[["2023",1,14]]},"author":[{"family":"Rekik","given":"Yacine"},{"family":"Sahin","given":"Evren"},{"family":"Jemai","given":"Zied"},{"family":"Dallery","given":"Yves"}],"citation-key":"rekikExecutionErrorsRetail2008","container-title":"International Journal of Systems Science","DOI":"10.1080/00207720802090948","ISSN":"0020-7721","issue":"7","issued":{"date-parts":[["2008",7]]},"page":"727–740","title":"Execution errors in retail supply chains: analysis of the case of misplaced products","title-short":"Execution errors in retail supply chains","type":"article-journal","URL":"https://doi.org/10.1080/00207720802090948","volume":"39"},{"id":"rekikExecutionErrorsRetail2008a","abstract":"This article considers the situation of a supply chain consisting of a manufacturer and a retail store which faces an uncertainty not only in consumer demand but also in inventory records. Among execution errors that induce an uncertainty in inventory records are undetected supplier unreliabilities, unrecorded item movements (either during the receiving process or within the store), theft, damaged products, etc. In our work, we assume that such inventory inaccuracies are introduced by misplacement-type errors that occur within the store, i.e. the whole quantity of products that is received from the manufacturer is not available on shelf to satisfy consumers’ demand either because within the store, the replenishment process from the backroom to shelves is prone to errors (e.g. products are lost during this transfer, some of the products are forbidden in the backroom, other products are put on the wrong shelves, etc…) or products are moved and put on other shelves by consumers during their visit to the store. The framework we consider to model the misplaced products issue is a decentralised Newsvendor model. Within this setting, we analyse four scenarii. Each scenario can be characterised by (i) whether the manufacturer and the retailer are aware of misplacement errors that occur in the store or not (they are not aware of misplacement errors or even that they know the existence of errors, they choose to ignore them) (ii) whether there is a coordination between actors or not. Based on these scenarii, our aim is to evaluate the benefit of having information on errors and optimising the system by taking this information into account as well as the benefit of coordinating the channel.","accessed":{"date-parts":[["2023",1,14]]},"author":[{"family":"Rekik","given":"Yacine"},{"family":"Sahin","given":"Evren"},{"family":"Jemai","given":"Zied"},{"family":"Dallery","given":"Yves"}],"citation-key":"rekikExecutionErrorsRetail2008a","container-title":"International Journal of Systems Science","DOI":"10.1080/00207720802090948","ISSN":"0020-7721","issue":"7","issued":{"date-parts":[["2008",7,1]]},"page":"727-740","publisher":"Taylor & Francis","source":"Taylor and Francis+NEJM","title":"Execution errors in retail supply chains: analysis of the case of misplaced products","title-short":"Execution errors in retail supply chains","type":"article-journal","URL":"https://doi.org/10.1080/00207720802090948","volume":"39"},{"id":"RewePickGo2023","abstract":"Die Pick and Go Geschäfte von Rewe finden Sie zurzeit nur in sehr wenigen Städten. Mit dem innovativen Konzept prescht die Handelskette voran und es bleibt natürlich abzuwarten, wie die Pick & Go Läden von den Kunden angenommen werden.","accessed":{"date-parts":[["2023",2,13]]},"citation-key":"RewePickGo2023","issued":{"date-parts":[["2023",2]]},"language":"de","title":"Rewe Pick&Go: So funktioniert der Rewe ohne Kasse","title-short":"Rewe Pick&Go","type":"document","URL":"https://praxistipps.chip.de/rewe-pickgo-so-funktioniert-der-rewe-ohne-kasse_155600"},{"id":"rfc3986","abstract":"A Uniform Resource Identifier (URI) is a compact sequence of characters that identifies an abstract or physical resource. This specification defines the generic URI syntax and a process for resolving URI references that might be in relative form, along with guidelines and security considerations for the use of URIs on the Internet. The URI syntax defines a grammar that is a superset of all valid URIs, allowing an implementation to parse the common components of a URI reference without knowing the scheme-specific requirements of every possible identifier. This specification does not define a generative grammar for URIs; that task is performed by the individual specifications of each URI scheme. [STANDARDS-TRACK]","author":[{"family":"Berners-Lee","given":"Tim"},{"family":"Fielding","given":"Roy T."},{"family":"Masinter","given":"Larry M"}],"citation-key":"rfc3986","DOI":"10.17487/RFC3986","issued":{"date-parts":[["2005",1]]},"number":"3986","publisher":"RFC Editor","title":"Uniform resource identifier (URI): Generic syntax","type":"document","URL":"https://www.rfc-editor.org/info/rfc3986"},{"id":"rfc3987","abstract":"This document defines a new protocol element, the Internationalized Resource Identifier (IRI), as a complement of the Uniform Resource Identifier (URI). An IRI is a sequence of characters from the Universal Character Set (Unicode/ISO 10646). A mapping from IRIs to URIs is defined, which means that IRIs can be used instead of URIs, where appropriate, to identify resources. The approach of defining a new protocol element was chosen instead of extending or changing the definition of URIs. This was done in order to allow a clear distinction and to avoid incompatibilities with existing software. Guidelines are provided for the use and deployment of IRIs in various protocols, formats, and software components that currently deal with URIs.","author":[{"family":"Dürst","given":"Martin J."},{"family":"Suignard","given":"Michel"}],"citation-key":"rfc3987","DOI":"10.17487/RFC3987","issued":{"date-parts":[["2005",1]]},"number":"3987","publisher":"RFC Editor","title":"Internationalized resource identifiers (IRIs)","type":"document","URL":"https://www.rfc-editor.org/info/rfc3987"},{"id":"RFIDBeiDecathlon2021","abstract":"Seit 2015 nutzt der Sport- und Outdoorartikelhändler Decathlon erfolgreich RFID-Technologie.","accessed":{"date-parts":[["2023",3,7]]},"citation-key":"RFIDBeiDecathlon2021","issued":{"date-parts":[["2021",8]]},"language":"de-DE","title":"RFID bei Decathlon: Eine historische Erfolgsgeschichte","title-short":"RFID bei Decathlon","type":"document","URL":"https://deltakonnect.de/rfid-bei-decathlon-erfolgsgeschichte/"},{"id":"rfidjournalHowMuchDoes","abstract":"Most companies that sell RFID tags do not quote prices because pricing is based on volume, the amount of memory […]","accessed":{"date-parts":[["2023",3,27]]},"author":[{"family":"RFID JOURNAL","given":""}],"citation-key":"rfidjournalHowMuchDoes","container-title":"RFID JOURNAL","language":"en-US","title":"How much does an RFID tag cost today?","type":"webpage","URL":"https://www.rfidjournal.com/faq/how-much-does-an-rfid-tag-cost-today"},{"id":"ricciIntroductionRecommenderSystems2011","abstract":"Recommender Systems (RSs) are software tools and techniques providing suggestions for items to be of use to a user. In this introductory chapter we briefly discuss basic RS ideas and concepts. Our main goal is to delineate, in a coherent and structured way, the chapters included in this handbook and to help the reader navigate the extremely rich and detailed content that the handbook offers.","accessed":{"date-parts":[["2024",8,1]]},"author":[{"family":"Ricci","given":"Francesco"},{"family":"Rokach","given":"Lior"},{"family":"Shapira","given":"Bracha"}],"citation-key":"ricciIntroductionRecommenderSystems2011","container-title":"Recommender Systems Handbook","DOI":"10.1007/978-0-387-85820-3_1","editor":[{"family":"Ricci","given":"Francesco"},{"family":"Rokach","given":"Lior"},{"family":"Shapira","given":"Bracha"},{"family":"Kantor","given":"Paul B."}],"event-place":"Boston, MA","ISBN":"978-0-387-85820-3","issued":{"date-parts":[["2011"]]},"language":"en","page":"1-35","publisher":"Springer US","publisher-place":"Boston, MA","source":"Springer Link","title":"Introduction to Recommender Systems Handbook","type":"chapter","URL":"https://doi.org/10.1007/978-0-387-85820-3_1"},{"id":"robinsonGraphDatabases2013","author":[{"family":"Robinson","given":"Ian"},{"family":"Webber","given":"James"},{"family":"Eifrem","given":"Emil"}],"call-number":"QA76.9.D3 R6455 2013","citation-key":"robinsonGraphDatabases2013","edition":"First edition","event-place":"Beijing ; Sebastopol, CA","ISBN":"978-1-4493-5626-2","issued":{"date-parts":[["2013"]]},"note":"OCLC: ocn829452424","number-of-pages":"208","publisher":"O'Reilly","publisher-place":"Beijing ; Sebastopol, CA","source":"Library of Congress ISBN","title":"Graph databases","type":"book"},{"id":"robinsonGraphDatabases2015","author":[{"family":"Robinson","given":"Ian"},{"family":"Webber","given":"James"},{"family":"Eifrem","given":"Emil"}],"call-number":"QA76.9.D3 R6455 2015","citation-key":"robinsonGraphDatabases2015","edition":"Second edition","event-place":"Beijing","ISBN":"978-1-4919-3089-2","issued":{"date-parts":[["2015"]]},"note":"OCLC: ocn911172345","number-of-pages":"218","publisher":"O'Reilly","publisher-place":"Beijing","source":"Library of Congress ISBN","title":"Graph databases","type":"book"},{"id":"RoboKudoDocumentation","accessed":{"date-parts":[["2023",1,16]]},"citation-key":"RoboKudoDocumentation","title":"RoboKudo documentation","type":"webpage","URL":"https://robokudo.ai.uni-bremen.de/"},{"id":"RunningAveragePower","abstract":"Overview and Intel's advisory guidance for side channel Running Average Power Limit Energy Reporting / CVE-2020-8694 , CVE-2020-8695 / INTEL-SA-00389","accessed":{"date-parts":[["2022",10,23]]},"citation-key":"RunningAveragePower","container-title":"Intel","language":"en","title":"Running Average Power Limit Energy Reporting CVE-2020-8694,...","type":"webpage","URL":"https://www.intel.com/content/www/us/en/developer/articles/technical/software-security-guidance/advisory-guidance/running-average-power-limit-energy-reporting.html"},{"id":"rw2021SHACLDataValidation2022","abstract":"We present an introduction and a review of Shapes Constraint Language (SHACL), the W3C recommendation language for validating RDF data. Validation can be used to detect inconsistencies in a dataset and it can provide data quality guarantees for the purpose of data exchange and interoperability. A SHACL document describes a set of constraints on RDF nodes, and a graph is valid with respect to the document if its nodes satisfy these constraints. In this lecture we explain the main concepts of this language, its constructs and their interaction. We review the different formal frameworks used to study this language and the different semantics proposed. We examine a number of related problems, from containment and satisfiability to the interaction of SHACL with inference rules. We also cover practical aspects of SHACL, discussing its implementations and state of adoption, to present a holistic review useful to practitioners and theoreticians alike.","accessed":{"date-parts":[["2024",2,16]]},"citation-key":"rw2021SHACLDataValidation2022","dimensions":"2:08:36","director":[{"literal":"RW 2021"}],"issued":{"date-parts":[["2022",5,2]]},"source":"YouTube","title":"SHACL: From Data Validation to Schema Reasoning for RDF Graphs - Paolo Pareti","title-short":"SHACL","type":"motion_picture","URL":"https://www.youtube.com/watch?v=GSw7ZhOvamY"},{"id":"sanlialpEnergyEfficiencyAnalysis2022","abstract":"Code refactoring is a time-consuming and effort-intensive process that is applied for making improvements to source codes. There exist several refactoring techniques to improve software quality. Some of them aim to reduce the energy consumption of the software. However, the combination of applied refactoring techniques is crucial to the success rate. In addition, to provide sustainable services on portable devices such as mobile phones and laptops, which rely on batteries, improving and optimizing the energy efficiency is important. This study focuses on examining the effect of code refactoring techniques on energy consumption. A total of 25 different source codes of applications programmed in the C# and Java languages are selected for the study, and combinations obtained from refactoring techniques are applied to these source codes. The combinations applied are analyzed using the maintainability index. Power consumption estimation tools are used to measure the energy consumption of the original and refactored codes. The results show that the combinations significantly improve the software’s energy efficiency. The results will provide a better understanding of the relationship between the energy efficiency of software and refactoring techniques. Moreover, they will help developers to improve their object-oriented code in terms of both energy efficiency and sustainability.","accessed":{"date-parts":[["2022",5,14]]},"author":[{"family":"Şanlıalp","given":"İbrahim"},{"family":"Öztürk","given":"Muhammed Maruf"},{"family":"Yiğit","given":"Tuncay"}],"citation-key":"sanlialpEnergyEfficiencyAnalysis2022","container-title":"Electronics","DOI":"10.3390/electronics11030442","ISSN":"2079-9292","issue":"3","issued":{"date-parts":[["2022",1]]},"language":"en","license":"http://creativecommons.org/licenses/by/3.0/","number":"3","page":"442","publisher":"Multidisciplinary Digital Publishing Institute","source":"www.mdpi.com","title":"Energy Efficiency Analysis of Code Refactoring Techniques for Green and Sustainable Software in Portable Devices","type":"article-journal","URL":"https://www.mdpi.com/2079-9292/11/3/442","volume":"11"},{"id":"santraComprehensiveSurveyComputer2019","abstract":"The ability to recognize a product on the shelf of a retail store is an ordinary human skill. The same recognition problem presents an exceptional challenge for machine vision systems. Automatic detection of products on the shelf of a retail store provides enhanced value-added consumer experience and commercial benefits to retailers. Compared to machine vision based object recognition system, automatic detection of retail products in a store setting has lesser number of successful attempts. In this paper, we present a survey of machine vision based retail product recognition system and define a new taxonomy for this field. We also describe the intrinsic challenges associated with the problem. In this comprehensive survey of published papers, we analyze features used in state-of-the-art attempts. The performances of these approaches are compared. The details of publicly available datasets are presented. The paper concludes pointing to possible directions of research in related fields.","accessed":{"date-parts":[["2023",1,14]]},"author":[{"family":"Santra","given":"Bikash"},{"family":"Mukherjee","given":"Dipti Prasad"}],"citation-key":"santraComprehensiveSurveyComputer2019","container-title":"Image and Vision Computing","DOI":"10.1016/j.imavis.2019.03.005","ISSN":"0262-8856","issued":{"date-parts":[["2019",6]]},"language":"en","page":"45–63","title":"A comprehensive survey on computer vision based approaches for automatic identification of products in retail store","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0262885619300277","volume":"86"},{"id":"santraComprehensiveSurveyComputer2019a","abstract":"The ability to recognize a product on the shelf of a retail store is an ordinary human skill. The same recognition problem presents an exceptional challenge for machine vision systems. Automatic detection of products on the shelf of a retail store provides enhanced value-added consumer experience and commercial benefits to retailers. Compared to machine vision based object recognition system, automatic detection of retail products in a store setting has lesser number of successful attempts. In this paper, we present a survey of machine vision based retail product recognition system and define a new taxonomy for this field. We also describe the intrinsic challenges associated with the problem. In this comprehensive survey of published papers, we analyze features used in state-of-the-art attempts. The performances of these approaches are compared. The details of publicly available datasets are presented. The paper concludes pointing to possible directions of research in related fields.","accessed":{"date-parts":[["2023",1,14]]},"author":[{"family":"Santra","given":"Bikash"},{"family":"Mukherjee","given":"Dipti Prasad"}],"citation-key":"santraComprehensiveSurveyComputer2019a","container-title":"Image and Vision Computing","container-title-short":"Image and Vision Computing","DOI":"10.1016/j.imavis.2019.03.005","ISSN":"0262-8856","issued":{"date-parts":[["2019",6,1]]},"language":"en","page":"45-63","source":"ScienceDirect","title":"A comprehensive survey on computer vision based approaches for automatic identification of products in retail store","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0262885619300277","volume":"86"},{"id":"sapArtikelEinzelartikelSAP","accessed":{"date-parts":[["2024",6,11]]},"author":[{"family":"SAP","given":""}],"citation-key":"sapArtikelEinzelartikelSAP","container-title":"Artikel: Einzelartikel","title":"Artikel: Einzelartikel | SAP Help Portal","type":"webpage","URL":"https://help.sap.com/docs/SAP_ERP/82265744ff764efb8b48ef431235214c/f296c7536e8e2a4be10000000a174cb4.html?version=6.06.latest"},{"id":"sapArtikelSammelartikelUnd","accessed":{"date-parts":[["2024",6,11]]},"author":[{"family":"SAP","given":""}],"citation-key":"sapArtikelSammelartikelUnd","container-title":"Artikel: Sammelartikel und Variantenartikel","title":"Artikel: Sammelartikel und Varianten | SAP Help Portal","type":"webpage","URL":"https://help.sap.com/docs/SAP_ERP/82265744ff764efb8b48ef431235214c/f596c7536e8e2a4be10000000a174cb4.html?version=6.06.latest"},{"id":"sapSAPHANACloud2024","accessed":{"date-parts":[["2024",7,24]]},"author":[{"family":"SAP","given":""}],"citation-key":"sapSAPHANACloud2024","issued":{"date-parts":[["2024"]]},"title":"SAP HANA Cloud, SAP HANA Database Graph Reference | SAP Help Portal","type":"webpage","URL":"https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-graph-reference/sap-hana-cloud-sap-hana-database-graph-reference"},{"id":"saranRobustVisualAnalysis2015","abstract":"This paper presents a novel visual analysis based framework for automated planogram compliance check in retail stores. Our framework provides an efficient and convenient solution for ensuring planogram compliance by real-time analysis of the shelf image acquired in freehand manner. We present a novel application of Hausdorff metric for occupancy computation in product shelf images. Subsequently, we present a robust solution for product counting which applies robust row detection algorithm, and exploits texture and color feature for accurate counting. In this context, our system addresses the most general scenario of multiple varieties in single product type. The empirical validation of our framework is demonstrated on range of real-life images from stores located across different geographies, where it has achieved satisfactory and encouraging results.","accessed":{"date-parts":[["2023",2,15]]},"author":[{"family":"Saran","given":"Anurag"},{"family":"Hassan","given":"Ehtesham"},{"family":"Maurya","given":"Avinash Kumar"}],"citation-key":"saranRobustVisualAnalysis2015","container-title":"2015 14th IAPR International Conference on Machine Vision Applications (MVA)","DOI":"10.1109/MVA.2015.7153257","issued":{"date-parts":[["2015",5]]},"page":"576–579","title":"Robust visual analysis for planogram compliance problem","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/7153257/"},{"id":"saranRobustVisualAnalysis2015a","abstract":"This paper presents a novel visual analysis based framework for automated planogram compliance check in retail stores. Our framework provides an efficient and convenient solution for ensuring planogram compliance by real-time analysis of the shelf image acquired in freehand manner. We present a novel application of Hausdorff metric for occupancy computation in product shelf images. Subsequently, we present a robust solution for product counting which applies robust row detection algorithm, and exploits texture and color feature for accurate counting. In this context, our system addresses the most general scenario of multiple varieties in single product type. The empirical validation of our framework is demonstrated on range of real-life images from stores located across different geographies, where it has achieved satisfactory and encouraging results.","accessed":{"date-parts":[["2023",2,15]]},"author":[{"family":"Saran","given":"Anurag"},{"family":"Hassan","given":"Ehtesham"},{"family":"Maurya","given":"Avinash Kumar"}],"citation-key":"saranRobustVisualAnalysis2015a","container-title":"2015 14th IAPR International Conference on Machine Vision Applications (MVA)","DOI":"10.1109/MVA.2015.7153257","event-place":"Tokyo, Japan","ISBN":"9784901122146","issued":{"date-parts":[["2015",5]]},"page":"576-579","publisher":"IEEE","publisher-place":"Tokyo, Japan","source":"Semantic Scholar","title":"Robust visual analysis for planogram compliance problem","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/7153257/"},{"id":"scalegrid2019DatabaseTrends2019","abstract":"Which databases are trending in 2019? Discover NoSQL vs. SQL, most popular databases, important metrics to track, and the most time-consuming management tasks.","accessed":{"date-parts":[["2024",6,23]]},"author":[{"family":"Scalegrid","given":""}],"citation-key":"scalegrid2019DatabaseTrends2019","issued":{"date-parts":[["2019"]]},"language":"en-US","section":"MySQL™","title":"2019 Database Trends: SQL Vs. NoSQL - Top Databases","title-short":"2019 Database Trends","type":"webpage","URL":"https://scalegrid.io/blog/2019-database-trends-sql-vs-nosql-top-databases-single-vs-multiple-database-use/"},{"id":"schema.orgHomepage","accessed":{"date-parts":[["2024",6,6]]},"author":[{"family":"Schema.org","given":""}],"citation-key":"schema.orgHomepage","title":"Homepage","type":"webpage","URL":"https://schema.org/"},{"id":"schneiderCourseModularizationApplied1973","accessed":{"date-parts":[["2024",2,23]]},"author":[{"family":"Schneider","given":"E. W."}],"citation-key":"schneiderCourseModularizationApplied1973","DOI":"10.1037/e436252004-001","issued":{"date-parts":[["1973"]]},"language":"en","publisher":"American Psychological Association","source":"DOI.org (Crossref)","title":"Course modularization applied: The interface system and its implications for sequence control and data analysis.: (436252004-001)","title-short":"Course modularization applied","type":"dataset","URL":"https://doi.apa.org/doi/10.1037/e436252004-001"},{"id":"schneiderDecadeKnowledgeGraphs2022","abstract":"In pace with developments in the research field of artificial intelligence, knowledge graphs (KGs) have attracted a surge of interest from both academia and industry. As a representation of semantic relations between entities, KGs have proven to be particularly relevant for natural language processing (NLP), experiencing a rapid spread and wide adoption within recent years. Given the increasing amount of research work in this area, several KG-related approaches have been surveyed in the NLP research community. However, a comprehensive study that categorizes established topics and reviews the maturity of individual research streams remains absent to this day. Contributing to closing this gap, we systematically analyzed 507 papers from the literature on KGs in NLP. Our survey encompasses a multifaceted review of tasks, research types, and contributions. As a result, we present a structured overview of the research landscape, provide a taxonomy of tasks, summarize our findings, and highlight directions for future work.","accessed":{"date-parts":[["2024",7,12]]},"author":[{"family":"Schneider","given":"Phillip"},{"family":"Schopf","given":"Tim"},{"family":"Vladika","given":"Juraj"},{"family":"Galkin","given":"Mikhail"},{"family":"Simperl","given":"Elena"},{"family":"Matthes","given":"Florian"}],"citation-key":"schneiderDecadeKnowledgeGraphs2022","container-title":"Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)","editor":[{"family":"He","given":"Yulan"},{"family":"Ji","given":"Heng"},{"family":"Li","given":"Sujian"},{"family":"Liu","given":"Yang"},{"family":"Chang","given":"Chua-Hui"}],"event-place":"Online only","event-title":"AACL-IJCNLP 2022","issued":{"date-parts":[["2022",11]]},"page":"601–614","publisher":"Association for Computational Linguistics","publisher-place":"Online only","source":"ACLWeb","title":"A Decade of Knowledge Graphs in Natural Language Processing: A Survey","title-short":"A Decade of Knowledge Graphs in Natural Language Processing","type":"paper-conference","URL":"https://aclanthology.org/2022.aacl-main.46"},{"id":"schwarzkopfYFilesGraphsJupyter","abstract":"Free diagram visualization extension for JupyterLab and Jupyter Notebook.","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Schwarzkopf","given":"Fabian"},{"family":"Müller","given":"sebastian"},{"family":"Häglsperger","given":"Michael"}],"citation-key":"schwarzkopfYFilesGraphsJupyter","container-title":"yWorks, the diagramming experts","language":"en","title":"yFiles Graphs for Jupyter - visualize graph networks with Python","type":"webpage","URL":"https://www.yworks.com/products/yfiles-graphs-for-jupyter"},{"id":"ScrapyFastPowerful","accessed":{"date-parts":[["2023",3,27]]},"citation-key":"ScrapyFastPowerful","title":"Scrapy | A Fast and Powerful Scraping and Web Crawling Framework","type":"webpage","URL":"https://scrapy.org/"},{"id":"SDMTIBSDMRDFizer2024","abstract":"An Efficient RML-Compliant Engine for Knowledge Graph Construction","accessed":{"date-parts":[["2024",2,24]]},"citation-key":"SDMTIBSDMRDFizer2024","genre":"Python","issued":{"date-parts":[["2024",2,18]]},"license":"Apache-2.0","original-date":{"date-parts":[["2018",5,3]]},"publisher":"Scientific Data Management Group","source":"GitHub","title":"SDM-TIB/SDM-RDFizer","type":"software","URL":"https://github.com/SDM-TIB/SDM-RDFizer"},{"id":"selingerAccessPathSelection1979","abstract":"In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research Laboratory.","author":[{"family":"Selinger","given":"P Griffiths"},{"family":"Astrahan","given":"M M"},{"family":"Chamberlin","given":"D D"},{"family":"Lorie","given":"R A"},{"family":"Price","given":"T G"}],"citation-key":"selingerAccessPathSelection1979","issued":{"date-parts":[["1979"]]},"language":"en","source":"Zotero","title":"Access Path Selection in a Relational Database Management System","type":"article-journal"},{"id":"ShelfArrangementKPIs","abstract":"Shelf arrangement and planogram KPIs are often considered the gold standard for retail visibility execution, which is why their compliance is integral to creating a Perfect Store.","accessed":{"date-parts":[["2023",2,15]]},"citation-key":"ShelfArrangementKPIs","language":"en","title":"Shelf Arrangement KPIs and Planogram Compliance through Image Recognition","type":"document","URL":"https://blog.paralleldots.com//blog/shelf-arrangement-kpis-and-planogram-compliance-through-image-recognition"},{"id":"skoshomepageSKOSSimpleKnowledge","accessed":{"date-parts":[["2024",6,16]]},"author":[{"family":"SKOS HomePage","given":""}],"citation-key":"skoshomepageSKOSSimpleKnowledge","title":"SKOS Simple Knowledge Organization System - home page","type":"webpage","URL":"https://www.w3.org/2004/02/skos/"},{"id":"skosreferenceSKOSSimpleKnowledge","accessed":{"date-parts":[["2024",5,17]]},"author":[{"family":"SKOS Reference","given":""}],"citation-key":"skosreferenceSKOSSimpleKnowledge","title":"SKOS Simple Knowledge Organization System Reference","type":"webpage","URL":"https://www.w3.org/TR/skos-reference/#mapping"},{"id":"SKOSSimpleKnowledge","accessed":{"date-parts":[["2024",2,23]]},"citation-key":"SKOSSimpleKnowledge","title":"SKOS Simple Knowledge Organization System - home page","type":"webpage","URL":"https://www.w3.org/2004/02/skos/"},{"id":"SKOSSimpleKnowledgea","accessed":{"date-parts":[["2024",4,20]]},"citation-key":"SKOSSimpleKnowledgea","title":"SKOS Simple Knowledge Organization System Reference","type":"webpage","URL":"https://www.w3.org/TR/skos-reference/#notation"},{"id":"SKOSSimpleKnowledgeb","accessed":{"date-parts":[["2024",4,20]]},"citation-key":"SKOSSimpleKnowledgeb","title":"SKOS Simple Knowledge Organization System Primer","type":"webpage","URL":"https://www.w3.org/TR/skos-primer/#secnotations"},{"id":"smelyakovEffectivenessModernText","abstract":"In the article features of functioning of the most common optical character recognition (OCR) tools EasyOCR and TesserOCR are considered; experimental analysis of results of work of these OCR is given for the most widespread data sources, such as electronic text document, internet resource, and banner; based on analysis of the experiment results from the comparative analysis of considered OCRs by time and accuracy was made; effective algorithm of using an OCR and recommendations for their application was offered for not-distorted data, for slightly and highly distorted data.","author":[{"family":"Smelyakov","given":"Kirill"},{"family":"Chupryna","given":"Anastasiya"},{"family":"Darahan","given":"Dmytro"},{"family":"Midina","given":"Serhii"}],"citation-key":"smelyakovEffectivenessModernText","language":"en","source":"Zotero","title":"Effectiveness of Modern Text Recognition Solutions and Tools for Common Data Sources","type":"article-journal"},{"id":"smelyakovEffectivenessModernTexta","abstract":"In the article features of functioning of the most common optical character recognition (OCR) tools EasyOCR and TesserOCR are considered; experimental analysis of results of work of these OCR is given for the most widespread data sources, such as electronic text document, internet resource, and banner; based on analysis of the experiment results from the comparative analysis of considered OCRs by time and accuracy was made; effective algorithm of using an OCR and recommendations for their application was offered for not-distorted data, for slightly and highly distorted data.","author":[{"family":"Smelyakov","given":"Kirill"},{"family":"Chupryna","given":"Anastasiya"},{"family":"Darahan","given":"Dmytro"},{"family":"Midina","given":"Serhii"}],"citation-key":"smelyakovEffectivenessModernTexta","language":"en","source":"Zotero","title":"Effectiveness of Modern Text Recognition Solutions and Tools for Common Data Sources","type":"article-journal"},{"id":"smithOverviewTesseractOCR2007","abstract":"The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy[1], is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier.","accessed":{"date-parts":[["2023",9,26]]},"author":[{"family":"Smith","given":"R."}],"citation-key":"smithOverviewTesseractOCR2007","container-title":"Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2","DOI":"10.1109/ICDAR.2007.4376991","event-place":"Curitiba, Parana, Brazil","event-title":"Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2","ISBN":"978-0-7695-2822-9","ISSN":"1520-5363","issued":{"date-parts":[["2007",9]]},"language":"en","page":"629-633","publisher":"IEEE","publisher-place":"Curitiba, Parana, Brazil","source":"DOI.org (Crossref)","title":"An Overview of the Tesseract OCR Engine","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/4376991/"},{"id":"soltiMisplacedProductDetection2018","abstract":"Accurate and timely provisioning of products to the customers is essential in retail environments to avoid missed sales opportunities. One cause for missed sales is that products are misplaced in the store. This can be addressed by fast and accurately detecting those misplacements. A problem of current detection methods for misplaced products is their reliance on up-to-date planogram information, which is often missing in practice. This paper investigates the effectiveness and efficiency of outlier detection methods for finding misplaced products without planograms. To that end, we conduct simulation studies with realistic parameters for different store parameters and sensor infrastructure settings. We also evaluate the detection methods in a real setting with an RFID inventory robot. The findings indicate that our proposed MiProD aggregation of individual detection methods consistently outperforms individual techniques in detecting misplaced products.","accessed":{"date-parts":[["2023",1,13]]},"author":[{"family":"Solti","given":"Andreas"},{"family":"Raffel","given":"Manuel"},{"family":"Romagnoli","given":"Giovanni"},{"family":"Mendling","given":"Jan"}],"citation-key":"soltiMisplacedProductDetection2018","container-title":"Decision Support Systems","DOI":"10.1016/j.dss.2018.06.006","ISSN":"0167-9236","issued":{"date-parts":[["2018",8]]},"language":"en","page":"76–87","title":"Misplaced product detection using sensor data without planograms","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0167923618301039","volume":"112"},{"id":"soltiMisplacedProductDetection2018a","abstract":"Accurate and timely provisioning of products to the customers is essential in retail environments to avoid missed sales opportunities. One cause for missed sales is that products are misplaced in the store. This can be addressed by fast and accurately detecting those misplacements. A problem of current detection methods for misplaced products is their reliance on up-to-date planogram information, which is often missing in practice. This paper investigates the effectiveness and efficiency of outlier detection methods for finding misplaced products without planograms. To that end, we conduct simulation studies with realistic parameters for different store parameters and sensor infrastructure settings. We also evaluate the detection methods in a real setting with an RFID inventory robot. The findings indicate that our proposed MiProD aggregation of individual detection methods consistently outperforms individual techniques in detecting misplaced products.","accessed":{"date-parts":[["2023",1,13]]},"author":[{"family":"Solti","given":"Andreas"},{"family":"Raffel","given":"Manuel"},{"family":"Romagnoli","given":"Giovanni"},{"family":"Mendling","given":"Jan"}],"citation-key":"soltiMisplacedProductDetection2018a","container-title":"Decision Support Systems","container-title-short":"Decision Support Systems","DOI":"10.1016/j.dss.2018.06.006","ISSN":"0167-9236","issued":{"date-parts":[["2018",8,1]]},"language":"en","page":"76-87","source":"ScienceDirect","title":"Misplaced product detection using sensor data without planograms","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0167923618301039","volume":"112"},{"id":"SPARQLQueryLanguage","accessed":{"date-parts":[["2024",2,19]]},"citation-key":"SPARQLQueryLanguage","title":"SPARQL Query Language for RDF","type":"webpage","URL":"https://www.w3.org/TR/rdf-sparql-query/"},{"id":"SPARQLTutorialPrimer","accessed":{"date-parts":[["2024",3,27]]},"citation-key":"SPARQLTutorialPrimer","title":"SPARQL Tutorial - Primer · GitBook","type":"webpage","URL":"https://docs.data.world/tutorials/sparql/primer_intro.html"},{"id":"speerConceptNetOpenMultilingual2018","abstract":"Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.","accessed":{"date-parts":[["2024",2,12]]},"author":[{"family":"Speer","given":"Robyn"},{"family":"Chin","given":"Joshua"},{"family":"Havasi","given":"Catherine"}],"citation-key":"speerConceptNetOpenMultilingual2018","DOI":"10.48550/arXiv.1612.03975","issued":{"date-parts":[["2018",12,11]]},"number":"arXiv:1612.03975","publisher":"arXiv","source":"arXiv.org","title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge","title-short":"ConceptNet 5.5","type":"article","URL":"http://arxiv.org/abs/1612.03975"},{"id":"speerConceptNetOpenMultilingual2018a","abstract":"Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.","accessed":{"date-parts":[["2024",7,23]]},"author":[{"family":"Speer","given":"Robyn"},{"family":"Chin","given":"Joshua"},{"family":"Havasi","given":"Catherine"}],"citation-key":"speerConceptNetOpenMultilingual2018a","DOI":"10.48550/arXiv.1612.03975","issued":{"date-parts":[["2018",12,11]]},"number":"arXiv:1612.03975","publisher":"arXiv","source":"arXiv.org","title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge","title-short":"ConceptNet 5.5","type":"article","URL":"http://arxiv.org/abs/1612.03975"},{"id":"spinellisCodeQualityOpen2006","abstract":"Page 26: How can I avoid off-by-one errors? Page 143: Are Trojan Horse attacks for real? Page 158: Where should I look when my application can't handle its workload? Page 256: How can I detect memory leaks? Page 309: How do I target my application to international markets? Page 394: How should I name my code's identifiers? Page 441: How can I find and improve the code coverage of my tests?  Diomidis Spinellis' first book, Code Reading, showed programmers how to understand and modify key functional properties of software.  Code Quality  focuses on non-functional properties, demonstrating how to meet such critical requirements as reliability, security, portability, and maintainability, as well as efficiency in time and space. Spinellis draws on hundreds of examples from open source projects--such as the Apache web and application servers, the BSD Unix systems, and the HSQLDB Java database--to illustrate concepts and techniques that every professional software developer will be able to appreciate and apply immediately. Complete files for the open source code illustrated in this book are available online at: http://www.spinellis.gr/codequality/","author":[{"family":"Spinellis","given":"Diomidis"}],"citation-key":"spinellisCodeQualityOpen2006","ISBN":"978-0-7686-8512-1","issued":{"date-parts":[["2006",4,3]]},"language":"en","number-of-pages":"610","publisher":"Adobe Press","source":"Google Books","title":"Code Quality: The Open Source Perspective","title-short":"Code Quality","type":"book"},{"id":"StartupWirWollen","abstract":"Das Start-up Ubica will mithilfe von Robotern die operativen Kosten in den Läden deutlich senken. Auch Rewe zeigt sich interessiert.","accessed":{"date-parts":[["2023",1,19]]},"citation-key":"StartupWirWollen","language":"de","title":"Start-up: „Wir wollen nun in die Skalierung gehen“ – Dieser Roboter fährt nachts durch DM-Filialen","title-short":"Start-up","type":"document","URL":"https://www.handelsblatt.com/unternehmen/start-up-wir-wollen-nun-in-die-skalierung-gehen-dieser-roboter-faehrt-nachts-durch-dm-filialen/28501380.html"},{"id":"statistaWikipediaAnzahlArtikel","abstract":"In den letzten zehn Jahren hat sich die Anzahl der Artikel bei Wikipedia mehr als verdoppelt – zum 14.","accessed":{"date-parts":[["2024",6,18]]},"author":[{"family":"Statista","given":""}],"citation-key":"statistaWikipediaAnzahlArtikel","container-title":"Statista","language":"de","title":"Wikipedia - Anzahl der Artikel weltweit 2023","type":"webpage","URL":"https://de.statista.com/statistik/daten/studie/195081/umfrage/anzahl-der-artikel-auf-wikipedia-weltweit/"},{"id":"stegmaierEvaluationCurrentRDF2011","abstract":"Unstructured data (e.g., digital still images) is generated, distributed and stored worldwide at an ever increasing rate. In order to provide eﬃcient annotation, storage and search capabilities among this data and XML based description formats, data stores and query languages have been introduced. As XML lacks on expressing semantic meanings and coherences, it has been enhanced by the Resource Description Format (RDF) and the associated query language SPARQL.","author":[{"family":"Stegmaier","given":"Florian"},{"family":"Grobner","given":"Udo"},{"family":"Doller","given":"Mario"},{"family":"Kosch","given":"Harald"}],"citation-key":"stegmaierEvaluationCurrentRDF2011","issued":{"date-parts":[["2011"]]},"language":"en","source":"Zotero","title":"Evaluation of Current RDF Database Solutions","type":"article-journal"},{"id":"stevensDeepLearningPyTorch2020","author":[{"family":"Stevens","given":"Eli"},{"family":"Antiga","given":"Luca"},{"family":"Viehmann","given":"Thomas"}],"citation-key":"stevensDeepLearningPyTorch2020","event-place":"Shelter Island, NY","ISBN":"978-1-61729-526-3","issued":{"date-parts":[["2020"]]},"language":"en","publisher":"Manning Publications Co","publisher-place":"Shelter Island, NY","title":"Deep learning with PyTorch","type":"book"},{"id":"stevensDeepLearningPyTorch2020a","author":[{"family":"Stevens","given":"Eli"},{"family":"Antiga","given":"Luca"},{"family":"Viehmann","given":"Thomas"}],"call-number":"QA76.87 .S745 2020","citation-key":"stevensDeepLearningPyTorch2020a","event-place":"Shelter Island, NY","ISBN":"978-1-61729-526-3","issued":{"date-parts":[["2020"]]},"language":"en","number-of-pages":"490","publisher":"Manning Publications Co","publisher-place":"Shelter Island, NY","source":"Library of Congress ISBN","title":"Deep learning with PyTorch","type":"book"},{"id":"stevensDeepLearningPyTorch2020b","author":[{"family":"Stevens","given":"Eli"},{"family":"Antiga","given":"Luca"},{"family":"Viehmann","given":"Thomas"}],"call-number":"QA76.87 .S745 2020","citation-key":"stevensDeepLearningPyTorch2020b","event-place":"Shelter Island, NY","ISBN":"978-1-61729-526-3","issued":{"date-parts":[["2020"]]},"language":"en","number-of-pages":"490","publisher":"Manning Publications Co","publisher-place":"Shelter Island, NY","source":"Library of Congress ISBN","title":"Deep learning with PyTorch","type":"book"},{"id":"stevensDeepLearningPyTorch2020c","author":[{"family":"Stevens","given":"Eli"},{"family":"Antiga","given":"Luca"},{"family":"Viehmann","given":"Thomas"}],"call-number":"QA76.87 .S745 2020","citation-key":"stevensDeepLearningPyTorch2020c","event-place":"Shelter Island, NY","ISBN":"978-1-61729-526-3","issued":{"date-parts":[["2020"]]},"language":"en","number-of-pages":"490","publisher":"Manning Publications Co","publisher-place":"Shelter Island, NY","source":"Library of Congress ISBN","title":"Deep learning with PyTorch","type":"book"},{"id":"stevensDeepLearningPyTorch2020d","author":[{"family":"Stevens","given":"Eli"},{"family":"Antiga","given":"Luca"},{"family":"Viehmann","given":"Thomas"}],"call-number":"QA76.87 .S745 2020","citation-key":"stevensDeepLearningPyTorch2020d","event-place":"Shelter Island, NY","ISBN":"978-1-61729-526-3","issued":{"date-parts":[["2020"]]},"language":"en","number-of-pages":"490","publisher":"Manning Publications Co","publisher-place":"Shelter Island, NY","source":"Library of Congress ISBN","title":"Deep learning with PyTorch","type":"book"},{"id":"stolzIntegratingProductClassification2018","abstract":"Product classification standards like eCl@ss and UNSPSC define tens of thousands of product categories, and some standards additionally provide specific product property definitions and enumerated values. Many organizations hold respective meta-data for their products readily available in back-end databases. While many approaches have been presented for using such standards for more granular product information on the Web of Data, none has so far received mainstream adoption. This can be partly explained by legal, technical, and administrative barriers for adoption. In this paper, we describe a novel approach for using product classification standards in Web data markup in Microdata and JSON-LD syntax that does not require the availability of proper Web ontology variants of the underlying standards. We can show that the approach can provide the very same effect for the consumption and interpretation of the resulting mark-up in a Linked Data and Semantic Web context. Our proposal has already been integrated into the official version of schema.org and can be readily used for research and business applications.","author":[{"family":"Stolz","given":"Alex"},{"family":"Hepp","given":"Martin"}],"citation-key":"stolzIntegratingProductClassification2018","collection-title":"Lecture Notes in Computer Science","container-title":"On the Move to Meaningful Internet Systems. OTM 2017 Workshops","DOI":"10.1007/978-3-319-73805-5_11","editor":[{"family":"Debruyne","given":"Christophe"},{"family":"Panetto","given":"Hervé"},{"family":"Weichhart","given":"Georg"},{"family":"Bollen","given":"Peter"},{"family":"Ciuciu","given":"Ioana"},{"family":"Vidal","given":"Maria-Esther"},{"family":"Meersman","given":"Robert"}],"event-place":"Cham","ISBN":"978-3-319-73805-5","issued":{"date-parts":[["2018"]]},"language":"en","page":"103-113","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"Integrating Product Classification Standards into Schema.org: eCl@ss and UNSPSC on the Web of Data","title-short":"Integrating Product Classification Standards into Schema.org","type":"paper-conference"},{"id":"stolzPCS2OWLGenericApproach2014","abstract":"The classiﬁcation of products and services enables reliable and eﬃcient electronic exchanges of product data across organizations. Many companies classify products (a) according to generic or industryspeciﬁc product classiﬁcation standards, or (b) by using proprietary category systems. Such classiﬁcation systems often contain thousands of product classes that are updated over time. This implies a large quantity of useful product category information for e-commerce applications on the Web of Data. Thus, instead of building up product ontologies from scratch, which is costly, tedious, error-prone, and high-maintenance, it is generally easier to derive them from existing classiﬁcations. In this paper, we (1) describe a generic, semi-automated method for deriving OWL ontologies from product classiﬁcation standards and proprietary category systems. Moreover, we (2) show that our approach generates logically and semantically correct vocabularies, and (3) present the practical beneﬁt of our approach. The resulting product ontologies are compatible with the GoodRelations vocabulary for e-commerce and with schema.org and can be used to enrich product and oﬀer descriptions on the Semantic Web with granular product type information from existing data sources.","accessed":{"date-parts":[["2024",2,22]]},"author":[{"family":"Stolz","given":"Alex"},{"family":"Rodriguez-Castro","given":"Bene"},{"family":"Radinger","given":"Andreas"},{"family":"Hepp","given":"Martin"}],"citation-key":"stolzPCS2OWLGenericApproach2014","collection-editor":[{"family":"Hutchison","given":"David"},{"family":"Kanade","given":"Takeo"},{"family":"Kittler","given":"Josef"},{"family":"Kleinberg","given":"Jon M."},{"family":"Kobsa","given":"Alfred"},{"family":"Mattern","given":"Friedemann"},{"family":"Mitchell","given":"John C."},{"family":"Naor","given":"Moni"},{"family":"Nierstrasz","given":"Oscar"},{"family":"Pandu Rangan","given":"C."},{"family":"Steffen","given":"Bernhard"},{"family":"Terzopoulos","given":"Demetri"},{"family":"Tygar","given":"Doug"},{"family":"Weikum","given":"Gerhard"}],"container-title":"The Semantic Web: Trends and Challenges","DOI":"10.1007/978-3-319-07443-6_43","editor":[{"family":"Presutti","given":"Valentina"},{"family":"Amato","given":"Claudia","non-dropping-particle":"d’"},{"family":"Gandon","given":"Fabien"},{"family":"Aquin","given":"Mathieu","non-dropping-particle":"d’"},{"family":"Staab","given":"Steffen"},{"family":"Tordai","given":"Anna"}],"event-place":"Cham","ISBN":"978-3-319-07442-9 978-3-319-07443-6","issued":{"date-parts":[["2014"]]},"language":"en","page":"644-658","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"PCS2OWL: A Generic Approach for Deriving Web Ontologies from Product Classification Systems","title-short":"PCS2OWL","type":"chapter","URL":"http://link.springer.com/10.1007/978-3-319-07443-6_43","volume":"8465"},{"id":"StrukturierteDatenFuer","abstract":"Hier erfährst du, wie du mithilfe strukturierter Daten für Produkte potenzielle Käufer ansprechen kannst, während sie auf Google nach Artikeln suchen.","accessed":{"date-parts":[["2024",6,6]]},"citation-key":"StrukturierteDatenFuer","container-title":"Google for Developers","language":"de-x-mtfrom-en","title":"Strukturierte Daten für Produkte hinzufügen | Google Search Central | Dokumentation","type":"webpage","URL":"https://developers.google.com/search/docs/appearance/structured-data/product?hl=de"},{"id":"StudieITTrends2023Pdf","accessed":{"date-parts":[["2024",2,21]]},"citation-key":"StudieITTrends2023Pdf","title":"Studie-IT-Trends-2023.pdf","type":"webpage","URL":"https://www.capgemini.com/de-de/wp-content/uploads/sites/8/2022/03/Studie-IT-Trends-2023.pdf"},{"id":"suchanekIntegratingWikidataTaxonomy2023","abstract":"Wikidata is one of the largest public general-purpose Knowledge Bases (KBs). Yet, due to its collaborative nature, its schema and taxonomy have become convoluted. For the YAGO 4 KB, we combined Wikidata with the ontology from Schema.org, which reduced and cleaned up the taxonomy and constraints and made it possible to run automated reasoners on the data. However, it also cut away large parts of the Wikidata taxonomy. In this paper, we present our eﬀort to merge the entire Wikidata taxonomy into the YAGO KB as much as possible. We pay particular attention to logical constraints and a careful distinction of classes and instances. Our work creates YAGO 4.5, which adds a rich layer of informative classes to YAGO, while at the same time keeping the KB logically consistent.","accessed":{"date-parts":[["2024",2,12]]},"author":[{"family":"Suchanek","given":"Fabian"},{"family":"Alam","given":"Mehwish"},{"family":"Bonald","given":"Thomas"},{"family":"Paris","given":"Pierre-Henri"},{"family":"Soria","given":"Jules"}],"citation-key":"suchanekIntegratingWikidataTaxonomy2023","issued":{"date-parts":[["2023",8,22]]},"language":"en","number":"arXiv:2308.11884","publisher":"arXiv","source":"arXiv.org","title":"Integrating the Wikidata Taxonomy into YAGO","type":"article","URL":"http://arxiv.org/abs/2308.11884"},{"id":"suchanekYAGOCoreSemantic","abstract":"We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as hasWonPrize). The facts have been automatically extracted from Wikipedia and uniﬁed with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships – and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.","author":[{"family":"Suchanek","given":"Fabian M"},{"family":"Kasneci","given":"Gjergji"},{"family":"Weikum","given":"Gerhard"}],"citation-key":"suchanekYAGOCoreSemantic","language":"en","source":"Zotero","title":"YAGO: A Core of Semantic Knowledge Unifying WordNet and Wikipedia","type":"article-journal"},{"id":"sunLoFTRDetectorFreeLocal2021","abstract":"We present a novel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to ﬁrst establish pixel-wise dense matches at a coarse level and later reﬁne the good matches at a ﬁne level. In contrast to dense methods that use a cost volume to search correspondences, we use self and cross attention layers in Transformer to obtain feature descriptors that are conditioned on both images. The global receptive ﬁeld provided by Transformer enables our method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outperforms state-of-the-art methods by a large margin. LoFTR also ranks ﬁrst on two public benchmarks of visual localization among the published methods. Code is available at our project page: https://zju3dv.github.io/loftr/.","accessed":{"date-parts":[["2023",9,9]]},"author":[{"family":"Sun","given":"Jiaming"},{"family":"Shen","given":"Zehong"},{"family":"Wang","given":"Yuang"},{"family":"Bao","given":"Hujun"},{"family":"Zhou","given":"Xiaowei"}],"citation-key":"sunLoFTRDetectorFreeLocal2021","issued":{"date-parts":[["2021",4,1]]},"language":"en","number":"arXiv:2104.00680","publisher":"arXiv","source":"arXiv.org","title":"LoFTR: Detector-Free Local Feature Matching with Transformers","title-short":"LoFTR","type":"article","URL":"http://arxiv.org/abs/2104.00680"},{"id":"TeamHeroBeginner","abstract":"This guide provides players using an entirely new account with a team build that is soon available and financed on the fly. It is meant to be played in the Nightfall campaign: It grows in power while progressing with the main quest and empowers the player to deal with more challenging PvE...","accessed":{"date-parts":[["2023",2,1]]},"citation-key":"TeamHeroBeginner","container-title":"PvXwiki","language":"en","title":"Team - 7 Hero Beginner Team","type":"webpage","URL":"https://gwpvx.fandom.com/wiki/Build:Team_-_7_Hero_Beginner_Team"},{"id":"TeamHeroBeginnera","abstract":"This page covers some basics for beginners. If you've played the game before you can skip it and return to the Beginner Team. The choice of your primary profession will have a great impact on your experience in Guild Wars. Although all professions can be used for all mid- and endgame content...","accessed":{"date-parts":[["2023",2,1]]},"citation-key":"TeamHeroBeginnera","container-title":"PvXwiki","language":"en","title":"Team - 7 Hero Beginner Team/Basics","type":"webpage","URL":"https://gwpvx.fandom.com/wiki/Build:Team_-_7_Hero_Beginner_Team/Basics"},{"id":"TechnologieFunktioniertAmazon","abstract":"Amazon schafft mit seinem neuen Shop-Konzept Amazon Go den lästigsten Teil beim Einkauf ab: Das Bezahlen an der Kasse. Schlange stehen war gestern. Möglich macht das \"Sensor Fusion\". Doch was verbirgt sich hinter dem Zauberwort? Amazon macht daraus ein Geheimnis. Mit welcher Technik also revolutioniert Amazon den Einkauf im Supermarkt?","accessed":{"date-parts":[["2023",2,13]]},"citation-key":"TechnologieFunktioniertAmazon","language":"de","title":"Technologie: So funktioniert Amazon Go: Die Technik hinter dem Zauberwort „Sensor Fusion“","title-short":"Technologie","type":"document","URL":"https://etailment.de/news/stories/Technologie-So-funktioniert-Amazon-Go-Die-Technik-hinter-dem-Zauberwort-Sensor-Fusion-20194"},{"id":"thomeClassificationInventoryManagement2010","abstract":"Inventory management plays a central role in modern economy and businesses. The One Warehouse and N retailers' operations research problem of inventory management is applied to a case study. Actual inventory classification schemes and management policies for the case study are analyzed. Alternatives are proposed.","author":[{"family":"Thomé","given":"Antonio Marcio"},{"family":"Vieira","given":"A.F.C."}],"citation-key":"thomeClassificationInventoryManagement2010","issued":{"date-parts":[["2010",1,1]]},"page":"87-94","source":"ResearchGate","title":"Classification and inventory management of retail products: A case study","title-short":"Classification and inventory management of retail products","type":"article-journal"},{"id":"tiddiKnowledgeGraphsTools2022","abstract":"This paper provides an extensive overview of the use of knowledge graphs in the context of Explainable Machine Learning. As of late, explainable AI has become a very active field of research by addressing the limitations of the latest machine learning solutions that often provide highly accurate, but hardly scrutable and interpretable decisions. An increasing interest has also been shown in the integration of Knowledge Representation techniques in Machine Learning applications, mostly motivated by the complementary strengths and weaknesses that could lead to a new generation of hybrid intelligent systems. Following this idea, we hypothesise that knowledge graphs, which naturally provide domain background knowledge in a machine-readable format, could be integrated in Explainable Machine Learning approaches to help them provide more meaningful, insightful and trustworthy explanations. Using a systematic literature review methodology we designed an analytical framework to explore the current landscape of Explainable Machine Learning. We focus particularly on the integration with structured knowledge at large scale, and use our framework to analyse a variety of Machine Learning domains, identifying the main characteristics of such knowledge-based, explainable systems from different perspectives. We then summarise the strengths of such hybrid systems, such as improved understandability, reactivity, and accuracy, as well as their limitations, e.g. in handling noise or extracting knowledge efficiently. We conclude by discussing a list of open challenges left for future research.","accessed":{"date-parts":[["2024",2,12]]},"author":[{"family":"Tiddi","given":"Ilaria"},{"family":"Schlobach","given":"Stefan"}],"citation-key":"tiddiKnowledgeGraphsTools2022","container-title":"Artificial Intelligence","container-title-short":"Artificial Intelligence","DOI":"10.1016/j.artint.2021.103627","ISSN":"0004-3702","issued":{"date-parts":[["2022",1,1]]},"page":"103627","source":"ScienceDirect","title":"Knowledge graphs as tools for explainable machine learning: A survey","title-short":"Knowledge graphs as tools for explainable machine learning","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0004370221001788","volume":"302"},{"id":"tomasdottirWhyHowJavaScript2017","accessed":{"date-parts":[["2022",5,14]]},"author":[{"family":"Tomasdottir","given":"Kristin Fjola"},{"family":"Aniche","given":"Mauricio"},{"family":"Deursen","given":"Arie","non-dropping-particle":"van"}],"citation-key":"tomasdottirWhyHowJavaScript2017","container-title":"2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)","DOI":"10.1109/ASE.2017.8115668","event-place":"Urbana, IL","event-title":"2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)","ISBN":"978-1-5386-2684-9","issued":{"date-parts":[["2017",10]]},"page":"578-589","publisher":"IEEE","publisher-place":"Urbana, IL","source":"DOI.org (Crossref)","title":"Why and how JavaScript developers use linters","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/8115668/"},{"id":"tonEFFECTPRODUCTVARIETY","author":[{"family":"Ton","given":"Zeynep"}],"citation-key":"tonEFFECTPRODUCTVARIETY","language":"en","title":"THE EFFECT OF PRODUCT VARIETY AND INVENTORY LEVELS ON MISPLACED PRODUCTS AT RETAIL STORES: A LONGITUDINAL STUDY","type":"article-journal"},{"id":"tonEFFECTPRODUCTVARIETYa","author":[{"family":"Ton","given":"Zeynep"}],"citation-key":"tonEFFECTPRODUCTVARIETYa","language":"en","source":"Zotero","title":"THE EFFECT OF PRODUCT VARIETY AND INVENTORY LEVELS ON MISPLACED PRODUCTS AT RETAIL STORES: A LONGITUDINAL STUDY","type":"article-journal"},{"id":"topwebsitesTopWebsitesRanking","abstract":"Top websites ranking in May 2024: See the full list of most visited websites in every category and country in the world for free - Click here","accessed":{"date-parts":[["2024",6,18]]},"author":[{"family":"TopWebsites","given":""}],"citation-key":"topwebsitesTopWebsitesRanking","container-title":"Similarweb","language":"en","title":"Top Websites Ranking - Most Visited Websites in May 2024","type":"webpage","URL":"https://www.similarweb.com/top-websites/"},{"id":"tyagiIntroductionFeatureDetection2020","abstract":"Feature detection and matching is an important task in many computer vision applications, such as structure-from-motion, image retrieval…","accessed":{"date-parts":[["2023",9,9]]},"author":[{"family":"Tyagi","given":"Deepanshu"}],"citation-key":"tyagiIntroductionFeatureDetection2020","container-title":"Data Breach","issued":{"date-parts":[["2020",4,7]]},"language":"en","title":"Introduction To Feature Detection And Matching","type":"post-weblog","URL":"https://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d"},{"id":"tyszkiewiczDISKLearningLocal2020","abstract":"Local feature frameworks are difficult to learn in an end-to-end fashion, due to the discreteness inherent to the selection and matching of sparse keypoints. We introduce DISK (DIScrete Keypoints), a novel method that overcomes these obstacles by leveraging principles from Reinforcement Learning (RL), optimizing end-to-end for a high number of correct feature matches. Our simple yet expressive probabilistic model lets us keep the training and inference regimes close, while maintaining good enough convergence properties to reliably train from scratch. Our features can be extracted very densely while remaining discriminative, challenging commonly held assumptions about what constitutes a good keypoint, as showcased in Fig. 1, and deliver state-of-the-art results on three public benchmarks.","accessed":{"date-parts":[["2023",9,26]]},"author":[{"family":"Tyszkiewicz","given":"Michał J."},{"family":"Fua","given":"Pascal"},{"family":"Trulls","given":"Eduard"}],"citation-key":"tyszkiewiczDISKLearningLocal2020","DOI":"10.48550/arXiv.2006.13566","issued":{"date-parts":[["2020",10,27]]},"number":"arXiv:2006.13566","publisher":"arXiv","source":"arXiv.org","title":"DISK: Learning local features with policy gradient","title-short":"DISK","type":"article","URL":"http://arxiv.org/abs/2006.13566"},{"id":"ubicaroboticsScanroboterFuerEinzelhandel","abstract":"Autonome Roboter zur Erstellung digitaler Zwillinge von Einzelhandelsfilialen mithilfe von KI-Bilderkennung.","accessed":{"date-parts":[["2023",3,27]]},"author":[{"family":"Ubica Robotics","given":""}],"citation-key":"ubicaroboticsScanroboterFuerEinzelhandel","container-title":"Ubica Robotics","language":"de-DE","title":"Scanroboter für den Einzelhandel","type":"webpage","URL":"https://www.ubica-robotics.eu"},{"id":"UnwelcomeGuestGuild","accessed":{"date-parts":[["2023",2,1]]},"citation-key":"UnwelcomeGuestGuild","language":"en","title":"An Unwelcome Guest - Guild Wars Wiki (GWW)","type":"webpage","URL":"https://wiki.guildwars.com/wiki/An_Unwelcome_Guest"},{"id":"vanasscheDeclarativeRDFGraph2023","abstract":"More and more data in various formats are integrated into knowledge graphs. However, there is no overview of existing approaches for generating knowledge graphs from heterogeneous (semi-)structured data, making it difficult to select the right one for a certain use case. To support better decision making, we study the existing approaches for generating knowledge graphs from heterogeneous (semi-)structured data relying on mapping languages. In this paper, we investigated existing mapping languages for schema and data transformations, and corresponding materialization and virtualization systems that generate knowledge graphs. We gather and unify 52 articles regarding knowledge graph generation from heterogeneous (semi-)structured data. We assess 15 characteristics on mapping languages for schema transformations, 5 characteristics for data transformations, and 14 characteristics for systems. Our survey paper provides an overview of the mapping languages and systems proposed the past two decades. Our work paves the way towards a better adoption of knowledge graph generation, as the right mapping language and system can be selected for each use case.","accessed":{"date-parts":[["2024",2,24]]},"author":[{"family":"Van Assche","given":"Dylan"},{"family":"Delva","given":"Thomas"},{"family":"Haesendonck","given":"Gerald"},{"family":"Heyvaert","given":"Pieter"},{"family":"De Meester","given":"Ben"},{"family":"Dimou","given":"Anastasia"}],"citation-key":"vanasscheDeclarativeRDFGraph2023","container-title":"Journal of Web Semantics","container-title-short":"Journal of Web Semantics","DOI":"10.1016/j.websem.2022.100753","ISSN":"1570-8268","issued":{"date-parts":[["2023",1,1]]},"page":"100753","source":"ScienceDirect","title":"Declarative RDF graph generation from heterogeneous (semi-)structured data: A systematic literature review","title-short":"Declarative RDF graph generation from heterogeneous (semi-)structured data","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S1570826822000373","volume":"75"},{"id":"vanengenQmFoodingredientparserruby2024","abstract":"Extract the structure of ingredient lists on food products","accessed":{"date-parts":[["2024",7,4]]},"author":[{"family":"Engen","given":"William","non-dropping-particle":"van"}],"citation-key":"vanengenQmFoodingredientparserruby2024","genre":"Ruby","issued":{"date-parts":[["2024",6,19]]},"license":"MIT","original-date":{"date-parts":[["2018",6,5]]},"publisher":"Questionmark","source":"GitHub","title":"q-m/food-ingredient-parser-ruby","type":"software","URL":"https://github.com/q-m/food-ingredient-parser-ruby"},{"id":"vaswaniAttentionAllYou2023","abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","accessed":{"date-parts":[["2023",9,26]]},"author":[{"family":"Vaswani","given":"Ashish"},{"family":"Shazeer","given":"Noam"},{"family":"Parmar","given":"Niki"},{"family":"Uszkoreit","given":"Jakob"},{"family":"Jones","given":"Llion"},{"family":"Gomez","given":"Aidan N."},{"family":"Kaiser","given":"Lukasz"},{"family":"Polosukhin","given":"Illia"}],"citation-key":"vaswaniAttentionAllYou2023","DOI":"10.48550/arXiv.1706.03762","issued":{"date-parts":[["2023",8,1]]},"number":"arXiv:1706.03762","publisher":"arXiv","source":"arXiv.org","title":"Attention Is All You Need","type":"article","URL":"http://arxiv.org/abs/1706.03762"},{"id":"visitbasisMachineLeaningProduct2021","accessed":{"date-parts":[["2023",2,28]]},"author":[{"literal":"VisitBasis"}],"citation-key":"visitbasisMachineLeaningProduct2021","issued":{"date-parts":[["2021",6]]},"title":"Machine leaning Product recognition in retail. VisitBasis is Free for teams of up to 10.","type":"document","URL":"https://www.youtube.com/watch?v=FsHanRz4nCo"},{"id":"visitbasisMachineLeaningProduct2021a","accessed":{"date-parts":[["2023",2,28]]},"citation-key":"visitbasisMachineLeaningProduct2021a","dimensions":"1:07","director":[{"literal":"VisitBasis"}],"issued":{"date-parts":[["2021",6,12]]},"source":"YouTube","title":"Machine leaning Product recognition in retail. VisitBasis is Free for teams of up to 10.","type":"motion_picture","URL":"https://www.youtube.com/watch?v=FsHanRz4nCo"},{"id":"visitbasisQuickerPlanogramCompliance","accessed":{"date-parts":[["2023",2,28]]},"author":[{"family":"visitbasis","given":""}],"citation-key":"visitbasisQuickerPlanogramCompliance","language":"en-US","title":"Quicker planogram compliance checks with Image recognition in VisitBasis retail audit mobile app – Managing a field team, merchandising and store audit. Retail execution software.","type":"post-weblog","URL":"https://www.visitbasis.com/image-recognition/image-recognition-in-retail/"},{"id":"VisualDataWebWebVOWL2024","abstract":"Visualizing ontologies on the Web","accessed":{"date-parts":[["2024",3,19]]},"citation-key":"VisualDataWebWebVOWL2024","genre":"JavaScript","issued":{"date-parts":[["2024",3,18]]},"license":"MIT","original-date":{"date-parts":[["2015",4,17]]},"publisher":"Visual Data Web","source":"GitHub","title":"VisualDataWeb/WebVOWL","type":"software","URL":"https://github.com/VisualDataWeb/WebVOWL"},{"id":"vrandecicWikidataFreeCollaborative2014","abstract":"This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.","accessed":{"date-parts":[["2024",2,12]]},"author":[{"family":"Vrandečić","given":"Denny"},{"family":"Krötzsch","given":"Markus"}],"citation-key":"vrandecicWikidataFreeCollaborative2014","container-title":"Communications of the ACM","container-title-short":"Commun. ACM","DOI":"10.1145/2629489","ISSN":"0001-0782","issue":"10","issued":{"date-parts":[["2014",9,23]]},"page":"78–85","source":"ACM Digital Library","title":"Wikidata: a free collaborative knowledgebase","title-short":"Wikidata","type":"article-journal","URL":"https://dl.acm.org/doi/10.1145/2629489","volume":"57"},{"id":"w3cnquadsRDFNQuads","accessed":{"date-parts":[["2024",6,27]]},"author":[{"family":"W3Cnquads","given":""}],"citation-key":"w3cnquadsRDFNQuads","title":"RDF 1.1 N-Quads","type":"webpage","URL":"https://www.w3.org/TR/n-quads/"},{"id":"w3crdf*RDFstarSPARQLstar","accessed":{"date-parts":[["2024",6,26]]},"author":[{"family":"W3CRDF*","given":""}],"citation-key":"w3crdf*RDFstarSPARQLstar","title":"RDF-star and SPARQL-star","type":"webpage","URL":"https://w3c.github.io/rdf-star/cg-spec/editors_draft.html"},{"id":"w3cRDFPrimer2014","accessed":{"date-parts":[["2024",2,13]]},"author":[{"family":"W3C","given":""}],"citation-key":"w3cRDFPrimer2014","issued":{"date-parts":[["2014",6,24]]},"title":"RDF 1.1 Primer","type":"webpage","URL":"https://www.w3.org/TR/rdf11-primer/"},{"id":"w3cRDFSchema2014","accessed":{"date-parts":[["2024",2,13]]},"author":[{"family":"W3C","given":""}],"citation-key":"w3cRDFSchema2014","issued":{"date-parts":[["2014",2,25]]},"title":"RDF Schema 1.1","type":"webpage","URL":"https://www.w3.org/TR/rdf-schema/"},{"id":"w3cRDFSchema2014a","accessed":{"date-parts":[["2024",3,17]]},"author":[{"family":"W3C","given":""}],"citation-key":"w3cRDFSchema2014a","issued":{"date-parts":[["2014",2,25]]},"title":"RDF Schema 1.1","type":"webpage","URL":"https://www.w3.org/TR/rdf-schema/"},{"id":"w3cResourceDescriptionFramework","accessed":{"date-parts":[["2024",2,23]]},"author":[{"family":"W3C","given":""}],"citation-key":"w3cResourceDescriptionFramework","title":"Resource Description Framework (RDF) Model and Syntax Specification","type":"webpage","URL":"https://www.w3.org/TR/1999/REC-rdf-syntax-19990222/"},{"id":"w3cSPARQLQueryLanguage2013","accessed":{"date-parts":[["2024",6,21]]},"author":[{"family":"W3C","given":""}],"citation-key":"w3cSPARQLQueryLanguage2013","issued":{"date-parts":[["2013"]]},"title":"SPARQL 1.1 Query Language","type":"webpage","URL":"https://www.w3.org/TR/sparql11-query/"},{"id":"w3ctrigRDFTriG","accessed":{"date-parts":[["2024",6,27]]},"author":[{"family":"W3Ctrig","given":""}],"citation-key":"w3ctrigRDFTriG","title":"RDF 1.1 TriG","type":"webpage","URL":"https://www.w3.org/TR/trig/"},{"id":"w4Top21Chinese2023","abstract":"Discover the key to cross-border success in China with a roundup of 21 e-commerce platforms tailored for global businesses.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"W4","given":""}],"citation-key":"w4Top21Chinese2023","issued":{"date-parts":[["2023"]]},"language":"en","title":"Top 21 Chinese Ecommerce Platforms: A Comprehensive Guide","title-short":"Top 21 Chinese Ecommerce Platforms","type":"webpage","URL":"https://blog.marketingblatt.com/en/china-ecommerce"},{"id":"wang2019dgl","author":[{"family":"Wang","given":"Minjie"},{"family":"Zheng","given":"Da"},{"family":"Ye","given":"Zihao"},{"family":"Gan","given":"Quan"},{"family":"Li","given":"Mufei"},{"family":"Song","given":"Xiang"},{"family":"Zhou","given":"Jinjing"},{"family":"Ma","given":"Chao"},{"family":"Yu","given":"Lingfan"},{"family":"Gai","given":"Yu"},{"family":"Xiao","given":"Tianjun"},{"family":"He","given":"Tong"},{"family":"Karypis","given":"George"},{"family":"Li","given":"Jinyang"},{"family":"Zhang","given":"Zheng"}],"citation-key":"wang2019dgl","container-title":"arXiv preprint arXiv:1909.01315","issued":{"date-parts":[["2019"]]},"title":"Deep graph library: a graph-centric, highly-performant package for graph neural networks","type":"article-journal"},{"id":"wangBillionscaleCommodityEmbedding2018","abstract":"Recommender systems (RSs) have been the most important technology for increasing the business in Taobao, the largest online consumer-to-consumer (C2C) platform in China. There are three major challenges facing RS in Taobao: scalability, sparsity and cold start. In this paper, we present our technical solutions to address these three challenges. The methods are based on a wellknown graph embedding framework. We first construct an item graph from users’ behavior history, and learn the embeddings of all items in the graph. The item embeddings are employed to compute pairwise similarities between all items, which are then used in the recommendation process. To alleviate the sparsity and cold start problems, side information is incorporated into the graph embedding framework. We propose two aggregation methods to integrate the embeddings of items and the corresponding side information. Experimental results from offline experiments show that methods incorporating side information are superior to those that do not. Further, we describe the platform upon which the embedding methods are deployed and the workflow to process the billion-scale data in Taobao. Using A/B test, we show that the online Click-Through-Rates (CTRs) are improved comparing to the previous collaborative filtering based methods widely used in Taobao, further demonstrating the effectiveness and feasibility of our proposed methods in Taobao’s live production environment.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Wang","given":"Jizhe"},{"family":"Huang","given":"Pipei"},{"family":"Zhao","given":"Huan"},{"family":"Zhang","given":"Zhibo"},{"family":"Zhao","given":"Binqiang"},{"family":"Lee","given":"Dik Lun"}],"citation-key":"wangBillionscaleCommodityEmbedding2018","issued":{"date-parts":[["2018",5,24]]},"language":"en","number":"arXiv:1803.02349","publisher":"arXiv","source":"arXiv.org","title":"Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba","type":"article","URL":"http://arxiv.org/abs/1803.02349"},{"id":"wangImageQualityAssessment2004","accessed":{"date-parts":[["2022",5,28]]},"author":[{"family":"Wang","given":"Z."},{"family":"Bovik","given":"A.C."},{"family":"Sheikh","given":"H.R."},{"family":"Simoncelli","given":"E.P."}],"citation-key":"wangImageQualityAssessment2004","container-title":"IEEE Transactions on Image Processing","container-title-short":"IEEE Trans. on Image Process.","DOI":"10.1109/TIP.2003.819861","ISSN":"1057-7149","issue":"4","issued":{"date-parts":[["2004",4]]},"language":"en","page":"600-612","source":"DOI.org (Crossref)","title":"Image Quality Assessment: From Error Visibility to Structural Similarity","title-short":"Image Quality Assessment","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/1284395/","volume":"13"},{"id":"wangImageQualityAssessment2004a","accessed":{"date-parts":[["2022",5,10]]},"author":[{"family":"Wang","given":"Z."},{"family":"Bovik","given":"A.C."},{"family":"Sheikh","given":"H.R."},{"family":"Simoncelli","given":"E.P."}],"citation-key":"wangImageQualityAssessment2004a","container-title":"IEEE Transactions on Image Processing","container-title-short":"IEEE Trans. on Image Process.","DOI":"10.1109/TIP.2003.819861","ISSN":"1057-7149","issue":"4","issued":{"date-parts":[["2004",4]]},"language":"en","page":"600-612","source":"DOI.org (Crossref)","title":"Image Quality Assessment: From Error Visibility to Structural Similarity","title-short":"Image Quality Assessment","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/1284395/","volume":"13"},{"id":"WasIstPIMSystem","abstract":"Hier erfahren Sie alles zu PIM-Systeme: ✓Was ist ein PIM-System ✓Aufgaben & Funktionsweise ✓Nutzen & Vorteile uvm. ► Jetzt informieren","accessed":{"date-parts":[["2024",3,28]]},"citation-key":"WasIstPIMSystem","container-title":"https://www.pim-verzeichnis.de/","language":"de_DE","title":"Was ist ein PIM-System? Alle Infos im Überblick!","title-short":"Was ist ein PIM-System?","type":"post-weblog","URL":"https://www.pim-verzeichnis.de/was-ist-ein-pim-system/"},{"id":"Wd2scENDORSEVOTATION","abstract":"info\n\nProject,<a href=\"https://github.com/CPT-PC/schemaOrg-bdMaps\">https://github.com/CPT-PC/schemaOrg-bdMaps</a>\nThis file/interface,<a href=\"https://github.com/CPT-PC/schemaOrg-bdMaps/issues/3\">https://github.com/CPT-PC/schemaOrg-bdMaps/issues/3</a>\nSpreadsheets\nwd2schema-std,Build with data fr...","accessed":{"date-parts":[["2024",2,12]]},"citation-key":"Wd2scENDORSEVOTATION","container-title":"Google Docs","language":"de","title":"wd2sc-ENDORSE-VOTATION","type":"webpage","URL":"https://docs.google.com/spreadsheets/d/1NjJNKzeTKXk6xiYdLv9iKCvvR68SOTGni90nZXjMJJ0/edit?usp=embed_facebook"},{"id":"weikumMachineKnowledgeCreation2021","abstract":"Equipping machines with comprehensive knowledge of the world’s entities and their relationships has been a long-standing goal of AI. Over the last decade, large-scale knowledge bases, also known as knowledge graphs, have been automatically constructed from web contents and text sources, and have become a key asset for search engines. This machine knowledge can be harnessed to semantically interpret textual phrases in news, social media and web tables, and contributes to question answering, natural language processing and data analytics.","accessed":{"date-parts":[["2024",2,23]]},"author":[{"family":"Weikum","given":"Gerhard"},{"family":"Dong","given":"Luna"},{"family":"Razniewski","given":"Simon"},{"family":"Suchanek","given":"Fabian"}],"citation-key":"weikumMachineKnowledgeCreation2021","issued":{"date-parts":[["2021",3,22]]},"language":"en","number":"arXiv:2009.11564","publisher":"arXiv","source":"arXiv.org","title":"Machine Knowledge: Creation and Curation of Comprehensive Knowledge Bases","title-short":"Machine Knowledge","type":"article","URL":"http://arxiv.org/abs/2009.11564"},{"id":"weikumMachineKnowledgeCreation2021a","abstract":"Equipping machines with comprehensive knowledge of the world's entities and their relationships has been a long-standing goal of AI. Over the last decade, large-scale knowledge bases, also known as knowledge graphs, have been automatically constructed from web contents and text sources, and have become a key asset for search engines. This machine knowledge can be harnessed to semantically interpret textual phrases in news, social media and web tables, and contributes to question answering, natural language processing and data analytics. This article surveys fundamental concepts and practical methods for creating and curating large knowledge bases. It covers models and methods for discovering and canonicalizing entities and their semantic types and organizing them into clean taxonomies. On top of this, the article discusses the automatic extraction of entity-centric properties. To support the long-term life-cycle and the quality assurance of machine knowledge, the article presents methods for constructing open schemas and for knowledge curation. Case studies on academic projects and industrial knowledge graphs complement the survey of concepts and methods.","accessed":{"date-parts":[["2024",6,27]]},"author":[{"family":"Weikum","given":"Gerhard"},{"family":"Dong","given":"Luna"},{"family":"Razniewski","given":"Simon"},{"family":"Suchanek","given":"Fabian"}],"citation-key":"weikumMachineKnowledgeCreation2021a","DOI":"10.48550/arXiv.2009.11564","issued":{"date-parts":[["2021",3,22]]},"number":"arXiv:2009.11564","publisher":"arXiv","source":"arXiv.org","title":"Machine Knowledge: Creation and Curation of Comprehensive Knowledge Bases","title-short":"Machine Knowledge","type":"article","URL":"http://arxiv.org/abs/2009.11564"},{"id":"WhatBestTime2015","abstract":"Our customers often ask what's the best time to send a survey email invitation. We crunched last year's numbers and here are the results...","accessed":{"date-parts":[["2024",3,17]]},"author":[{"family":"SurveyMonkey","given":""}],"citation-key":"WhatBestTime2015","container-title":"CheckMarket","issued":{"date-parts":[["2015",2,24]]},"language":"en-US","title":"What's the best time to send a survey?","type":"post-weblog","URL":"https://www.checkmarket.com/blog/survey-invitations-best-time-send/"},{"id":"whatis.comWhatPlanogramWhy","abstract":"Discover the purpose of planograms in retail environments and how they optimize product placement in stores.","accessed":{"date-parts":[["2023",3,12]]},"author":[{"family":"WhatIs.com","given":""}],"citation-key":"whatis.comWhatPlanogramWhy","container-title":"WhatIs.com","language":"en","title":"What is a planogram and why is it important in retail? – TechTarget Definition","title-short":"What is a planogram and why is it important in retail?","type":"webpage","URL":"https://www.techtarget.com/whatis/definition/planogram"},{"id":"WhatOntologyWhy","accessed":{"date-parts":[["2024",7,27]]},"citation-key":"WhatOntologyWhy","title":"What is an ontology and why we need it","type":"webpage","URL":"https://protege.stanford.edu/publications/ontology_development/ontology101-noy-mcguinness.html"},{"id":"WhatPlanogramWhy","abstract":"Discover the purpose of planograms in retail environments and how they optimize product placement in stores.","accessed":{"date-parts":[["2023",3,12]]},"citation-key":"WhatPlanogramWhy","language":"en","title":"What is a planogram and why is it important in retail? – TechTarget Definition","title-short":"What is a planogram and why is it important in retail?","type":"document","URL":"https://www.techtarget.com/whatis/definition/planogram"},{"id":"WhenBestTime","abstract":"You’ve written your questions. You’ve configured your survey logic. You’ve even had your colleagues take a spin through to make sure everything is working just right. You’re about to launch your survey, but maybe you’re wondering, “Is today the best day to send it?” Well, wonder no more. We looked across 100,000 surveys to figure […]","accessed":{"date-parts":[["2024",3,17]]},"author":[{"family":"CheckMarket","given":""}],"citation-key":"WhenBestTime","container-title":"SurveyMonkey","language":"en-US","title":"When is the best time to send a survey?","type":"webpage","URL":"https://www.surveymonkey.com/curiosity/day-of-the-week/"},{"id":"wikimediafoundationHomepage","abstract":"The nonprofit Wikimedia Foundation provides the essential infrastructure for free knowledge. We host Wikipedia, the free online encyclopedia, created, edited, and verified by volunteers around the world, as well as many other vital community projects. All of which is made possible thanks to donations from individuals like you. We welcome anyone who shares our vision….","accessed":{"date-parts":[["2024",6,18]]},"author":[{"family":"Wikimedia Foundation","given":""}],"citation-key":"wikimediafoundationHomepage","container-title":"Wikimedia Foundation","language":"en-US","title":"Homepage","type":"webpage","URL":"https://wikimediafoundation.org/"},{"id":"wikipedia_homepage","author":[{"literal":"Wikipedia"}],"citation-key":"wikipedia_homepage","issued":{"date-parts":[["2023"]]},"title":"Wikipedia, the free encyclopedia","type":"document","URL":"https://www.wikipedia.org/"},{"id":"wikipediaAmazonGoWikipedia","accessed":{"date-parts":[["2023",2,13]]},"author":[{"family":"wikipedia","given":""}],"citation-key":"wikipediaAmazonGoWikipedia","title":"Amazon Go – Wikipedia","type":"webpage","URL":"https://de.wikipedia.org/wiki/Amazon_Go"},{"id":"WikipediaHauptseite2024","abstract":"Artikel des Tages\nWas geschah am …?\nIn den Nachrichten\nVerstorben\nSchon gewusst?\nSchwesterprojekte","accessed":{"date-parts":[["2024",6,17]]},"citation-key":"WikipediaHauptseite2024","container-title":"Die freie Enzyklopädie","issued":{"date-parts":[["2024",4,10]]},"language":"de","license":"Creative Commons Attribution-ShareAlike License","note":"Page Version ID: 243938148","source":"Wikipedia","title":"Wikipedia:Hauptseite","title-short":"Wikipedia","type":"entry-encyclopedia","URL":"https://de.wikipedia.org/w/index.php?title=Wikipedia:Hauptseite&oldid=243938148"},{"id":"winklerKnowledgeEnabledRoboticAgents2016","abstract":"Autonomous robots in unstructured and dynamically changing retail environments have to master complex perception, knowledge processing, and manipulation tasks. To enable them to act competently, we propose a framework based on three core components: (◦) A background knowledge enabled perception system, which is capable of combining diverse information sources to cope with challenging conditions, such as occlusions and stacked objects with a variety of textures and shapes, (◦) Knowledge processing methods that identify the current scene, and produce strategies for tidying up supermarket racks, and (◦) The necessary careful manipulation skills in conﬁned spaces to arrange objects in semi-accessible rack shelves. We show that our approach yields feasible, situation aware manipulation strategies. We demonstrate our framework in an idealistic simulated environment, as well as on a real shopping rack using a PR2 robot. Typical supermarket products are detected and rearranged in the retail rack, tidying up what was left after customer interaction, or while restocking sold items.","accessed":{"date-parts":[["2023",1,14]]},"author":[{"family":"Winkler","given":"Jan"},{"family":"Balint-Benczedi","given":"Ferenc"},{"family":"Wiedemeyer","given":"Thiemo"},{"family":"Beetz","given":"Michael"},{"family":"Vaskevicius","given":"Narunas"},{"family":"Mueller","given":"Christian A."},{"family":"Fromm","given":"Tobias"},{"family":"Birk","given":"Andreas"}],"citation-key":"winklerKnowledgeEnabledRoboticAgents2016","issued":{"date-parts":[["2016",5]]},"language":"en","publisher":"arXiv","title":"Knowledge-Enabled Robotic Agents for Shelf Replenishment in Cluttered Retail Environments","type":"document","URL":"http://arxiv.org/abs/1605.04177"},{"id":"winklerKnowledgeEnabledRoboticAgents2016a","abstract":"Autonomous robots in unstructured and dynamically changing retail environments have to master complex perception, knowledge processing, and manipulation tasks. To enable them to act competently, we propose a framework based on three core components: (◦) A background knowledge enabled perception system, which is capable of combining diverse information sources to cope with challenging conditions, such as occlusions and stacked objects with a variety of textures and shapes, (◦) Knowledge processing methods that identify the current scene, and produce strategies for tidying up supermarket racks, and (◦) The necessary careful manipulation skills in conﬁned spaces to arrange objects in semi-accessible rack shelves. We show that our approach yields feasible, situation aware manipulation strategies. We demonstrate our framework in an idealistic simulated environment, as well as on a real shopping rack using a PR2 robot. Typical supermarket products are detected and rearranged in the retail rack, tidying up what was left after customer interaction, or while restocking sold items.","accessed":{"date-parts":[["2023",1,14]]},"author":[{"family":"Winkler","given":"Jan"},{"family":"Balint-Benczedi","given":"Ferenc"},{"family":"Wiedemeyer","given":"Thiemo"},{"family":"Beetz","given":"Michael"},{"family":"Vaskevicius","given":"Narunas"},{"family":"Mueller","given":"Christian A."},{"family":"Fromm","given":"Tobias"},{"family":"Birk","given":"Andreas"}],"citation-key":"winklerKnowledgeEnabledRoboticAgents2016a","issued":{"date-parts":[["2016",5,13]]},"language":"en","number":"arXiv:1605.04177","publisher":"arXiv","source":"arXiv.org","title":"Knowledge-Enabled Robotic Agents for Shelf Replenishment in Cluttered Retail Environments","type":"article","URL":"http://arxiv.org/abs/1605.04177"},{"id":"wongOptimizationNestedSQLQueries1987","author":[{"family":"Wong","given":"Harry K T"},{"family":"Laboratones","given":"LawrenceBerkeley"}],"citation-key":"wongOptimizationNestedSQLQueries1987","issued":{"date-parts":[["1987"]]},"language":"en","source":"Zotero","title":"Optimization of NestedSQL Queries Revisited","type":"article-journal"},{"id":"woodQueryLanguagesGraph2012","abstract":"Query languages for graph databases started to be investigated some 25 years ago. With much current data, such as linked data on the Web and social network data, being graph-structured, there has been a recent resurgence in interest in graph query languages. We provide a brief survey of many of the graph query languages that have been proposed, focussing on the core functionality provided in these languages. We also consider issues such as expressive power and the computational complexity of query evaluation.","accessed":{"date-parts":[["2024",6,26]]},"author":[{"family":"Wood","given":"Peter T."}],"citation-key":"woodQueryLanguagesGraph2012","container-title":"SIGMOD Rec.","DOI":"10.1145/2206869.2206879","ISSN":"0163-5808","issue":"1","issued":{"date-parts":[["2012",4,25]]},"page":"50–60","source":"ACM Digital Library","title":"Query languages for graph databases","type":"article-journal","URL":"https://doi.org/10.1145/2206869.2206879","volume":"41"},{"id":"xinKnowledgeGraphMachine2020","author":[{"family":"Xin","given":"Luna Dong"}],"citation-key":"xinKnowledgeGraphMachine2020","event-place":"Stanford CS520 Knowlege Graphs--How should AI explicitly represent knowledge","issued":{"date-parts":[["2020"]]},"language":"English","publisher-place":"Stanford CS520 Knowlege Graphs--How should AI explicitly represent knowledge","title":"Knowledge Graph And Machine Learning: A Natural Synergy","type":"speech"},{"id":"xiu-shenweiRPCLargescaleFinegrained2022","author":[{"family":"Xiu-Shen WEI","given":"Jian YANG","suffix":"Quan CUI, Lei YANG, Peng WANG, Lingqiao LIU"}],"citation-key":"xiu-shenweiRPCLargescaleFinegrained2022","container-title":"SCIENCE CHINA Information Sciences","issue":"9","issued":{"date-parts":[["2022"]]},"page":"197101-","title":"RPC: a large-scale and fine-grained retail product checkout dataset","type":"article-journal","URL":"http://www.sciengine.com/publisher/Science China Press/journal/SCIENCE CHINA Information Sciences/65/9/10.1007/s11432-022-3513-y,   doi =","volume":"65"},{"id":"xiu-shenweiRPCLargescaleFinegrained2022a","author":[{"family":"Xiu-Shen WEI","given":"Jian YANG","suffix":"Quan CUI, Lei YANG, Peng WANG, Lingqiao LIU"}],"citation-key":"xiu-shenweiRPCLargescaleFinegrained2022a","container-title":"SCIENCE CHINA Information Sciences","issue":"9","issued":{"date-parts":[["2022"]]},"page":"197101-","title":"RPC: a large-scale and fine-grained retail product checkout dataset","type":"article-journal","URL":"http://www.sciengine.com/publisher/Science China Press/journal/SCIENCE CHINA Information Sciences/65/9/10.1007/s11432-022-3513-y,   doi =","volume":"65"},{"id":"xuProductKnowledgeGraph2020","accessed":{"date-parts":[["2024",2,5]]},"author":[{"family":"Xu","given":"Da"},{"family":"Ruan","given":"Chuanwei"},{"family":"Korpeoglu","given":"Evren"},{"family":"Kumar","given":"Sushant"},{"family":"Achan","given":"Kannan"}],"citation-key":"xuProductKnowledgeGraph2020","container-title":"Proceedings of the 13th International Conference on Web Search and Data Mining","DOI":"10.1145/3336191.3371778","event-place":"Houston TX USA","event-title":"WSDM '20: The Thirteenth ACM International Conference on Web Search and Data Mining","ISBN":"978-1-4503-6822-3","issued":{"date-parts":[["2020",1,20]]},"language":"en","page":"672-680","publisher":"ACM","publisher-place":"Houston TX USA","source":"DOI.org (Crossref)","title":"Product Knowledge Graph Embedding for E-commerce","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3336191.3371778"},{"id":"xuProductKnowledgeGraph2020a","abstract":"In this paper, we propose a new product knowledge graph (PKG) embedding approach for learning the intrinsic product relations as product knowledge for e-commerce. We define the key entities and summarize the pivotal product relations that are critical for general e-commerce applications including marketing, advertisement, search ranking and recommendation. We first provide a comprehensive comparison between PKG and ordinary knowledge graph (KG) and then illustrate why KG embedding methods are not suitable for PKG learning. We construct a self-attention-enhanced distributed representation learning model for learning PKG embeddings from raw customer activity data in an end-to-end fashion. We design an effective multi-task learning schema to fully leverage the multi-modal e-commerce data. The Poincaré embedding is also employed to handle complex entity structures. We use a real-world dataset from grocery.walmart.com to evaluate the performances on knowledge completion, search ranking and recommendation. The proposed approach compares favourably to baselines in knowledge completion and downstream tasks.","accessed":{"date-parts":[["2024",7,12]]},"author":[{"family":"Xu","given":"Da"},{"family":"Ruan","given":"Chuanwei"},{"family":"Korpeoglu","given":"Evren"},{"family":"Kumar","given":"Sushant"},{"family":"Achan","given":"Kannan"}],"citation-key":"xuProductKnowledgeGraph2020a","container-title":"Proceedings of the 13th International Conference on Web Search and Data Mining","DOI":"10.1145/3336191.3371778","issued":{"date-parts":[["2020",1,20]]},"language":"en","page":"672-680","source":"arXiv.org","title":"Product Knowledge Graph Embedding for E-commerce","type":"paper-conference","URL":"http://arxiv.org/abs/1911.12481"},{"id":"yokochiEvaluatingOxigraphServer2023","abstract":"With the escalating complexity and volume of bioinformatics data, there is an escalating demand for efficient and multifaceted triplestore technologies. Contemporary programming languages, such as Rust, provide solutions to the constraints identified in traditional languages, placing emphasis on safety, performance, and enhanced developer experience. A paradigm of this modern approach is Oxigraph, a Rust-based graph database demonstrating proficient graph data management, predominantly targeting single-node use case applications. Despite its genesis as a hobby project, Oxigraph yields competitive performance in administering straightforward Online Transaction Processing (OLTP) workloads, exhibiting a considerable potential for future refinement. This study is focused on a comprehensive appraisal of the Oxigraph server's efficacy in distinct use cases, transcending beyond the typical SPARQL performance. The evaluation thoroughly examines various operational aspects, including data loading, backup procedures, deployment strategies, maintenance protocols, and overall server usability. The authors used a subset of PDB/RDF and complete chem_comp/RDF archives; totals around 0.5 B triples have been used to conduct this evaluation.","accessed":{"date-parts":[["2024",7,10]]},"author":[{"family":"Yokochi","given":"Masashi"},{"family":"Thalhath","given":"Nishad"}],"citation-key":"yokochiEvaluatingOxigraphServer2023","DOI":"10.37044/osf.io/yru4b","issued":{"date-parts":[["2023",7,1]]},"language":"en-us","publisher":"OSF","source":"OSF Preprints","title":"Evaluating Oxigraph Server as a triple store for small and medium-sized datasets","type":"article","URL":"https://osf.io/yru4b"},{"id":"Yourselves","abstract":"Full: For 4...6 seconds, all other party members gain +100 armor. Concise: (4...6 seconds.) All other party members gain +100 armor. Faction Rewards: Luxon Scavenger (Cavalon, Guild Hall) Kurzick Bureaucrat (House zu Heltzer, Guild Hall) This skill can only be used in PvE, and is unusable by heroes. There is a duplicate of this skill for both Kurzick and Luxon Allegiance rank. They differ only in appearance, and in which Allegiance rank they use for their progression. You may equip only one of t","accessed":{"date-parts":[["2023",2,1]]},"citation-key":"Yourselves","container-title":"GuildWars Wiki","language":"en","title":"Save Yourselves!","type":"webpage","URL":"https://guildwars.fandom.com/wiki/%22Save_Yourselves!%22"},{"id":"YourselvesGuildWars","accessed":{"date-parts":[["2023",2,1]]},"citation-key":"YourselvesGuildWars","title":"\"Save Yourselves!\" - Guild Wars Wiki (GWW)","type":"webpage","URL":"https://wiki.guildwars.com/wiki/%22Save_Yourselves!%22"},{"id":"zalmoutAllYouNeed2021","accessed":{"date-parts":[["2024",2,4]]},"author":[{"family":"Zalmout","given":"Nasser"},{"family":"Zhang","given":"Chenwei"},{"family":"Li","given":"Xian"},{"family":"Liang","given":"Yan"},{"family":"Dong","given":"Xin Luna"}],"citation-key":"zalmoutAllYouNeed2021","container-title":"Product_Knowledge_Graph_Tutorial_KDD2021","issued":{"date-parts":[["2021"]]},"language":"en-US","title":"All You Need to Know to Build a Product Knowledge Graph (KDD 2021 Tutorial)","type":"webpage","URL":"https://naixlee.github.io/Product_Knowledge_Graph_Tutorial_KDD2021/"},{"id":"zhangFSIMFeatureSimilarity2011","abstract":"Image quality assessment (IQA) aims to use computational models to measure the image quality consistently with subjective evaluations. The well-known structural similarity index brings IQA from pixel- to structure-based stage. In this paper, a novel feature similarity (FSIM) index for full reference IQA is proposed based on the fact that human visual system (HVS) understands an image mainly according to its low-level features. Specifically, the phase congruency (PC), which is a dimensionless measure of the significance of a local structure, is used as the primary feature in FSIM. Considering that PC is contrast invariant while the contrast information does affect HVS' perception of image quality, the image gradient magnitude (GM) is employed as the secondary feature in FSIM. PC and GM play complementary roles in characterizing the image local quality. After obtaining the local quality map, we use PC again as a weighting function to derive a single quality score. Extensive experiments performed on six benchmark IQA databases demonstrate that FSIM can achieve much higher consistency with the subjective evaluations than state-of-the-art IQA metrics.","author":[{"family":"Zhang","given":"Lin"},{"family":"Zhang","given":"Lei"},{"family":"Mou","given":"Xuanqin"},{"family":"Zhang","given":"David"}],"citation-key":"zhangFSIMFeatureSimilarity2011","container-title":"IEEE Transactions on Image Processing","DOI":"10.1109/TIP.2011.2109730","ISSN":"1941-0042","issue":"8","issued":{"date-parts":[["2011",8]]},"page":"2378-2386","source":"IEEE Xplore","title":"FSIM: A Feature Similarity Index for Image Quality Assessment","title-short":"FSIM","type":"article-journal","volume":"20"},{"id":"zhangRadiofrequencyIdentificationRFID2018","abstract":"We investigate RFID adoption strategies in a decentralized supply chain with one manufacturer and two competing retailers both of whom face inventory misplacement problems. If a retailer adopts RFID, his misplacement problem is resolved. Retailer 1 is a Stackelberg leader in the retail market and Retailer 2 is a follower. The two retailers sequentially make decisions on whether or not to adopt RFID. After that, the manufacturer offers a wholesale price contract to a non-RFID adoption retailer or a cost-sharing contract to an RFID adoption retailer, and delivers products with(without) RFID tags to the RFID (non-RFID) adoption retailer. The two retailers then sequentially determine their retail prices to engage in price competition. We fully characterize the equilibrium on RFID adoption, contracts and retail prices. It is shown that the equilibrium RFID adoption strategies depend on the competition intensity, misplacement rates, and RFID tagging cost. We highlight the strategic role of RFID adoption in a competitive market: when the unit RFID tagging cost is intermediate, the two retailers use differentiated RFID adoption strategies such that exactly one of them adopts RFID. With more intense competition, a retailer can be more likely to adopt RFID, identifying competition as a key driving force of RFID adoption. Both retailers adopting RFID cannot be an equilibrium when the competition intensity is low. If only one retailer adopts RFID technology, he pays the manufacturer the same price for an RFID-tagged item regardless of whether or not the other retailer adopts the technology.","accessed":{"date-parts":[["2023",1,13]]},"author":[{"family":"Zhang","given":"Li-Hao"},{"family":"Li","given":"Tian"},{"family":"Fan","given":"Ti-Jun"}],"citation-key":"zhangRadiofrequencyIdentificationRFID2018","container-title":"European Journal of Operational Research","DOI":"10.1016/j.ejor.2018.04.038","ISSN":"0377-2217","issue":"3","issued":{"date-parts":[["2018",11]]},"language":"en","page":"1028–1043","title":"Radio-frequency identification (RFID) adoption with inventory misplacement under retail competition","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0377221718303588","volume":"270"},{"id":"zhangRadiofrequencyIdentificationRFID2018a","abstract":"We investigate RFID adoption strategies in a decentralized supply chain with one manufacturer and two competing retailers both of whom face inventory misplacement problems. If a retailer adopts RFID, his misplacement problem is resolved. Retailer 1 is a Stackelberg leader in the retail market and Retailer 2 is a follower. The two retailers sequentially make decisions on whether or not to adopt RFID. After that, the manufacturer offers a wholesale price contract to a non-RFID adoption retailer or a cost-sharing contract to an RFID adoption retailer, and delivers products with(without) RFID tags to the RFID (non-RFID) adoption retailer. The two retailers then sequentially determine their retail prices to engage in price competition. We fully characterize the equilibrium on RFID adoption, contracts and retail prices. It is shown that the equilibrium RFID adoption strategies depend on the competition intensity, misplacement rates, and RFID tagging cost. We highlight the strategic role of RFID adoption in a competitive market: when the unit RFID tagging cost is intermediate, the two retailers use differentiated RFID adoption strategies such that exactly one of them adopts RFID. With more intense competition, a retailer can be more likely to adopt RFID, identifying competition as a key driving force of RFID adoption. Both retailers adopting RFID cannot be an equilibrium when the competition intensity is low. If only one retailer adopts RFID technology, he pays the manufacturer the same price for an RFID-tagged item regardless of whether or not the other retailer adopts the technology.","accessed":{"date-parts":[["2023",1,13]]},"author":[{"family":"Zhang","given":"Li-Hao"},{"family":"Li","given":"Tian"},{"family":"Fan","given":"Ti-Jun"}],"citation-key":"zhangRadiofrequencyIdentificationRFID2018a","container-title":"European Journal of Operational Research","container-title-short":"European Journal of Operational Research","DOI":"10.1016/j.ejor.2018.04.038","ISSN":"0377-2217","issue":"3","issued":{"date-parts":[["2018",11,1]]},"language":"en","page":"1028-1043","source":"ScienceDirect","title":"Radio-frequency identification (RFID) adoption with inventory misplacement under retail competition","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0377221718303588","volume":"270"},{"id":"zhouOnlineShoppingAcceptance2007","abstract":"Since the late 1990s, online shopping has taken off as an increasing number of consumers purchase increasingly diversified products on the Internet. Given that how to attract and retain consumers is critical to the success of online retailers, research on the antecedents of consumer acceptance of online shopping has attracted widespread attention. There has yet to be a holistic view of online shopping acceptance from the perspective of consumers. In this research, we conducted an extensive survey of extant related studies and synthesized their findings into a reference model called OSAM (Online Shopping Acceptance Model) to explain consumer acceptance of online shopping. Our literature survey reveals that a myriad of factors have been examined in the context of online shopping and mixed results on those factors have been reported. The proposed model helps reconcile conflicting findings, discover recent trends in this line of research, and shed light on future research directions.","author":[{"family":"Zhou","given":"Lina"},{"family":"Dai","given":"Liwei"},{"family":"Zhang","given":"Dongsong"}],"citation-key":"zhouOnlineShoppingAcceptance2007","issued":{"date-parts":[["2007"]]},"language":"en","source":"Zotero","title":"Online Shopping Acceptance Model","type":"article-journal","volume":"8"},{"id":"zhouOnlineShoppingAcceptance2007a","abstract":"ABATRACT Since the late 1990s, online shopping has taken off as an increasing number of consumers purchase increasingly diversified products on the Internet. Given that how to attract and retain consumers is critical to the success of online retailers, research on the antecedents of consumer acceptance of online shopping has attracted widespread attention. There has yet to be a holistic view of online shopping acceptance from the perspective of consumers. In this research, we conducted an extensive survey of extant related studies and synthesized their findings into a reference model called OSAM (Online Shopping Acceptance Model) to explain consumer acceptance of online shopping. Our literature survey reveals that a myriad of factors have been examined in the context of online shopping and mixed results on those factors have been reported. The proposed model helps reconcile conflicting findings, discover recent trends in this line of research, and shed light on future research directions.","author":[{"family":"Zhou","given":"Lina"},{"family":"Dai","given":"Liwei"},{"family":"Zhang","given":"Dongsong"}],"citation-key":"zhouOnlineShoppingAcceptance2007a","container-title":"Journal of Electronic Commerce Research","container-title-short":"Journal of Electronic Commerce Research","issued":{"date-parts":[["2007",1,1]]},"source":"ResearchGate","title":"Online shopping acceptance model - A critical survey of consumer factors in online shopping","type":"article-journal","volume":"8"},{"id":"zhuLearnHowKnowledge","accessed":{"date-parts":[["2024",7,9]]},"author":[{"family":"Zhu","given":"Mushua"},{"family":"Liu","given":"Luxin"},{"family":"Cao","given":"Yuanpeng"},{"family":"Xu","given":"Mengtao"},{"family":"Ziyin","given":""},{"family":"Li","given":"Qiang"}],"citation-key":"zhuLearnHowKnowledge","title":"Learn How a Knowledge Graph Can Improve Your Online Shopping Experience - Alibaba Cloud Community","type":"webpage","URL":"https://www.alibabacloud.com/blog/learn-how-a-knowledge-graph-can-improve-your-online-shopping-experience_595668"},{"id":"zotero-1146","citation-key":"zotero-1146","type":"book"},{"id":"zotero-188","citation-key":"zotero-188","type":"paper-conference"},{"id":"guytUnlockingPotentialWeb2024","type":"article-journal","abstract":"Web data collected via web scraping and application programming interfaces (APIs) has opened many new avenues for retail innovations and research opportunities. Yet, despite the abundance of online data on retailers, brands, products, and consumers, its use in retailing research remains limited. To spur the increased use of web data, we aim to achieve three goals. First, we review existing retailing applications using web data. Second, we demystify the use of web data by discussing its value in the context of existing retail data sets and to-be-constructed primary web datasets. Third, we provide a hands-on guide to help retailing researchers incorporate web data collection into their research routines. Our paper is accompanied by a mock-up digital retail store (music-to-scrape.org) that researchers and students can use to learn to collect web data using web scraping and APIs.","container-title":"Journal of Retailing","DOI":"10.1016/j.jretai.2024.02.002","ISSN":"0022-4359","issue":"1","journalAbbreviation":"Journal of Retailing","page":"130-147","source":"ScienceDirect","title":"Unlocking the Potential of Web Data for Retailing Research","URL":"https://www.sciencedirect.com/science/article/pii/S0022435924000046","volume":"100","author":[{"family":"Guyt","given":"Jonne Y."},{"family":"Datta","given":"Hannes"},{"family":"Boegershausen","given":"Johannes"}],"accessed":{"date-parts":[["2025",1,16]]},"issued":{"date-parts":[["2024",3,1]]},"citation-key":"guytUnlockingPotentialWeb2024","library":"My Library","citekey":"guytUnlockingPotentialWeb2024"}]