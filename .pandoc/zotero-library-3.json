[{"id":"bezuMulticomponentSimilarityMethod2015","abstract":"Due to the growing number of Web shops, aggregating product data from the Web is growing in importance. One of the problems encountered in product aggregation is duplicate detection. In this paper, we extend and significantly improve an existing state-of-the-art product duplicate detection method. Our approach employs a novel method for combining the titles' and the attributes' similarities into a final product similarity. We use q-grams to handle partial matching of words, such as abbreviations. Where existing methods cluster products of only two Web shops, we propose a hierarchical clustering method to handle multiple Web shops. Applying our new method to a dataset of TV's from four Web shops reveals that it significantly outperforms the Hybrid Similarity Method, the Title Model Words Method, and the well-known TF-IDF method, with an F1 score of 0.475 compared to 0.287, 0.298, and 0.335, respectively.","author":[{"family":"Bezu","given":"Ronald"},{"family":"Borst","given":"Sjoerd"},{"family":"Rijkse","given":"Rick"},{"family":"Verhagen","given":"Jim"},{"family":"Vandic","given":"Damir"},{"family":"Frasincar","given":"Flavius"}],"citation-key":"bezuMulticomponentSimilarityMethod2015","DOI":"10.1145/2695664.2695818","issued":{"date-parts":[["2015",4,13]]},"page":"761-768","source":"ResearchGate","title":"Multi-component similarity method for web product duplicate detection","type":"paper-conference"},{"id":"brinkmannUsingLLMsExtraction2024","abstract":"Product offers on e-commerce websites often consist of a product title and a textual product description. In order to enable features such as faceted product search or to generate product comparison tables, it is necessary to extract structured attribute-value pairs from the unstructured product titles and descriptions and to normalize the extracted values to a single, unified scale for each attribute. This paper explores the potential of using large language models (LLMs), such as GPT-3.5 and GPT-4, to extract and normalize attribute values from product titles and descriptions. We experiment with different zero-shot and few-shot prompt templates for instructing LLMs to extract and normalize attribute-value pairs. We introduce the Web Data Commons - Product Attribute Value Extraction (WDC-PAVE) benchmark dataset for our experiments. WDC-PAVE consists of product offers from 59 different websites which provide schema.org annotations. The offers belong to five different product categories, each with a specific set of attributes. The dataset provides manually verified attribute-value pairs in two forms: (i) directly extracted values and (ii) normalized attribute values. The normalization of the attribute values requires systems to perform the following types of operations: name expansion, generalization, unit of measurement conversion, and string wrangling. Our experiments demonstrate that GPT-4 outperforms the PLM-based extraction methods SU-OpenTag, AVEQA, and MAVEQA by 10%, achieving an F1-score of 91%. For the extraction and normalization of product attribute values, GPT-4 achieves a similar performance to the extraction scenario, while being particularly strong at string wrangling and name expansion.","accessed":{"date-parts":[["2024",12,17]]},"author":[{"family":"Brinkmann","given":"Alexander"},{"family":"Baumann","given":"Nick"},{"family":"Bizer","given":"Christian"}],"citation-key":"brinkmannUsingLLMsExtraction2024","DOI":"10.1007/978-3-031-70626-4_15","issued":{"date-parts":[["2024"]]},"language":"en","page":"217-230","source":"arXiv.org","title":"Using LLMs for the Extraction and Normalization of Product Attribute Values","type":"chapter","URL":"http://arxiv.org/abs/2403.02130","volume":"14918"},{"id":"christenDataMatchingConcepts2012","accessed":{"date-parts":[["2024",10,2]]},"author":[{"family":"Christen","given":"Peter"}],"citation-key":"christenDataMatchingConcepts2012","DOI":"10.1007/978-3-642-31164-2","event-place":"Berlin, Heidelberg","ISBN":"978-3-642-31163-5 978-3-642-31164-2","issued":{"date-parts":[["2012"]]},"language":"en","license":"https://www.springernature.com/gp/researchers/text-and-data-mining","publisher":"Springer Berlin Heidelberg","publisher-place":"Berlin, Heidelberg","source":"DOI.org (Crossref)","title":"Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection","title-short":"Data Matching","type":"book","URL":"https://link.springer.com/10.1007/978-3-642-31164-2"},{"id":"devlinBERTPretrainingDeep2019","abstract":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"Devlin","given":"Jacob"},{"family":"Chang","given":"Ming-Wei"},{"family":"Lee","given":"Kenton"},{"family":"Toutanova","given":"Kristina"}],"citation-key":"devlinBERTPretrainingDeep2019","issued":{"date-parts":[["2019",5,24]]},"number":"arXiv:1810.04805","publisher":"arXiv","source":"arXiv.org","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","title-short":"BERT","type":"article","URL":"http://arxiv.org/abs/1810.04805"},{"id":"devlinBERTPretrainingDeep2019a","abstract":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","accessed":{"date-parts":[["2024",9,9]]},"author":[{"family":"Devlin","given":"Jacob"},{"family":"Chang","given":"Ming-Wei"},{"family":"Lee","given":"Kenton"},{"family":"Toutanova","given":"Kristina"}],"citation-key":"devlinBERTPretrainingDeep2019a","container-title":"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","DOI":"10.18653/v1/N19-1423","editor":[{"family":"Burstein","given":"Jill"},{"family":"Doran","given":"Christy"},{"family":"Solorio","given":"Thamar"}],"event-place":"Minneapolis, Minnesota","event-title":"NAACL-HLT 2019","issued":{"date-parts":[["2019",6]]},"page":"4171–4186","publisher":"Association for Computational Linguistics","publisher-place":"Minneapolis, Minnesota","source":"ACLWeb","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","title-short":"BERT","type":"paper-conference","URL":"https://aclanthology.org/N19-1423"},{"id":"dooleyFoodOnHarmonizedFood2018","abstract":"The construction of high capacity data sharing networks to support increasing government and commercial data exchange has highlighted a key roadblock: the content of existing Internet-connected information remains siloed due to a multiplicity of local languages and data dictionaries. This lack of a digital lingua franca is obvious in the domain of human food as materials travel from their wild or farm origin, through processing and distribution chains, to consumers. Well defined, hierarchical vocabulary, connected with logical relationships—in other words, an ontology—is urgently needed to help tackle data harmonization problems that span the domains of food security, safety, quality, production, distribution, and consumer health and convenience. FoodOn (http://foodon.org) is a consortium-driven project to build a comprehensive and easily accessible global farm-to-fork ontology about food, that accurately and consistently describes foods commonly known in cultures from around the world. FoodOn addresses food product terminology gaps and supports food traceability. Focusing on human and domesticated animal food description, FoodOn contains animal and plant food sources, food categories and products, and other facets like preservation processes, contact surfaces, and packaging. Much of FoodOn’s vocabulary comes from transforming LanguaL, a mature and popular food indexing thesaurus, into a World Wide Web Consortium (W3C) OWL Web Ontology Language-formatted vocabulary that provides system interoperability, quality control, and software-driven intelligence. FoodOn compliments other technologies facilitating food traceability, which is becoming critical in this age of increasing globalization of food networks.","author":[{"family":"Dooley","given":"Damion M."},{"family":"Griffiths","given":"Emma J."},{"family":"Gosal","given":"Gurinder S."},{"family":"Buttigieg","given":"Pier L."},{"family":"Hoehndorf","given":"Robert"},{"family":"Lange","given":"Matthew C."},{"family":"Schriml","given":"Lynn M."},{"family":"Brinkman","given":"Fiona S. L."},{"family":"Hsiao","given":"William W. L."}],"citation-key":"dooleyFoodOnHarmonizedFood2018","container-title":"npj Science of Food","container-title-short":"npj Science of Food","DOI":"10.1038/s41538-018-0032-6","ISSN":"2396-8370","issue":"1","issued":{"date-parts":[["2018",12,18]]},"page":"23","title":"FoodOn: a harmonized food ontology to increase global food traceability, quality control and data integration","type":"article-journal","URL":"https://doi.org/10.1038/s41538-018-0032-6","volume":"2"},{"id":"droleNutriGreenImageDataset2024","abstract":"<sec><title>Introduction</title><p>In this research, we introduce the NutriGreen dataset, which is a collection of images representing branded food products aimed for training segmentation models for detecting various labels on food packaging. Each image in the dataset comes with three distinct labels: one indicating its nutritional quality using the Nutri-Score, another denoting whether it is vegan or vegetarian origin with the V-label, and a third displaying the EU organic certification (BIO) logo.</p></sec><sec><title>Methods</title><p>To create the dataset, we have used semi-automatic annotation pipeline that combines domain expert annotation and automatic annotation using a deep learning model.</p></sec><sec><title>Results</title><p>The dataset comprises a total of 10,472 images. Among these, the Nutri-Score label is distributed across five sub-labels: Nutri-Score grade A with 1,250 images, grade B with 1,107 images, grade C with 867 images, grade D with 1,001 images, and grade E with 967 images. Additionally, there are 870 images featuring the V-Label, 2,328 images showcasing the BIO label, and 3,201 images without before-mentioned labels. Furthermore, we have fine-tuned the YOLOv5 segmentation model to demonstrate the practicality of using these annotated datasets, achieving an impressive accuracy of 94.0%.</p></sec><sec><title>Discussion</title><p>These promising results indicate that this dataset has significant potential for training innovative systems capable of detecting food labels. Moreover, it can serve as a valuable benchmark dataset for emerging computer vision systems.</p></sec>","accessed":{"date-parts":[["2025",1,6]]},"author":[{"family":"Drole","given":"Jan"},{"family":"Pravst","given":"Igor"},{"family":"Eftimov","given":"Tome"},{"family":"Koroušić Seljak","given":"Barbara"}],"citation-key":"droleNutriGreenImageDataset2024","container-title":"Frontiers in Nutrition","container-title-short":"Front. Nutr.","DOI":"10.3389/fnut.2024.1342823","ISSN":"2296-861X","issued":{"date-parts":[["2024",3,26]]},"language":"English","publisher":"Frontiers","source":"Frontiers","title":"NutriGreen image dataset: a collection of annotated nutrition, organic, and vegan food products","title-short":"NutriGreen image dataset","type":"article-journal","URL":"https://www.frontiersin.org/journals/nutrition/articles/10.3389/fnut.2024.1342823/full","volume":"11"},{"id":"fangLLMEnsembleOptimalLarge2024","abstract":"Product attribute value extraction is a pivotal component in Natural Language Processing (NLP) and the contemporary e-commerce industry. The provision of precise product attribute values is fundamental in ensuring high-quality recommendations and enhancing customer satisfaction. The recently emerging Large Language Models (LLMs) have demonstrated state-of-the-art performance in numerous attribute extraction tasks, without the need for domain-specific training data. Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters. This variation makes them complementary to each other, with no single LLM dominating all others. Considering the diverse strengths and weaknesses of LLMs, it becomes necessary to develop an ensemble method that leverages their complementary potentials. In this paper, we propose a novel algorithm called LLM-ensemble to ensemble different LLMs' outputs for attribute value extraction. We iteratively learn the weights for different LLMs to aggregate the labels with weights to predict the final attribute value. Not only can our proposed method be proven theoretically optimal, but it also ensures efficient computation, fast convergence, and safe deployment. We have also conducted extensive experiments with various state-of-the-art LLMs, including Llama2-13B, Llama2-70B, PaLM-2, GPT-3.5, and GPT-4, on Walmart's internal data. Our offline metrics demonstrate that the LLM-ensemble method outperforms all the state-of-the-art single LLMs on Walmart's internal dataset. This method has been launched in several production models, leading to improved Gross Merchandise Volume (GMV), Click-Through Rate (CTR), Conversion Rate (CVR), and Add-to-Cart Rate (ATC).","accessed":{"date-parts":[["2024",12,17]]},"author":[{"family":"Fang","given":"Chenhao"},{"family":"Li","given":"Xiaohan"},{"family":"Fan","given":"Zezhong"},{"family":"Xu","given":"Jianpeng"},{"family":"Nag","given":"Kaushiki"},{"family":"Korpeoglu","given":"Evren"},{"family":"Kumar","given":"Sushant"},{"family":"Achan","given":"Kannan"}],"citation-key":"fangLLMEnsembleOptimalLarge2024","DOI":"10.48550/ARXIV.2403.00863","issued":{"date-parts":[["2024"]]},"license":"arXiv.org perpetual, non-exclusive license","publisher":"arXiv","source":"Semantic Scholar","title":"LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction","title-short":"LLM-Ensemble","type":"article-journal","URL":"https://arxiv.org/abs/2403.00863","version":"2"},{"id":"fredrikssonDataLabelingEmpirical2020","abstract":"Labeling is a cornerstone of supervised machine learning. However, in industrial applications, data is often not labeled, which complicates using this data for machine learning. Although there are well-established labeling techniques such as crowdsourcing, active learning, and semi-supervised learning, these still do not provide accurate and reliable labels for every machine learning use case in the industry. In this context, the industry still relies heavily on manually annotating and labeling their data. This study investigates the challenges that companies experience when annotating and labeling their data. We performed a case study using a semi-structured interview with data scientists at two companies to explore their problems when labeling and annotating their data. This paper provides two contributions. We identify industry challenges in the labeling process, and then we propose mitigation strategies for these challenges.","author":[{"family":"Fredriksson","given":"Teodor"},{"family":"Mattos","given":"David Issa"},{"family":"Bosch","given":"Jan"},{"family":"Olsson","given":"Helena Holmström"}],"citation-key":"fredrikssonDataLabelingEmpirical2020","collection-title":"Lecture Notes in Computer Science","container-title":"Product-Focused Software Process Improvement","DOI":"10.1007/978-3-030-64148-1_13","editor":[{"family":"Morisio","given":"Maurizio"},{"family":"Torchiano","given":"Marco"},{"family":"Jedlitschka","given":"Andreas"}],"event-place":"Cham","ISBN":"978-3-030-64148-1","issued":{"date-parts":[["2020"]]},"language":"en","page":"202-216","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"Data Labeling: An Empirical Investigation into Industrial Challenges and Mitigation Strategies","title-short":"Data Labeling","type":"paper-conference"},{"id":"GPCProduktklassifikationGlobal","accessed":{"date-parts":[["2024",10,15]]},"citation-key":"GPCProduktklassifikationGlobal","title":"GPC Produktklassifikation: global eindeutig | GS1 Germany","type":"webpage","URL":"https://www.gs1-germany.de/standards/produktklassifikation-gpc/"},{"id":"gs1aisblGlobalDataModel","abstract":"The Global Data Model defines a consistent set of product attributes to harmonise and simplify data exchange between trading partners. Find out more about it here.","accessed":{"date-parts":[["2024",9,16]]},"author":[{"family":"GS1 AISBL","given":""}],"citation-key":"gs1aisblGlobalDataModel","language":"en","title":"Global Data Model","type":"webpage","URL":"https://www.gs1.org/standards/gs1-global-data-model"},{"id":"gs1aisblWebVocabulary","accessed":{"date-parts":[["2024",9,16]]},"author":[{"family":"GS1 AISBL","given":""}],"citation-key":"gs1aisblWebVocabulary","title":"Web Vocabulary","type":"webpage","URL":"https://ref.gs1.org/voc/"},{"id":"gs1germanygmbhHomepage2024","abstract":"The Global Language of Business","accessed":{"date-parts":[["2024",6,15]]},"author":[{"family":"GS1 Germany GmbH","given":""}],"citation-key":"gs1germanygmbhHomepage2024","issued":{"date-parts":[["2024",4,23]]},"language":"en","title":"Homepage","type":"webpage","URL":"https://www.gs1.org/"},{"id":"gurUnderstandingHTMLLarge2023","abstract":"Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -- i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniWoB benchmark, LLMs successfully complete 50% more tasks using 192x less data compared to the previous best supervised model. Out of the LLMs we evaluate, we show evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture. To promote further research on LLMs for HTML understanding, we create and open-source a large-scale HTML dataset distilled and auto-labeled from CommonCrawl.","accessed":{"date-parts":[["2024",12,17]]},"author":[{"family":"Gur","given":"Izzeddin"},{"family":"Nachum","given":"Ofir"},{"family":"Miao","given":"Yingjie"},{"family":"Safdari","given":"Mustafa"},{"family":"Huang","given":"Austin"},{"family":"Chowdhery","given":"Aakanksha"},{"family":"Narang","given":"Sharan"},{"family":"Fiedel","given":"Noah"},{"family":"Faust","given":"Aleksandra"}],"citation-key":"gurUnderstandingHTMLLarge2023","DOI":"10.48550/arXiv.2210.03945","issued":{"date-parts":[["2023",5,19]]},"language":"en","number":"arXiv:2210.03945","publisher":"arXiv","source":"arXiv.org","title":"Understanding HTML with Large Language Models","type":"article","URL":"http://arxiv.org/abs/2210.03945"},{"id":"HowGPCWorks","accessed":{"date-parts":[["2024",10,15]]},"citation-key":"HowGPCWorks","title":"How GPC works - Standards | GS1","type":"webpage","URL":"https://www.gs1.org/standards/gpc/how-gpc-works"},{"id":"HowLabelData2019","abstract":"The main challenge for a data science team is to decide who will be responsible for labeling, how much time it will take, and what tools are better to use.","accessed":{"date-parts":[["2023",11,9]]},"citation-key":"HowLabelData2019","container-title":"AltexSoft","issued":{"date-parts":[["2019",7,16]]},"language":"en","title":"How to Label Data for Machine Learning: Process and Tools","title-short":"How to Label Data for Machine Learning","type":"webpage","URL":"https://www.altexsoft.com/blog/how-to-organize-data-labeling-for-machine-learning-approaches-and-tools/"},{"id":"huangAutoScraperProgressiveUnderstanding2024","abstract":"Web scraping is a powerful technique that extracts data from websites, enabling automated data collection, enhancing data analysis capabilities, and minimizing manual data entry efforts. Existing methods, wrappers-based methods suffer from limited adaptability and scalability when faced with a new website, while language agents, empowered by large language models (LLMs), exhibit poor reusability in diverse web environments. In this work, we introduce the paradigm of generating web scrapers with LLMs and propose AutoScraper, a two-stage framework that can handle diverse and changing web environments more efficiently. AutoScraper leverages the hierarchical structure of HTML and similarity across different web pages for generating web scrapers. Besides, we propose a new executability metric for better measuring the performance of web scraper generation tasks. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework. Resources of this paper can be found at \\url{https://github.com/EZ-hwh/AutoScraper}","accessed":{"date-parts":[["2024",12,17]]},"author":[{"family":"Huang","given":"Wenhao"},{"family":"Gu","given":"Zhouhong"},{"family":"Peng","given":"Chenghao"},{"family":"Li","given":"Zhixu"},{"family":"Liang","given":"Jiaqing"},{"family":"Xiao","given":"Yanghua"},{"family":"Wen","given":"Liqian"},{"family":"Chen","given":"Zulong"}],"citation-key":"huangAutoScraperProgressiveUnderstanding2024","DOI":"10.48550/arXiv.2404.12753","issued":{"date-parts":[["2024",9,26]]},"number":"arXiv:2404.12753","publisher":"arXiv","source":"arXiv.org","title":"AutoScraper: A Progressive Understanding Web Agent for Web Scraper Generation","title-short":"AutoScraper","type":"article","URL":"http://arxiv.org/abs/2404.12753"},{"id":"huggingfaceDbmdzBertbasegermanuncasedHugging","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"Huggingface","given":""}],"citation-key":"huggingfaceDbmdzBertbasegermanuncasedHugging","title":"dbmdz/bert-base-german-uncased · Hugging Face","type":"webpage","URL":"https://huggingface.co/dbmdz/bert-base-german-uncased"},{"id":"huggingfaceSentencetransformersDistilusebasemultilingualcasedv2Hugging","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"Huggingface","given":""}],"citation-key":"huggingfaceSentencetransformersDistilusebasemultilingualcasedv2Hugging","title":"sentence-transformers/distiluse-base-multilingual-cased-v2 · Hugging Face","type":"webpage","URL":"https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2"},{"id":"huLoRALowRankAdaptation2021","abstract":"An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.","accessed":{"date-parts":[["2023",4,19]]},"author":[{"family":"Hu","given":"Edward J."},{"family":"Shen","given":"Yelong"},{"family":"Wallis","given":"Phillip"},{"family":"Allen-Zhu","given":"Zeyuan"},{"family":"Li","given":"Yuanzhi"},{"family":"Wang","given":"Shean"},{"family":"Wang","given":"Lu"},{"family":"Chen","given":"Weizhu"}],"citation-key":"huLoRALowRankAdaptation2021","DOI":"10.48550/arXiv.2106.09685","issued":{"date-parts":[["2021",10,16]]},"number":"arXiv:2106.09685","publisher":"arXiv","source":"arXiv.org","title":"LoRA: Low-Rank Adaptation of Large Language Models","title-short":"LoRA","type":"article","URL":"http://arxiv.org/abs/2106.09685"},{"id":"ilinaSurveySymmetricalNeural2022","abstract":"A number of modern techniques for neural network training and recognition enhancement are based on their structures’ symmetry. Such approaches demonstrate impressive results, both for recognition practice, and for understanding of data transformation processes in various feature spaces. This survey examines symmetrical neural network architectures—Siamese and triplet. Among a wide range of tasks having various mathematical formulation areas, especially effective applications of symmetrical neural network architectures are revealed. We systematize and compare different architectures of symmetrical neural networks, identify genetic relationships between significant studies of different authors’ groups, and discuss opportunities to improve the element base of such neural networks. Our survey builds bridges between a large number of isolated studies with significant practical results in the considered area of knowledge, so that the presented survey acquires additional relevance.","accessed":{"date-parts":[["2024",10,1]]},"author":[{"family":"Ilina","given":"Olga"},{"family":"Ziyadinov","given":"Vadim"},{"family":"Klenov","given":"Nikolay"},{"family":"Tereshonok","given":"Maxim"}],"citation-key":"ilinaSurveySymmetricalNeural2022","container-title":"Symmetry","container-title-short":"Symmetry","DOI":"10.3390/sym14071391","ISSN":"2073-8994","issue":"7","issued":{"date-parts":[["2022",7,6]]},"language":"en","license":"https://creativecommons.org/licenses/by/4.0/","page":"1391","source":"DOI.org (Crossref)","title":"A Survey on Symmetrical Neural Network Architectures and Applications","type":"article-journal","URL":"https://www.mdpi.com/2073-8994/14/7/1391","volume":"14"},{"id":"krosnickPromisesPitfallsUsing2023","abstract":"ChatGPT and other publicly available large language models (LLMs) put AI into the hands of everyday computer users, offering the possibility of automating computer tasks. One candidate task is web scraping. We informally experimented with ChatGPT to explore the potential promises and pitfalls of using it for scraping data from web user interfaces. We share our observations and considerations for future human-LLM web scraping systems.","author":[{"family":"Krosnick","given":"Rebecca"},{"family":"Oney","given":"Steve"}],"citation-key":"krosnickPromisesPitfallsUsing2023","issued":{"date-parts":[["2023"]]},"language":"en","source":"Zotero","title":"Promises and Pitfalls of Using LLMs for Scraping Web UIs","type":"article-journal"},{"id":"meuselExploitingMicrodataAnnotations2015","abstract":"Semantically annotated data, using markup languages like RDFa and Microdata, has become more and more publicly available in the Web, especially in the area of e-commerce. Thus, a large amount of structured product descriptions are freely available and can be used for various applications, such as product search or recommendation. However, little eﬀorts have been made to analyze the categories of the available product descriptions. Although some products have an explicit category assigned, the categorization schemes vary a lot, as the products originate from thousands of diﬀerent sites. This heterogeneity makes the use of supervised methods, which have been proposed by most previous works, hard to apply. Therefore, in this paper, we explain how distantly supervised approaches can be used to exploit the heterogeneous category information in order to map the products to set of target categories from an existing product catalogue. Our results show that, even though this task is by far not trivial, we can reach almost 56% accuracy for classifying products into 37 categories.","accessed":{"date-parts":[["2023",10,30]]},"author":[{"family":"Meusel","given":"Robert"},{"family":"Primpeli","given":"Anna"},{"family":"Meilicke","given":"Christian"},{"family":"Paulheim","given":"Heiko"},{"family":"Bizer","given":"Christian"}],"citation-key":"meuselExploitingMicrodataAnnotations2015","container-title":"E-Commerce and Web Technologies","DOI":"10.1007/978-3-319-27729-5_7","editor":[{"family":"Stuckenschmidt","given":"Heiner"},{"family":"Jannach","given":"Dietmar"}],"event-place":"Cham","ISBN":"978-3-319-27728-8 978-3-319-27729-5","issued":{"date-parts":[["2015"]]},"language":"en","page":"83-99","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Exploiting Microdata Annotations to Consistently Categorize Product Offers at Web Scale","type":"chapter","URL":"http://link.springer.com/10.1007/978-3-319-27729-5_7","volume":"239"},{"id":"meuselExploitingMicrodataAnnotations2015a","abstract":"Semantically annotated data, using markup languages like RDFa and Microdata, has become more and more publicly available in the Web, especially in the area of e-commerce. Thus, a large amount of structured product descriptions are freely available and can be used for various applications, such as product search or recommendation. However, little efforts have been made to analyze the categories of the available product descriptions. Although some products have an explicit category assigned, the categorization schemes vary a lot, as the products originate from thousands of different sites. This heterogeneity makes the use of supervised methods, which have been proposed by most previous works, hard to apply. Therefore, in this paper, we explain how distantly supervised approaches can be used to exploit the heterogeneous category information in order to map the products to set of target categories from an existing product catalogue. Our results show that, even though this task is by far not trivial, we can reach almost $$56\\,\\%$$accuracy for classifying products into 37 categories.","author":[{"family":"Meusel","given":"Robert"},{"family":"Primpeli","given":"Anna"},{"family":"Meilicke","given":"Christian"},{"family":"Paulheim","given":"Heiko"},{"family":"Bizer","given":"Christian"}],"citation-key":"meuselExploitingMicrodataAnnotations2015a","collection-title":"Lecture Notes in Business Information Processing","container-title":"E-Commerce and Web Technologies","DOI":"10.1007/978-3-319-27729-5_7","editor":[{"family":"Stuckenschmidt","given":"Heiner"},{"family":"Jannach","given":"Dietmar"}],"event-place":"Cham","ISBN":"978-3-319-27729-5","issued":{"date-parts":[["2015"]]},"language":"en","page":"83-99","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"Exploiting Microdata Annotations to Consistently Categorize Product Offers at Web Scale","type":"paper-conference"},{"id":"MultimodalEntityResolution","accessed":{"date-parts":[["2024",9,9]]},"citation-key":"MultimodalEntityResolution","title":"Towards Multi-modal Entity Resolution for Product Matching | Database Group Leipzig","type":"webpage","URL":"https://dbs.uni-leipzig.de/research/publications/towards-multi-modal-entity-resolution-for-product-matching"},{"id":"NaturalLanguageProcessing","abstract":"Access the most inclusive natural language processing (NLP) coverage. We cover NLP from zero to common uses, techniques, and open-source libraries.","accessed":{"date-parts":[["2023",11,13]]},"citation-key":"NaturalLanguageProcessing","language":"en","title":"Natural language processing (NLP): Techniques and use cases | SuperAnnotate","title-short":"Natural language processing (NLP)","type":"webpage","URL":"https://www.superannotate.com/blog/what-is-natural-language-processing"},{"id":"niemirBasicProductData2021","abstract":"Purpose: This paper summarizes and compares the types and interpretations of the basic attributes necessary to enter product data in selected e-commerce platforms. Design/Methodology/Approach: The research methodology was based on an analysis and selection of a reference group of basic product attributes and identification of appropriate market representatives, platforms and tools commonly used in e-commerce. Furthermore, for each of the selected basic attributes of the product, an analysis was made in terms of the presence, mandatory field, and data input validators. The best practices indicated by the platform developers were also reviewed.","accessed":{"date-parts":[["2023",10,30]]},"author":[{"family":"Niemir","given":"Maciej"},{"family":"Mrugalska","given":"Beata"}],"citation-key":"niemirBasicProductData2021","container-title":"EUROPEAN RESEARCH STUDIES JOURNAL","container-title-short":"ERSJ","DOI":"10.35808/ersj/2735","ISSN":"1108-2976","issue":"Special Issue 5","issued":{"date-parts":[["2021",12,1]]},"language":"en","page":"317-329","source":"DOI.org (Crossref)","title":"Basic Product Data in E-Commerce: Specifications and Problems of Data Exchange","title-short":"Basic Product Data in E-Commerce","type":"article-journal","URL":"http://ersj.eu/journal/2735","volume":"XXIV"},{"id":"openaiEmbeddingsOpenAIAPI","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"Openai","given":""}],"citation-key":"openaiEmbeddingsOpenAIAPI","container-title":"Embeddings","title":"Embeddings - OpenAI API","type":"webpage","URL":"https://platform.openai.com/docs/guides/embeddings/what-are-embeddings"},{"id":"parmarReviewRandomForest2019","accessed":{"date-parts":[["2024",10,1]]},"author":[{"family":"Parmar","given":"Aakash"},{"family":"Katariya","given":"Rakesh"},{"family":"Patel","given":"Vatsal"}],"citation-key":"parmarReviewRandomForest2019","container-title":"International Conference on Intelligent Data Communication Technologies and Internet of Things (ICICI) 2018","DOI":"10.1007/978-3-030-03146-6_86","editor":[{"family":"Hemanth","given":"Jude"},{"family":"Fernando","given":"Xavier"},{"family":"Lafata","given":"Pavel"},{"family":"Baig","given":"Zubair"}],"event-place":"Cham","ISBN":"978-3-030-03145-9 978-3-030-03146-6","issued":{"date-parts":[["2019"]]},"page":"758-763","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"A Review on Random Forest: An Ensemble Classifier","title-short":"A Review on Random Forest","type":"chapter","URL":"http://link.springer.com/10.1007/978-3-030-03146-6_86","volume":"26"},{"id":"peetersEntityMatchingUsing2024","abstract":"Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity and is a central step in most data integration pipelines. Many state-of-the-art entity matching methods rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. This paper investigates using generative large language models (LLMs) as a less task-specific training data-dependent and more robust alternative to PLM-based matchers. Our study covers hosted and open-source LLMs, which can be run locally. We evaluate these models in a zero-shot scenario and a scenario where task-specific training data is available. We compare different prompt designs and the prompt sensitivity of the models and show that there is no single best prompt but needs to be tuned for each model/dataset combination. We further investigate (i) the selection of in-context demonstrations, (ii) the generation of matching rules, as well as (iii) fine-tuning a hosted LLM using the same pool of training data. Our experiments show that the best LLMs require no or only a few training examples to perform similarly to PLMs that were fine-tuned using thousands of examples. LLM-based matchers further exhibit higher robustness to unseen entities. We show that GPT4 can generate structured explanations for matching decisions. The model can automatically identify potential causes of matching errors by analyzing explanations of wrong decisions. We demonstrate that the model can generate meaningful textual descriptions of the identified error classes, which can help data engineers improve entity matching pipelines.","accessed":{"date-parts":[["2024",9,9]]},"author":[{"family":"Peeters","given":"Ralph"},{"family":"Bizer","given":"Christian"}],"citation-key":"peetersEntityMatchingUsing2024","DOI":"10.48550/arXiv.2310.11244","issued":{"date-parts":[["2024",6,5]]},"number":"arXiv:2310.11244","publisher":"arXiv","source":"arXiv.org","title":"Entity Matching using Large Language Models","type":"article","URL":"http://arxiv.org/abs/2310.11244"},{"id":"peetersIntermediateTrainingBERT2020","abstract":"Transformer-based models like BERT have pushed the state-of the-art for a wide range of tasks in natural language processing. \nGeneral-purpose pre-training on large corpora allows Transformers to yield good performance even with small amounts of training data for task-specific fine-tuning. In this work, we apply BERT to the task of product matching in e-commerce and show that BERT is much more training data efficient than other state-of-the-art methods. Moreover, we show that we can further boost its effectiveness through an intermediate training step, exploiting large collections of product offers. Our intermediate training leads to strong performance (>90% F1) on new, unseen products without any product-specific fine-tuning. Further fine-tuning yields additional gains, resulting in improvements of up to 12% F1 for small training sets. Adding the masked language modeling objective in the intermediate training step in order to further adapt the language model to the application domain leads to an additional increase of up to 3% F1.","accessed":{"date-parts":[["2024",9,9]]},"author":[{"family":"Peeters","given":"R."},{"family":"Bizer","given":"Christian"},{"family":"Glavas","given":"Goran"}],"citation-key":"peetersIntermediateTrainingBERT2020","event-title":"DI2KG@VLDB","issued":{"date-parts":[["2020"]]},"source":"Semantic Scholar","title":"Intermediate Training of BERT for Product Matching","type":"paper-conference","URL":"https://www.semanticscholar.org/paper/Intermediate-Training-of-BERT-for-Product-Matching-Peeters-Bizer/6e177525526925dd30a860bbfc1bbad0c5d42421"},{"id":"peetersIntermediateTrainingBERT2020a","abstract":"Transformer-based models like BERT have pushed the state-of the-art for a wide range of tasks in natural language processing. General-purpose pre-training on large corpora allows Transformers to yield good performance even with small amounts of training data for task-specific fine-tuning. In this work, we apply BERT to the task of product matching in e-commerce and show that BERT is much more training data efficient than other state-of-the-art methods. Moreover, we show that we can further boost its effectiveness through an intermediate training step, exploiting large collections of product offers. Our intermediate training leads to strong performance (>90% F1) on new, unseen products without any product-specific fine-tuning. Further fine-tuning yields additional gains, resulting in improvements of up to 12% F1 for small training sets. Adding the masked language modeling objective in the intermediate training step in order to further adapt the language model to the application domain leads to an additional increase of up to 3% F1.","author":[{"family":"Peeters","given":"Ralph"},{"family":"Bizer","given":"Christian"},{"family":"Glavaš","given":"Goran"}],"citation-key":"peetersIntermediateTrainingBERT2020a","issued":{"date-parts":[["2020"]]},"language":"en","source":"Zotero","title":"Intermediate Training of BERT for Product Matching","type":"article-journal"},{"id":"primpeliWDCTrainingDataset2019","abstract":"A current research question in the area of entity resolution (also called link discovery or duplicate detection) is whether and in which cases embeddings and deep neural network based matching methods outperform traditional symbolic matching methods. The problem with answering this question is that deep learning based matchers need large amounts of training data. The entity resolution benchmark datasets that are currently available to the public are too small to properly evaluate this new family of matching methods. The WDC Training Dataset for Large-Scale Product Matching fills this gap. The English language subset of the training dataset consists of 20 million pairs of offers referring to the same products. The offers were extracted from 43 thousand e-shops which provide schema.org annotations including some form of product ID such as a GTIN or MPN. We also created a gold standard by manually verifying 2200 pairs of offers belonging to four product categories. Using a subset of our training dataset together with this gold standard, we are able to publicly replicate the recent result of Mudgal et al. that embeddings and deep neural network based matching methods outperform traditional symbolic matching methods on less structured data.","accessed":{"date-parts":[["2024",9,9]]},"author":[{"family":"Primpeli","given":"Anna"},{"family":"Peeters","given":"Ralph"},{"family":"Bizer","given":"Christian"}],"citation-key":"primpeliWDCTrainingDataset2019","container-title":"Companion Proceedings of The 2019 World Wide Web Conference","DOI":"10.1145/3308560.3316609","event-place":"San Francisco USA","event-title":"WWW '19: The Web Conference","ISBN":"978-1-4503-6675-5","issued":{"date-parts":[["2019",5,13]]},"language":"en","page":"381-386","publisher":"ACM","publisher-place":"San Francisco USA","source":"DOI.org (Crossref)","title":"The WDC Training Dataset and Gold Standard for Large-Scale Product Matching","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3308560.3316609"},{"id":"pytorchResnet50TorchvisionMain","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"PyTorch","given":""}],"citation-key":"pytorchResnet50TorchvisionMain","title":"resnet50 — Torchvision main documentation","type":"webpage","URL":"https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html"},{"id":"reimersSentenceBERTSentenceEmbeddings2019","abstract":"BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"Reimers","given":"Nils"},{"family":"Gurevych","given":"Iryna"}],"citation-key":"reimersSentenceBERTSentenceEmbeddings2019","issued":{"date-parts":[["2019",8,27]]},"number":"arXiv:1908.10084","publisher":"arXiv","source":"arXiv.org","title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks","title-short":"Sentence-BERT","type":"article","URL":"http://arxiv.org/abs/1908.10084"},{"id":"schema.orgHomepage","accessed":{"date-parts":[["2024",6,6]]},"author":[{"family":"Schema.org","given":""}],"citation-key":"schema.orgHomepage","title":"Homepage","type":"webpage","URL":"https://schema.org/"},{"id":"schultheisEASYEnergyEfficientAnalysis2024","abstract":"According to the guiding principles of Industry 4.0, edge computing enables the data-sovereign and near-real-time processing of data directly at the point of origin. Using these edge devices in manufacturing organization will drive the use of industrial analysis, control, and Artificial Intelligence (AI) applications close to production. The goal of the EASY project is to make the added value of edge computing available by providing an easily usable Edge-Cloud Continuum with a runtime environment and services for the execution of AI-based Analysis and Control processes. Within this continuum, a dynamic, distributed, and optimized execution of services is automated across the entire spectrum from centralized cloud to decentralized edge instances to increase productivity and resource efficiency.","accessed":{"date-parts":[["2024",9,9]]},"author":[{"family":"Schultheis","given":"Alexander"},{"family":"Alt","given":"Benjamin"},{"family":"Bast","given":"Sebastian"},{"family":"Guldner","given":"Achim"},{"family":"Jilg","given":"David"},{"family":"Katic","given":"Darko"},{"family":"Mundorf","given":"Johannes"},{"family":"Schlagenhauf","given":"Tobias"},{"family":"Weber","given":"Sebastian"},{"family":"Bergmann","given":"Ralph"},{"family":"Bergweiler","given":"Simon"},{"family":"Creutz","given":"Lars"},{"family":"Dartmann","given":"Guido"},{"family":"Malburg","given":"Lukas"},{"family":"Naumann","given":"Stefan"},{"family":"Rezapour","given":"Mahdi"},{"family":"Ruskowski","given":"Martin"}],"citation-key":"schultheisEASYEnergyEfficientAnalysis2024","container-title":"KI - Künstliche Intelligenz","container-title-short":"Künstl Intell","DOI":"10.1007/s13218-024-00868-3","ISSN":"1610-1987","issued":{"date-parts":[["2024",9,3]]},"language":"en","source":"Springer Link","title":"EASY: Energy-Efficient Analysis and Control Processes in the Dynamic Edge-Cloud Continuum for Industrial Manufacturing","title-short":"EASY","type":"article-journal","URL":"https://doi.org/10.1007/s13218-024-00868-3"},{"id":"shahNeuralNetworkBased2018","abstract":"Matching a seller listed item to an appropriate product has become a fundamental and one of the most significant step for e-commerce platforms for product based experience. It has a huge impact on making the search effective, search engine optimization, providing product reviews and product price estimation etc. along with many other advantages for a better user experience. As significant and vital it has become, the challenge to tackle the complexity has become huge with the exponential growth of individual and business sellers trading millions of products everyday. We explored two approaches; classification based on shallow neural network and similarity based on deep siamese network. These models outperform the baseline by more than 5% in term of accuracy and are capable of extremely efficient training and inference.","accessed":{"date-parts":[["2024",9,9]]},"author":[{"family":"Shah","given":"Kashif"},{"family":"Kopru","given":"Selcuk"},{"family":"Ruvini","given":"Jean-David"}],"citation-key":"shahNeuralNetworkBased2018","container-title":"Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)","DOI":"10.18653/v1/N18-3002","editor":[{"family":"Bangalore","given":"Srinivas"},{"family":"Chu-Carroll","given":"Jennifer"},{"family":"Li","given":"Yunyao"}],"event-place":"New Orleans - Louisiana","event-title":"NAACL-HLT 2018","issued":{"date-parts":[["2018",6]]},"page":"8–15","publisher":"Association for Computational Linguistics","publisher-place":"New Orleans - Louisiana","source":"ACLWeb","title":"Neural Network based Extreme Classification and Similarity Models for Product Matching","type":"paper-conference","URL":"https://aclanthology.org/N18-3002"},{"id":"singerBeverageGraphConnecting","abstract":"We describe the design and ongoing update of a knowledge graph and its assorted ontologies which describe beverages and their commercial availability as products. Previous approaches have focused on beverage types or brands with limited support for tracing the product’s content or identifying the specific product being consumed by a person. This inability to link the product and source has until now been a hindrance to nutritional studies and food traceability systems.","author":[{"family":"Singer","given":"Jessica"},{"family":"Warren","given":"Robert"}],"citation-key":"singerBeverageGraphConnecting","language":"en","source":"Zotero","title":"Beverage Graph: Connecting Data about Consumable Liquids","type":"article-journal"},{"id":"sunLabelandLearnVisualizingLikelihood2017","abstract":"While machine learning is a powerful tool for the analysis and classification of complex real-world datasets, it is still challenging, particularly for developers with limited expertise, to incorporate this technology into their software systems. The first step in machine learning, data labeling, is traditionally thought of as a tedious, unavoidable task in building a machine learning classifier. However, in this paper, we argue that it can also serve as the first opportunity for developers to gain insight into their dataset. Through a Label-and-Learn interface, we explore visualization strategies that leverage the data labeling task to enhance developers' knowledge about their dataset, including the likely success of the classifier and the rationale behind the classifier's decisions. At the same time, we show that the visualizations also improve users' labeling experience by showing them the impact they have made on classifier performance. We assess the visualizations in Label-and-Learn and experimentally demonstrate their value to software developers who seek to assess the utility of machine learning during the data labeling process.","accessed":{"date-parts":[["2023",11,9]]},"author":[{"family":"Sun","given":"Yunjia"},{"family":"Lank","given":"Edward"},{"family":"Terry","given":"Michael"}],"citation-key":"sunLabelandLearnVisualizingLikelihood2017","collection-title":"IUI '17","container-title":"Proceedings of the 22nd International Conference on Intelligent User Interfaces","DOI":"10.1145/3025171.3025208","event-place":"New York, NY, USA","ISBN":"978-1-4503-4348-0","issued":{"date-parts":[["2017",3,7]]},"page":"523–534","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Label-and-Learn: Visualizing the Likelihood of Machine Learning Classifier's Success During Data Labeling","title-short":"Label-and-Learn","type":"paper-conference","URL":"https://doi.org/10.1145/3025171.3025208"},{"id":"topFoodDataFood2020","abstract":"The project Personalised Nutrition and Health (PNH) aims to develop methods and knowledge needed to make personalised food and health advice. Smart applications can support consumers in making healthy, but also safe and sustainable choices. Such applications require the availability of high quality data about food products and ingredients. This includes attributes such as nutritional values, ingredients, way of application, associated CO2 impact, etc. This information needs to be accessible, timely, accurate, reliable, comprehensive and reusable. Currently, many data sources about food are available. Although very rich in content, it is very difficult to extract and combine the data from these different sources, and even then many values remain missing or unreliable. An approach based on the principles of Linked Data can make the data available for automated processing, in particular if additional attributes are specified. Another issue related to the availability of product data is that it should ideally be openly available, directly from the producer of a product. In this document we focus on food data for health, in particular in the context of personalised dietary advice. We present an overview of existing data sources. It appears that for the Netherlands, nutritional values at the level of generic products can still best be found the NEVO table, but they are also increasingly available at the level of commercial product from the GS1 data pool. Many other attributes related to consumer preferences, application context, sustainable responsible production are scattered, if available at all. Several other data sources are available for retrieving food data relevant for applications in personalised nutrition. These data sources are still difficult to combine. Therefore the development of semantic standards is crucial. The most promising approach would be to combine the efforts by GS1, representing food supply chains worldwide and FoodOn as a research effort to introduce Linked Data standards in the world of food.","author":[{"family":"Top","given":"J.L.","dropping-particle":"van der"},{"family":"Timmer","given":"M.J."},{"family":"Simsek-Senel","given":"G."}],"citation-key":"topFoodDataFood2020","event-place":"Wageningen","issued":{"date-parts":[["2020"]]},"publisher":"Wageningen Food & Biobased Research","publisher-place":"Wageningen","title":"Food data and food vocabularies : PNH 2020 deliverable D5.3","type":"report","URL":"https://edepot.wur.nl/538580"},{"id":"touvronLLaMAOpenEfficient2023","abstract":"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.","accessed":{"date-parts":[["2023",4,20]]},"author":[{"family":"Touvron","given":"Hugo"},{"family":"Lavril","given":"Thibaut"},{"family":"Izacard","given":"Gautier"},{"family":"Martinet","given":"Xavier"},{"family":"Lachaux","given":"Marie-Anne"},{"family":"Lacroix","given":"Timothée"},{"family":"Rozière","given":"Baptiste"},{"family":"Goyal","given":"Naman"},{"family":"Hambro","given":"Eric"},{"family":"Azhar","given":"Faisal"},{"family":"Rodriguez","given":"Aurelien"},{"family":"Joulin","given":"Armand"},{"family":"Grave","given":"Edouard"},{"family":"Lample","given":"Guillaume"}],"citation-key":"touvronLLaMAOpenEfficient2023","DOI":"10.48550/arXiv.2302.13971","issued":{"date-parts":[["2023",2,27]]},"number":"arXiv:2302.13971","publisher":"arXiv","source":"arXiv.org","title":"LLaMA: Open and Efficient Foundation Language Models","title-short":"LLaMA","type":"article","URL":"http://arxiv.org/abs/2302.13971"},{"id":"traczBERTbasedSimilarityLearning2020","abstract":"Product matching, i.e., being able to infer the product being sold for a merchant-created offer, is crucial for any e-commerce marketplace, enabling product-based navigation, price comparisons, product reviews, etc. This problem proves a challenging task, mostly due to the extent of product catalog, data heterogeneity, missing product representants, and varying levels of data quality. Moreover, new products are being introduced every day, making it difficult to cast the problem as a classification task. In this work, we apply BERT-based models in a similarity learning setup to solve the product matching problem. We provide a thorough ablation study, showing the impact of architecture and training objective choices. Application of transformer-based architectures and proper sampling techniques significantly boosts performance for a range of e-commerce domains, allowing for production deployment.","accessed":{"date-parts":[["2024",9,9]]},"author":[{"family":"Tracz","given":"Janusz"},{"family":"Wójcik","given":"Piotr Iwo"},{"family":"Jasinska-Kobus","given":"Kalina"},{"family":"Belluzzo","given":"Riccardo"},{"family":"Mroczkowski","given":"Robert"},{"family":"Gawlik","given":"Ireneusz"}],"citation-key":"traczBERTbasedSimilarityLearning2020","container-title":"Proceedings of Workshop on Natural Language Processing in E-Commerce","editor":[{"family":"Zhao","given":"Huasha"},{"family":"Sondhi","given":"Parikshit"},{"family":"Bach","given":"Nguyen"},{"family":"Hewavitharana","given":"Sanjika"},{"family":"He","given":"Yifan"},{"family":"Si","given":"Luo"},{"family":"Ji","given":"Heng"}],"event-place":"Barcelona, Spain","event-title":"EcomNLP 2020","issued":{"date-parts":[["2020",12]]},"page":"66–75","publisher":"Association for Computational Linguistics","publisher-place":"Barcelona, Spain","source":"ACLWeb","title":"BERT-based similarity learning for product matching","type":"paper-conference","URL":"https://aclanthology.org/2020.ecomnlp-1.7"},{"id":"vilcekTransformerBasedDeepSiamese2021","abstract":"In the current digital world, improving user-product interaction is more important than ever for both consumers and merchants. In this paper, we present a Transformer-based deep Siamese network for product matching and one-shot product taxonomy classification: We begin by using textual descriptions of products as input data to train the Siamese network initialized using a Microsoft DeBERTa[1] pre-trained model. During training, the network minimizes the distance between similar pairs of products while pushing dissimilar pairs away from each other, and by doing so it learns to map products with similar descriptions to nearby locations in an embedding space, thereby creating \"clusters\" of similar products. Once trained, the model can output the contextual embeddings for any given product description. We then use these product embeddings as inputs to a downstream one-shot hierarchical classification task in order to predict the product’s taxonomy. For all existing products, the label (the product taxonomy) is based on the product internal catalog. Finally, we design and conduct multiple experiments to validate and verify this approach.","accessed":{"date-parts":[["2024",9,9]]},"author":[{"family":"Vilcek","given":"Alexandre"},{"family":"Mottaghinejad","given":"Seth"},{"family":"SHI","given":"Microsoft Usa STEVEN"},{"family":"Gupte","given":"Ketki"},{"family":"Labs","given":"Walmart"},{"family":"Steven.Yukai","given":""},{"family":"Shi","given":""}],"citation-key":"vilcekTransformerBasedDeepSiamese2021","issued":{"date-parts":[["2021"]]},"source":"Semantic Scholar","title":"Transformer-Based Deep Siamese Network for At-Scale Product Matching and One-Shot Hierarchy Classification","type":"paper-conference","URL":"https://www.semanticscholar.org/paper/Transformer-Based-Deep-Siamese-Network-for-At-Scale-Vilcek-Mottaghinejad/c0af928ae24e7e0ff0ab11dad4c83bbe124fd5cc"},{"id":"wangKnowledgeGraphQuality2021","abstract":"A knowledge graph (KG), a special form of semantic network, integrates fragmentary data into a graph to support knowledge processing and reasoning. KG quality control is important to the utility of KGs. It is essential to investigate KG quality and the parameters influencing KG quality to better understand its quality control. Although many works have been conducted to evaluate the dimensions of KG quality, quality control of the construction process, and enhancement methods for quality, a comprehensive literature review has not been presented on this topic. This paper intends to fill this research gap by presenting a comprehensive survey on the quality control of KGs. First, this paper defines six main evaluation dimensions of KG quality and investigates their correlations and differences. Second, quality control treatments during KG construction are introduced from the perspective of these dimensions of KG quality. Third, the quality enhancement of a constructed KG is described from various dimensions. This paper ultimately aims to promote the research and applications of KGs.","accessed":{"date-parts":[["2023",4,11]]},"author":[{"family":"Wang","given":"Xiangyu"},{"family":"Chen","given":"Lyuzhou"},{"family":"Ban","given":"Taiyu"},{"family":"Usman","given":"Muhammad"},{"family":"Guan","given":"Yifeng"},{"family":"Liu","given":"Shikang"},{"family":"Wu","given":"Tianhao"},{"family":"Chen","given":"Huanhuan"}],"citation-key":"wangKnowledgeGraphQuality2021","container-title":"Fundamental Research","container-title-short":"Fundamental Research","DOI":"10.1016/j.fmre.2021.09.003","ISSN":"2667-3258","issue":"5","issued":{"date-parts":[["2021",9,1]]},"language":"en","page":"607-626","source":"ScienceDirect","title":"Knowledge graph quality control: A survey","title-short":"Knowledge graph quality control","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S2667325821001655","volume":"1"},{"id":"WasIstData2022","abstract":"Erfahren Sie bei Proofpoint, welche Rolle Data Labeling bei Machine Learning spielt, wie es funktioniert und welche Best Practices sich bewährt haben.","accessed":{"date-parts":[["2023",11,14]]},"citation-key":"WasIstData2022","container-title":"Proofpoint","issued":{"date-parts":[["2022",10,28]]},"language":"de","title":"Was ist Data Labeling in Machine Learning? Definition | Proofpoint DE","title-short":"Was ist Data Labeling in Machine Learning?","type":"webpage","URL":"https://www.proofpoint.com/de/threat-reference/data-labeling"},{"id":"WebDataCommons","accessed":{"date-parts":[["2023",10,30]]},"citation-key":"WebDataCommons","title":"Web Data Commons","type":"webpage","URL":"http://webdatacommons.org/"},{"id":"WhatDataLabeling","abstract":"So, you’ve got your data all set for labeling. What then? This comprehensive guide on data labeling is exactly what you need to get a headstart.","accessed":{"date-parts":[["2023",11,13]]},"citation-key":"WhatDataLabeling","language":"en","title":"What is data labeling? The ultimate guide | SuperAnnotate","title-short":"What is data labeling?","type":"webpage","URL":"https://www.superannotate.com/blog/guide-to-data-labeling"},{"id":"WhatDataLabelling","abstract":"Data labeling is the process of assigning labels to data. Explore different types of data labeling, and learn how to do it efficiently.","accessed":{"date-parts":[["2023",11,9]]},"citation-key":"WhatDataLabelling","language":"en","title":"What Is Data Labelling and How to Do It Efficiently [2023]","type":"webpage","URL":"https://www.v7labs.com/blog/data-labeling-guide, https://www.v7labs.com/blog/data-labeling-guide"},{"id":"WhatMilvusMilvus","accessed":{"date-parts":[["2024",10,15]]},"citation-key":"WhatMilvusMilvus","title":"What is Milvus | Milvus Documentation","type":"webpage","URL":"https://milvus.io/docs/overview.md"},{"id":"wilkeMultiModalEntityResolution","abstract":"Entity Resolution has been applied successfully to match product oﬀers from diﬀerent web shops. Unfortunately, in certain domains the (textual or numerical) attributes of a product are not suﬃcient for a reliable match decision. To overcome this problem we extend an attribute-based matching system to incorporate image data, which are available in almost every web shop. To evaluate the system we enhance the WDC product matching dataset with images crawled from the web. First evaluations show that the use of images is beneﬁcial to increase recall and overall match quality.","author":[{"family":"Wilke","given":"Moritz"},{"family":"Rahm","given":"Erhard"}],"citation-key":"wilkeMultiModalEntityResolution","language":"en","source":"Zotero","title":"Towards Multi-Modal Entity Resolution for Product Matching","type":"article-journal"},{"id":"wilkeMultiModalEntityResolution2021","abstract":"Entity Resolution has been applied successfully to match product oﬀers from diﬀerent web shops. Unfortunately, in certain domains the (textual or numerical) attributes of a product are not suﬃcient for a reliable match decision. To overcome this problem we extend an attribute-based matching system to incorporate image data, which are available in almost every web shop. To evaluate the system we enhance the WDC product matching dataset with images crawled from the web. First evaluations show that the use of images is beneﬁcial to increase recall and overall match quality.","accessed":{"date-parts":[["2024",9,9]]},"author":[{"family":"Wilke","given":"M."},{"family":"Rahm","given":"E."}],"citation-key":"wilkeMultiModalEntityResolution2021","event-title":"GvDB","issued":{"date-parts":[["2021"]]},"source":"Semantic Scholar","title":"Towards Multi-Modal Entity Resolution for Product Matching","type":"paper-conference","URL":"https://www.semanticscholar.org/paper/Towards-Multi-Modal-Entity-Resolution-for-Product-Wilke-Rahm/c6b0bc5b9940726f98ccb44f4b383e0906047053"},{"id":"windrichKategorisierungUndVisualisierung","author":[{"family":"Windrich","given":"Melanie"}],"citation-key":"windrichKategorisierungUndVisualisierung","language":"de","source":"Zotero","title":"Kategorisierung und Visualisierung von Datenschutzaspekten in Geschäftsprozessmodellen","type":"article-journal"},{"id":"zhouSimplifiedDOMTrees2021","abstract":"There has been a steady need to precisely extract structured knowledge from the web (i.e. HTML documents). Given a web page, extracting a structured object along with various attributes of interest (e.g. price, publisher, author, and genre for a book) can facilitate a variety of downstream applications such as large-scale knowledge base construction, e-commerce product search, and personalized recommendation. Considering each web page is rendered from an HTML DOM tree, existing approaches formulate the problem as a DOM tree node tagging task. However, they either rely on computationally expensive visual feature engineering or are incapable of modeling the relationship among the tree nodes. In this paper, we propose a novel transferable method, Simplified DOM Trees for Attribute Extraction (SimpDOM), to tackle the problem by efficiently retrieving useful context for each node by leveraging the tree structure. We study two challenging experimental settings: (i) intra-vertical few-shot extraction, and (ii) cross-vertical fewshot extraction with out-of-domain knowledge, to evaluate our approach. Extensive experiments on the SWDE public dataset show that SimpDOM outperforms the state-of-the-art (SOTA) method by 1.44% on the F1 score. We also find that utilizing knowledge from a different vertical (cross-vertical extraction) is surprisingly useful and helps beat the SOTA by a further 1.37%.","accessed":{"date-parts":[["2024",12,17]]},"author":[{"family":"Zhou","given":"Yichao"},{"family":"Sheng","given":"Ying"},{"family":"Vo","given":"Nguyen"},{"family":"Edmonds","given":"Nick"},{"family":"Tata","given":"Sandeep"}],"citation-key":"zhouSimplifiedDOMTrees2021","DOI":"10.48550/arXiv.2101.02415","issued":{"date-parts":[["2021",1,7]]},"language":"en","number":"arXiv:2101.02415","publisher":"arXiv","source":"arXiv.org","title":"Simplified DOM Trees for Transferable Attribute Extraction from the Web","type":"article","URL":"http://arxiv.org/abs/2101.02415"},{"id":"zuoFlexibleLargescaleSimilar2020","abstract":"Identifying similar products is a common pain-point in the world of E-commerce search and discovery. The key challenges lie in two aspects: 1) The definition of similarity varies across different applications, such as near identical products sold by different vendors, products that are…","accessed":{"date-parts":[["2024",9,9]]},"author":[{"family":"Zuo","given":"Zhen"},{"family":"Wang","given":"Lixi"},{"family":"Momma","given":"Michinari"},{"family":"Wang","given":"Wenbo"},{"family":"Ni","given":"Yikai"},{"family":"Lin","given":"Jianfeng"},{"family":"Sun","given":"Yi"}],"citation-key":"zuoFlexibleLargescaleSimilar2020","container-title":"Amazon Science","issued":{"date-parts":[["2020"]]},"language":"en","title":"A flexible large-scale similar product identification system in e-commerce","type":"webpage","URL":"https://www.amazon.science/publications/a-flexible-large-scale-similar-product-identification-system-in-e-commerce"},{"id":"zuoFlexibleLargescaleSimilar2020a","abstract":"Identifying similar products is a common pain-point in the world of E-commerce search and discovery. The key challenges lie in two aspects: 1) The definition of similarity varies across different applications, such as near identical products sold by different vendors, products that are…","accessed":{"date-parts":[["2024",9,9]]},"author":[{"family":"Zuo","given":"Zhen"},{"family":"Wang","given":"Lixi"},{"family":"Momma","given":"Michinari"},{"family":"Wang","given":"Wenbo"},{"family":"Ni","given":"Yikai"},{"family":"Lin","given":"Jianfeng"},{"family":"Sun","given":"Yi"}],"citation-key":"zuoFlexibleLargescaleSimilar2020a","container-title":"Amazon Science","issued":{"date-parts":[["2020"]]},"language":"en","title":"A flexible large-scale similar product identification system in e-commerce","type":"webpage","URL":"https://www.amazon.science/publications/a-flexible-large-scale-similar-product-identification-system-in-e-commerce"},{"id":"wang_2022","type":"paper-conference","abstract":"Structure information extraction refers to the task of extracting structured text fields from web pages, such as extracting a product offer from a shopping page including product title, description, brand and price. It is an important research topic which has been widely studied in document understanding and web search. Recent natural language models with sequence modeling have demonstrated state-of-the-art performance on web information extraction. However, effectively serializing tokens from unstructured web pages is challenging in practice due to a variety of web layout patterns. Limited work has focused on modeling the web layout for extracting the text fields. In this paper, we introduce WebFormer, a Web-page transFormer model for structure information extraction from web documents. First, we design HTML tokens for each DOM node in the HTML by embedding representations from their neighboring tokens through graph attention. Second, we construct rich attention patterns between HTML tokens and text tokens, which leverages the web layout for effective attention weight computation. We conduct an extensive set of experiments on SWDE and Common Crawl benchmarks. Experimental results demonstrate the superior performance of the proposed approach over several state-of-the-art methods.","collection-title":"WWW '22","container-title":"Proceedings of the ACM Web Conference 2022","DOI":"10.1145/3485447.3512032","event-place":"New York, NY, USA","ISBN":"978-1-4503-9096-5","page":"3124–3133","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"WebFormer: The Web-page Transformer for Structure Information Extraction","title-short":"WebFormer","URL":"https://doi.org/10.1145/3485447.3512032","author":[{"family":"Wang","given":"Qifan"},{"family":"Fang","given":"Yi"},{"family":"Ravula","given":"Anirudh"},{"family":"Feng","given":"Fuli"},{"family":"Quan","given":"Xiaojun"},{"family":"Liu","given":"Dongfang"}],"accessed":{"date-parts":[["2025",1,20]]},"issued":{"date-parts":[["2022",4,25]]},"citation-key":"wang_2022","library":"KE3P","citekey":"wang_2022"},{"id":"brinkmann_2023","type":"article","abstract":"Structured product data in the form of attribute/value pairs is the foundation of many e-commerce applications such as faceted product search, product comparison, and product recommendation. Product offers often only contain textual descriptions of the product attributes in the form of titles or free text. Hence, extracting attribute/value pairs from textual product descriptions is an essential enabler for e-commerce applications. In order to excel, state-of-the-art product information extraction methods require large quantities of task-specific training data. The methods also struggle with generalizing to out-of-distribution attributes and attribute values that were not a part of the training data. Due to being pre-trained on huge amounts of text as well as due to emergent effects resulting from the model size, Large Language Models like ChatGPT have the potential to address both of these shortcomings. This paper explores the potential of ChatGPT for extracting attribute/value pairs from product descriptions. We experiment with different zero-shot and few-shot prompt designs. Our results show that ChatGPT achieves a performance similar to a pre-trained language model but requires much smaller amounts of training data and computation for fine-tuning.","DOI":"10.48550/arXiv.2306.14921","note":"arXiv:2306.14921 [cs]","number":"arXiv:2306.14921","publisher":"arXiv","source":"arXiv.org","title":"Product Information Extraction using ChatGPT","URL":"http://arxiv.org/abs/2306.14921","author":[{"family":"Brinkmann","given":"Alexander"},{"family":"Shraga","given":"Roee"},{"family":"Der","given":"Reng Chiz"},{"family":"Bizer","given":"Christian"}],"accessed":{"date-parts":[["2025",1,21]]},"issued":{"date-parts":[["2023",6,23]]},"citation-key":"brinkmann_2023","library":"KE3P","citekey":"brinkmann_2023"},{"id":"xu_2024","type":"article-journal","abstract":"Information Extraction (IE) aims to extract structural knowledge from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation. As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and techniques, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on a thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related works and resources on GitHub (LLM4IE repository).","container-title":"Frontiers of Computer Science","DOI":"10.1007/s11704-024-40555-y","ISSN":"2095-2236","issue":"6","journalAbbreviation":"Front. Comput. Sci.","language":"en","page":"186357","source":"Springer Link","title":"Large language models for generative information extraction: a survey","title-short":"Large language models for generative information extraction","URL":"https://doi.org/10.1007/s11704-024-40555-y","volume":"18","author":[{"family":"Xu","given":"Derong"},{"family":"Chen","given":"Wei"},{"family":"Peng","given":"Wenjun"},{"family":"Zhang","given":"Chao"},{"family":"Xu","given":"Tong"},{"family":"Zhao","given":"Xiangyu"},{"family":"Wu","given":"Xian"},{"family":"Zheng","given":"Yefeng"},{"family":"Wang","given":"Yang"},{"family":"Chen","given":"Enhong"}],"accessed":{"date-parts":[["2025",1,21]]},"issued":{"date-parts":[["2024",11,11]]},"citation-key":"xu_2024","library":"KE3P","citekey":"xu_2024"},{"id":"guo_2025","type":"paper-conference","abstract":"Information Extraction (IE) aims to extract structural knowledge (e.g., entities, relations, events) from natural language texts. Recently, Large Language Models (LLMs) with code-style prompts have demonstrated powerful capabilities in IE tasks. However, adopting code LLMs to conduct IE tasks still has two challenges: (1) It still lacks a unified code-style prompt for different IE tasks since existing methods use task-specific prompts for separate IE tasks. (2) It still lacks an effective in-context learning (ICL) method to encourage LLMs to conduct IE tasks precisely, considering some powerful LLMs are close-sourced and not trainable. Therefore, this paper proposes a code generation framework for Universal IE (UIE) tasks called Code4UIE. Specifically, for the first challenge, Code4UIE designs a unified code-style schema for various IE tasks via Python classes. By so doing, different IE tasks can be associated, and LLMs can learn from various IE tasks effectively. For the second challenge, Code4UIE adopts a retrieval-augmented mechanism to comprehensively utilize the ICL ability of LLMs. Extensive experiments on five representative IE tasks across nine datasets demonstrate the effectiveness of the Code4UIE framework.","container-title":"Natural Language Processing and Chinese Computing","DOI":"10.1007/978-981-97-9434-8_3","event-place":"Singapore","ISBN":"978-981-97-9434-8","language":"en","page":"30-42","publisher":"Springer Nature","publisher-place":"Singapore","source":"Springer Link","title":"Retrieval-Augmented Code Generation for Universal Information Extraction","author":[{"family":"Guo","given":"Yucan"},{"family":"Li","given":"Zixuan"},{"family":"Jin","given":"Xiaolong"},{"family":"Liu","given":"Yantao"},{"family":"Zeng","given":"Yutao"},{"family":"Liu","given":"Wenxuan"},{"family":"Li","given":"Xiang"},{"family":"Yang","given":"Pan"},{"family":"Bai","given":"Long"},{"family":"Guo","given":"Jiafeng"},{"family":"Cheng","given":"Xueqi"}],"editor":[{"family":"Wong","given":"Derek F."},{"family":"Wei","given":"Zhongyu"},{"family":"Yang","given":"Muyun"}],"issued":{"date-parts":[["2025"]]},"citation-key":"guo_2025","library":"KE3P","citekey":"guo_2025"},{"id":"li_2024","type":"paper-conference","container-title":"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","DOI":"10.18653/v1/2024.acl-long.475","event-place":"Bangkok, Thailand","event-title":"ACL 2024","page":"8758–8779","publisher":"Association for Computational Linguistics","publisher-place":"Bangkok, Thailand","source":"ACLWeb","title":"KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction","title-short":"KnowCoder","URL":"https://aclanthology.org/2024.acl-long.475/","author":[{"family":"Li","given":"Zixuan"},{"family":"Zeng","given":"Yutao"},{"family":"Zuo","given":"Yuxin"},{"family":"Ren","given":"Weicheng"},{"family":"Liu","given":"Wenxuan"},{"family":"Su","given":"Miao"},{"family":"Guo","given":"Yucan"},{"family":"Liu","given":"Yantao"},{"family":"Lixiang","given":"Lixiang"},{"family":"Hu","given":"Zhilei"},{"family":"Bai","given":"Long"},{"family":"Li","given":"Wei"},{"family":"Liu","given":"Yidan"},{"family":"Yang","given":"Pan"},{"family":"Jin","given":"Xiaolong"},{"family":"Guo","given":"Jiafeng"},{"family":"Cheng","given":"Xueqi"}],"editor":[{"family":"Ku","given":"Lun-Wei"},{"family":"Martins","given":"Andre"},{"family":"Srikumar","given":"Vivek"}],"accessed":{"date-parts":[["2025",1,21]]},"issued":{"date-parts":[["2024",8]]},"citation-key":"li_2024","library":"KE3P","citekey":"li_2024"},{"id":"dang_2024","type":"article-journal","abstract":"Integrating Schema.org markup into web pages has resulted in the generation of billions of RDF triples. However, around 75% of web pages still lack this critical markup. Large Language Models (LLMs) present a promising solution by automatically generating the missing Schema.org markup. Despite this potential, there is currently no benchmark to evaluate the markup quality produced by LLMs. This paper introduces LLM4Schema.org, an innovative approach for assessing the performance of LLMs in generating Schema.org markup. Unlike traditional methods, LLM4Schema.org does not require a predefined ground truth. Instead, it compares the quality of LLM-generated markup against human-generated markup. Our findings reveal that 40–50% of the markup produced by GPT-3.5 and GPT-4 is invalid, non-factual, or non-compliant with the Schema.org ontology. These errors underscore the limitations of LLMs in adhering strictly to structured ontologies like Schema.org without additional filtering and validation mechanisms. We demonstrate that specialized LLM-powered agents can effectively identify and eliminate these errors. After applying such filtering for both human and LLM-generated markup, GPT-4 shows notable improvements in quality and outperforms humans. LLM4Schema.org highlights both the potential and challenges of leveraging LLMs for semantic annotations, emphasizing the critical role of careful curation and validation in achieving reliable results.","language":"en","source":"Zotero","title":"LLM4Schema.org: Generating Schema.org Markups with Large Language Models","author":[{"family":"Dang","given":"Minh-Hoang"},{"family":"Pham","given":"Thi Hoang Thi"},{"family":"Molli","given":"Pascal"},{"family":"Skaf-Molli","given":"Hala"},{"family":"Gaignard","given":"Alban"}],"issued":{"date-parts":[["2024"]]},"citation-key":"dang_2024","library":"KE3P","citekey":"dang_2024"}]