---
category: literaturenote
tags: 
citekey: li_2024
status: unread
dateread:
---

> [!Cite]
> Li, Z., Zeng, Y., Zuo, Y., Ren, W., Liu, W., Su, M., Guo, Y., Liu, Y., Lixiang, L., Hu, Z., Bai, L., Li, W., Liu, Y., Yang, P., Jin, X., Guo, J., & Cheng, X. (2024). KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction. In L.-W. Ku, A. Martins, & V. Srikumar (Eds.), _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ (pp. 8758–8779). Association for Computational Linguistics. [https://doi.org/10.18653/v1/2024.acl-long.475](https://doi.org/10.18653/v1/2024.acl-long.475)

>[!Synth]
>**Contribution**:: 
>
>**Related**:: 
>

>[!metadata]
> **FirstAuthor**:: Li, Zixuan  
> **Author**:: Zeng, Yutao  
> **Author**:: Zuo, Yuxin  
> **Author**:: Ren, Weicheng  
> **Author**:: Liu, Wenxuan  
> **Author**:: Su, Miao  
> **Author**:: Guo, Yucan  
> **Author**:: Liu, Yantao  
> **Author**:: Lixiang, Lixiang  
> **Author**:: Hu, Zhilei  
> **Author**:: Bai, Long  
> **Author**:: Li, Wei  
> **Author**:: Liu, Yidan  
> **Author**:: Yang, Pan  
> **Author**:: Jin, Xiaolong  
> **Author**:: Guo, Jiafeng  
> **Author**:: Cheng, Xueqi  
~> **FirstEditor**:: Ku, Lun-Wei  
> **Editor**:: Martins, Andre  
> **Editor**:: Srikumar, Vivek  
~    
> **Title**:: KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction  
> **Year**:: 2024   
> **Citekey**:: li_2024  
> **itemType**:: conferencePaper  
> **Publisher**:: Association for Computational Linguistics  
> **Location**:: Bangkok, Thailand   
> **Pages**:: 8758–8779  
> **DOI**:: 10.18653/v1/2024.acl-long.475    

> [!LINK] 
>
>  [Full Text PDF](file:///home/cbrosch/Zotero/storage/CEFSX8W9/Li%20et%20al.%20-%202024%20-%20KnowCoder%20Coding%20Structured%20Knowledge%20into%20LLMs%20for%20Universal%20Information%20Extraction.pdf).

> [!Abstract]
>> 
# Notes
>.

Example from the entities.py file auto-generated based off the wikidata rdfs schema.

```python
class Entity:
"""
The base class for all entities.
"""
def __init__(self, name: str):
	self.name = name

class NorwegianOfficialReport(PublicInquiry):
"""
Description:
Examples: Government Commission for Higher Education
"""
	pass

class MunicipalitiesOfGermany(FourthLevelAdministrativeCountrySubdivision):
"""
Description: Lowest official level of territorial division in Germany.

Examples: Schmallenberg, Gardelegen, Baden-Baden, Bayreuth, Kaiserslautern, Giessen, Bad Nenndorf, Bad Harzburg, Esslingen am Neckar, Stade
"""
	pass
```
![[Pasted image 20250121141724.png]]
# Annotations%% begin annotations %%



### Imported: 2025-01-21 1:22 pm

> ![[image-4-x63-y576.png]]


### Imported: 2025-01-21 2:35 pm



<mark style="background-color: #ffd400">Quote</mark>
> Information Extraction (IE) aims to extract explicit and structured knowledge following the manually designed schemas.

<mark style="background-color: #ffd400">Quote</mark>
> Recently, Large Language Models (LLMs) have demonstrated general understanding abilities through large-scale pretraining, which drives their increasing utilization in UIE. However, their performance on UIE is still limited because of two main challenges:

<mark style="background-color: #f19837">Challange</mark>
> (1) the lack of a unified schema representation method that LLMs can easily understand;

<mark style="background-color: #f19837">Challange</mark>
> (2) the lack of an effective learning framework that encourages LLMs to accurately follow specific schemas for extracting structured knowledge.

<mark style="background-color: #ffd400">Quote</mark>
> For the first challenge, the existing UIE models first represent different schemas in a universal way, such as classification labels (Lin et al., 2020a), keywords (Gui et al., 2023), or a specifically-designed formal language (Lu et al., 2022). These schema representation methods have three main restrictions:

<mark style="background-color: #ffd400">Quote</mark>
> (1) ignoring information like taxonomies (e.g., “fairytale” is a subclass of “written work”) and constraints among concepts (e.g., “spouse” relation exists between two “human” entities);

<mark style="background-color: #ffd400">Quote</mark>
> (2)  87158 classification labels or a specifically designed formal language is hard for LLMs to understand and follow;

<mark style="background-color: #ffd400">Quote</mark>
> (3) designed for specific IE datasets and lacking a general schema library.

<mark style="background-color: #e56eee">Contribution</mark>
> To solve these restrictions, in this paper, we propose a kind of code-style schema representation method, with which various types of knowledge are generally defined as Python classes.

<mark style="background-color: #e56eee">Contribution</mark>
> For the second challenge, the existing learning framework for UIE directly conducts instruction tuning on LLMs to extract knowledge following specific and limited schemas (Sainz et al., 2023; Wang et al., 2023b). The enormous concepts in the constructed schema library challenge the existing training framework. To help LLMs better understand and follow these schemas, we propose an effective two-phase framework containing a schema understanding phase and a schema following phase

<mark style="background-color: #e56eee">Contribution</mark>
> In general, the main contributions of this paper include:  • We propose a code-style schema representation method to uniformly represent different schemas for UIE. Using this method, we construct a large code-style schema library covering more than 30, 000 types of knowledge.  • We propose an effective learning framework for LLMs in a two-phase manner, which first enhances the schema understanding through code pretraining and then boosts schema following via instruction tuning.  • After training on billions of automatically annotated data and refining with humanannotated IE datasets, KnowCoder demonstrates superior performance on different IE tasks under the zero-shot, low-resource, and supervised settings.  • The constructed schema library, training data, code, and models are released for future research.

<mark style="background-color: #ffd400">Quote</mark>
> Thus, as shown in Figure 2, the proposed learning framework contains two phases, i.e., the schema understanding phase and the schema following phase. In the schema understanding phase, KnowCoder undergoes code pretraining to understand each concept in two manners: 1) Go through the class definition code of each concept. 2) Go through the instance codes of each concept. In the schema following phase, KnowCoder is finetuned using instruction tuning code, where multiple task-demanded concepts are given in the schemas, enhancing KnowCoder’s ability to follow schemas and generate instantiating code accordingly.

<mark style="background-color: #2ea8e5">Section</mark>
> Schema Understanding Phase

<mark style="background-color: #2ea8e5">Section</mark>
> Training Data Generation

<mark style="background-color: #ffd400">Quote</mark>
> To enhance KnowCoder’s schema understanding abilities, we construct a large-scale training dataset based on the schema library.

<mark style="background-color: #ffd400">Quote</mark>
> As shown in the left part of Figure 2, the training data consists of two kinds of codes, i.e., schema definition codes and instance codes.

<mark style="background-color: #ffd400">Quote</mark>
> The schema-instance codes are constructed based on KELM corpus (Agarwal et al., 2021), which contains 15, 628, 486 synthetic sentences to describe the structured knowledge from Wikidata.

<mark style="background-color: #ff6666">Conclusion</mark>
> In this paper, we introduced KnowCoder for UIE leveraging Large Language Models. KnowCoder is based on a code-style schema representation method and an effective two-phase learning framework. The code-style schema representation method uniformly transforms different schemas into Python classes, with which the UIE task can be converted to a code generation process. Based on the schema representation method, we constructed a comprehensive code-style schema library covering over 30, 000 types of knowledge. To let LLMs understand and follow these schemas, we further proposed a two-phase learning framework that first enhances the schema comprehension ability and then boosts its schema following ability. After training on billions of automatically annotated data and refining with human-annotated IE datasets, KnowCoder demonstrates remarkable performance improvements on different IE tasks under the various evalution settings.


%% end annotations %%

%% Import Date: 2025-01-21T14:35:23.567+01:00 %%
